# Bias-corrected estimation of the TPDM {#sec-shrinkage-tpdm}

```{r shrinkage-tpdm-load-packages}
#| include: false
library(tidyverse)
library(tidyr)
library(magrittr)
library(scales)
library(ggh4x)
library(ggpubr)
library(colorspace)
library(kableExtra)
library(reshape2)
library(mev)
library(Matrix)
library(gridExtra)

options(dplyr.summarise.inform = FALSE)
options(knitr.kable.NA = "")
```

```{r shrinkage-tpdm-source-functions}
#| include: false
sapply(list.files(path = "R/shrinkage-tpdm", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
sapply(list.files(path = "R/general", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

## Introduction and motivation

@sec-background-bias-issue demonstrated a deficiency of the empirical TPDM estimator, which we termed the bias issue. Specifically, the empirical TPDM $\hat{\Sigma}$ has a tendency to overestimate weak pairwise dependence -- see \eqref{eq-empirical-tpdm-bias}. The bias issue has arisen at several subsequent points throughout the thesis: our test for time-varying dependence becomes less powerful when dependence is weak (@fig-test-power-weak-dependence); projections onto principal eigenspaces of $\hat{\Sigma}$ result in poor reconstructions near the simplex boundary (e.g. see bottom right plot in @fig-sim-pca-hr-pmin); tail probability estimates based on completely positive factorisations of $\hat{\Sigma}$ are biased (@fig-eva-c4-prob-estimates). 

This chapter addresses the problem of reducing bias in the empirical TPDM. Bias reduction requires grappling with two interconnected problems. First, what *form* should a bias-corrected estimator take? The procedure proposed in @fixSimultaneousAutoregressiveModels2021 (see @sec-background-tpdm-fitting-max-linear) shrinks all entries of $\hat{\Sigma}$ equally. We contend that this is not ideal, given that the bias is greatest for pairs of variables where dependence is weak. Inspired by @rothmanGeneralizedThresholdingLarge2009, a flexible extension based on the adaptive lasso is proposed. This permits control over whether shrinkage is applied equally to all entries or targeted towards smaller entries, at the cost of introducing an additional parameter. The second problem concerns *how much* bias correction should be applied. @fixSimultaneousAutoregressiveModels2021 solve this by assuming that dependence strength is a decreasing function of spatial distance and is approximately zero for the most distant pair(s) of sites in the study region. However, these assumptions are only meaningful (let alone satisfied) in spatial settings, precluding its use in general applications. While we were unable to find a general solution to this research problem, we made progress in a special case where bias-reduction is achieved via linear shrinkage techniques [@ledoitImprovedEstimationCovariance2003]. For this class, we are able to devise a theoretically justified, data-driven procedure for selecting the magnitude of the shrinkage. 

## Regularised TPDM estimators

In high-dimensional settings, where the number of variables is comparable to the number of observations, the empirical covariance matrix may be noisy and poorly conditioned. Regularisation may be applied to obtain more stable and accurate estimates. In particular, thresholding is used to enforce sparsity by shrinking small values towards zero, enhancing stability and interpretability in cases where many variables are weakly correlated [@rothmanGeneralizedThresholdingLarge2009]. On the other hand, shrinkage methods move the empirical estimate towards a biased but highly structured pre-specified target matrix [@ledoitImprovedEstimationCovariance2003]. In both cases, more accurate estimates are obtained by reducing the entries of the empirical estimator in some way. Thus, we may consider applying these techniques to the empirical TPDM to achieve bias-reduction. This may have the additional benefit of improving stability in cases where $d$ is comparable to the number of extremes $k$, but this aspect will not be explored. 

### Thresholded TPDM estimators

The core idea of thresholded estimation is to apply a thresholding operator to the raw, empirical estimate (in our case $\hat{\Sigma}$). The thresholding operator controls the nature of the regularisation and must satisfy certain properties. 

:::{#def-thresholding-operator}
Let $\lambda\geq 0$ be a fixed threshold. A function $s_\lambda:\R_+\to\R_+$ is called a thresholding operator if, for all $x\in\R_+$,
\begin{align}
& s_{\lambda}(x)\leq x, \label{eq-threshold-operator-i} \\
& x\leq\lambda \implies s_\lambda(x)=0, \label{eq-threshold-operator-ii} \\
& |s_\lambda(x)-x|\leq \lambda. \label{eq-threshold-operator-iii}
\end{align}
:::

The tuning parameter $\lambda$ controls the strength of the regularisation. In the two extreme cases, we have $s_0(x)=x$ (no regularisation) and $s_\infty(x)=0$ for all $x\in\R_+$ (full shrinkage). Condition \eqref{eq-threshold-operator-i} states that $s_\lambda$ should push $x$ towards zero. Condition \eqref{eq-threshold-operator-ii} enforces a threshold below which values are mapped to zero, potentially inducing sparsity in the output. Condition \eqref{eq-threshold-operator-iii} limits the amount of shrinkage. For simplicity, we assume the threshold in \eqref{eq-threshold-operator-ii} and the maximal shrinkage in \eqref{eq-threshold-operator-iii} are identical. This could be relaxed by introducing separate parameters $\lambda_1$ and $\lambda_2$ if desired [@antoniadisRegularizationWaveletApproximations2001]. Thresholding operators may be equivalently formulated as solutions to regularised optimisation problems of the form
\begin{equation}\label{eq-thresholding-regularisation-problem}
    s_\lambda(x) = \argmin_{y\in\R_+}\left[\frac{1}{2}(y-x)^2 + p_\lambda(y;x)\right]
\end{equation}
for some penalty function $p_\lambda$. While we do not explicitly make use of this formulation, the penalty function is useful for providing insight into an operator's behaviour.

We will consider three popular thresholding operators: hard-thresholding, soft-thresholding, and adaptive lasso. For reference, the corresponding functions $x\mapsto s_{0.1}(x)$ are shown in @fig-threshold-operators. The hard-thresholding rule 
\begin{equation}
    s_\lambda^H(x) = x\ind\{x > \lambda\}
\end{equation}
applies no shrinkage to values above $\lambda$. The jump discontinuity in $s_\lambda^H(x)$ at $\lambda=x$ can be observed in @fig-threshold-operators. In the space of thresholding operators, it is diametrically opposed to the soft-thresholding operator
\begin{equation}
    s_\lambda^S(x) = (x-\lambda)_+,
\end{equation}
which induces the maximal amount of shrinkage permitted by \eqref{eq-threshold-operator-iii}. Soft-thresholding 
corresponds to solving \eqref{eq-thresholding-regularisation-problem} with lasso penalty function $p_\lambda(y;x)=\lambda y$. Note that $p_\lambda(y;x)$ does not depend on $x$, reflecting the fact that equal shrinkage is applied to large and small values (exceeding $\lambda$) alike. Other thresholding operators offer some kind of compromise between these two cases. The adaptive lasso penalty function $p_\lambda(y;x)=\lambda^{\eta+1}yx^{-\eta}$ down-weights larger values, leading to the thresholding rule
\begin{equation}\label{eq-adaptive-lasso}
    s_{\lambda,\eta}^{AL}(x) = (x-\lambda^{\eta+1}x^{-\eta})_+, \qquad (\eta \geq 0).
\end{equation}
The degree of down-weighting is controlled by the tuning parameter $\eta\geq 0$. Setting $\eta=0$ results in no down-weighting, yielding the soft-thresholding operator $s_{\lambda,0}^{AL}(x)=s_\lambda^S(x)$. On the other hand, $s_{\lambda,\eta}^{AL}(x)\to s_\lambda^H(x)$ as $\eta\to\infty$. Thus, soft- and hard-thresholding are special (limiting) cases of adaptive lasso. The clipped $L_1$-penalty and the smoothly clipped absolute deviations (SCAD) penalty behave in a similar fashion [@antoniadisRegularizationWaveletApproximations2001]. 

```{r make-fig-threshold-operators}
#| label: fig-threshold-operators
#| fig-cap: "Popular thresholding operators $s_{\\lambda}(x)$ with $\\lambda=0.1$."
#| fig-scap: "Popular thresholding operators $s_{\\lambda}(x)$ with $\\lambda=0.1$."
#| fig-height: 3.5

ggplot(data.frame(x = c(0, 0.3)), aes(x = x)) +
  stat_function(aes(colour = "Hard"), fun = s_hard, args = list(lambda = 0.1)) +
  stat_function(aes(colour = "Soft"), fun = s_soft, args = list(lambda = 0.1)) +
  stat_function(aes(colour = "Adaptive lasso", linetype = "1"), fun = s_adlasso, args = list(lambda = 0.1, eta = 1)) +
  stat_function(aes(colour = "Adaptive lasso", linetype = "2"), fun = s_adlasso, args = list(lambda = 0.1, eta = 2)) +
  scale_x_continuous(limits = c(0, 0.3), expand = c(0, 0), breaks = breaks_extended(n = 5)) +
  scale_y_continuous(limits = c(0, 0.3), expand = c(0, 0), breaks = breaks_extended(n = 5)) +
  scale_colour_manual(values = c("darkgreen", "red", "blue")) +
  scale_linetype_manual(values = c("solid", "dashed"), labels = c(1, 2)) +
  labs(x = expression(x), y = expression(s[lambda](x)), colour = "Threshold operator", linetype = expression(eta)) +
  theme_light()
```

We now proceed to apply thresholding to the empirical TPDM to define our first class of bias-reduced estimators. Let $A=(a_{ij})\in\R_+^{m\times n}$ be a non-negative matrix and $s_\lambda:\R_+\to\R_+$ a thresholding operator. The thresholded matrix $s_\lambda(A):=(s_\lambda(a_{ij}))$ is defined by element-wise application of $s_\lambda$ to $A$. Clearly this is very simple and fast to compute.  

:::{#def-thresholded-tpdm}
Let $s_\lambda$ be a thresholding operator. The thresholded TPDM estimator is the $d\times d$ matrix 
\begin{equation}\label{eq-thresholded-empirical-tpdm}
  \tilde{\Sigma}(\lambda) = (\tilde{\sigma}_{ij}(\lambda)), \qquad 
    \tilde{\sigma}_{ij} = 
    \begin{cases}
      \hat{\sigma}_{ij}, & i=j, \\
    s_\lambda(\hat{\sigma}_{ij}), & i\neq j.
    \end{cases}
\end{equation}
:::

Note that we do not shrink the diagonal entries, but in any case the performance of estimators will be evaluated using the off-diagonal entries only. The soft-thresholded TPDM $s_{\lambda}^S (\hat{\Sigma})$ coincides with estimator proposed by @fixSimultaneousAutoregressiveModels2021. However, @def-thresholded-tpdm includes a much broader class of estimators with more flexible behaviours. In particular, the adaptive lasso TPDM $s_{\lambda,\eta}^{AL}(\hat{\Sigma})$ allows us to concentrate shrinkage towards small entries in $\hat{\Sigma}$, where the bias is presumed to be greater. @fig-red-sea-tpdm provides a visual comparison between the three thresholding methods using the Red Sea surface temperature data from @sec-changing-ext-dep-red-sea. Each matrix represents a regularised TPDM $\tilde{\Sigma}$ of the random vector $\bm{X}$, where $X_i$ corresponds to the $i$th site in @fig-redsea-sites (ordered by longitude and then by latitude) for $i=1,\ldots,200$. The figure shows how the hard-thresholded, soft-thresholded and adaptive lasso ($\eta=2$) estimates vary with the regularisation parameter $\lambda\in\{0,0.1,\ldots,0.5\}$. The empirical TPDM is estimated from the proportion $k/n=0.15$ of largest observations; this matrix is shown in the sub-panels where $\lambda=0$. Since the components of $\bm{X}$ are ordered according to the spatial locations, $\hat{\Sigma}$ generally exhibits stronger dependencies among variable pairs closer to the diagonal and weaker dependencies for those farther away. Hard-thresholding (top left) has no effect when $\lambda<\min\{\hat{\sigma}_{ij}:i\neq j\}\approx 0.249$. Increasing the threshold introduces abrupt shifts, resulting in a non-smooth sequence of solutions. In contrast, soft-thresholding yields smoother solutions because all off-diagonal entries receive some shrinkage when $\lambda>0$. However, the regularisation acts indiscriminately by applying an equal penalty to all entries. Consequently, as $\lambda$ increases the dependence strength is close to zero for all pairs of sites, including neighbouring ones. The adaptive lasso (bottom left) combines the advantages of hard- and soft-thresholding, yielding smooth solution paths but prioritising shrinkage of entries that are close to zero.

```{r make-fig-red-sea-tpdm}
#| label: fig-red-sea-tpdm
#| fig-cap: "Regularised TPDM estimates $\\tilde{\\Sigma}(\\lambda)$ based on the Red Sea surface temperature data from @sec-changing-ext-dep-red-sea. The $d=200$ sites in @fig-redsea-sites are ordered by longitude and then latitude and $\\hat{\\Sigma}$ is estimated with $k/n=0.15$. The values at the top of each sub-panel represent the regularisation parameter $\\lambda\\in\\{0,0.1,\\ldots,0.5\\}$. For the adaptive lasso estimates we take $\\eta=2$."
#| fig-scap: "Regularised TPDM estimates corresponding to the Red Sea data."
#| fig-height: 6

red_sea_Sigma <- readRDS("scripts/shrinkage-tpdm/results/red-sea-tpdm.RDS")

p1 <- plot_tpdm(red_sea_Sigma$hard_Sigma, x_labels = FALSE, y_labels = FALSE, n_col = 21, main = "Hard-threshold")
p2 <- plot_tpdm(red_sea_Sigma$soft_Sigma, x_labels = FALSE, y_labels = FALSE, n_col = 21, main = "Soft-threshold")
p3 <- plot_tpdm(red_sea_Sigma$adlasso_Sigma, x_labels = FALSE, y_labels = FALSE, n_col = 21, main = "Adaptive lasso")
p4 <- plot_tpdm(red_sea_Sigma$lw_Sigma, x_labels = FALSE, y_labels = FALSE, n_col = 21, main = "Ledoit-Wolf")
ggarrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

A drawback of these estimators is that thresholding fails to preserve certain matrix properties, including positive semi-definiteness and complete positivity. Evidence of this is provided in @fig-red-sea-tpdm-eigenvalues, which depicts the eigenvalues of the hard-thresholded TPDMs $\tilde{\Sigma}(\lambda)$ from @fig-red-sea-tpdm. Specifically, the plot shows the absolute value of the eigenvalues, with positive eigenvalues in black and negative eigenvalues in red. As $\lambda$ increases and the thresholding takes effect, the eigenvalues of $\tilde{\Sigma}(\lambda)$ get shrunk, occasionally causing them to become negative. Thus, $\tilde{\Sigma}$ may need to undergo pre-processing before it can be incorporated into existing TPDM-based modelling tools such as PCA. This motivates us to propose an alternative approach to bias-reduction based on linear shrinkage methods. While this class is less flexible than thresholded TPDMs, its simplicity ensures estimates possess nice properties and are more amenable to mathematical analysis.

```{r make-fig-red-sea-tpdm}
#| label: fig-red-sea-tpdm-eigenvalues
#| fig-cap: "Absolute values of the eigenvalues of the hard-thresholded TPDM estimates $\\tilde{\\Sigma}(\\lambda)$ from @fig-red-sea-tpdm (top left) for $\\lambda\\in\\{0.2, 0.3, 0.4, 0.5\\}$. Positive and negative eigenvalues are coloured black and red, respectively."
#| fig-scap: "Hard-thresholded TPDM eigenvalues corresponding to the Red Sea data."
#| fig-height: 3.5

fancy_scientific_short <- function(l) {
  l %>%
    format(scientific = TRUE) %>%
    gsub("^(.*)e", "e", .) %>%
    gsub("e", "10^", .) %>%
    parse(text = .)
}

lapply(red_sea_Sigma$hard_Sigma, function(S) eigen(S)$values) %>%
  Reduce(cbind, .) %>%
  set_colnames(names(red_sea_Sigma$hard_Sigma)) %>%
  as.data.frame() %>%
  mutate(index = seq_len(nrow(.))) %>%
  pivot_longer(cols = -index, names_to = "reg_lambda", values_to = "eigenvalue") %>%
  filter(reg_lambda >= 0.2) %>%
  ggplot(aes(x = index, y = abs(eigenvalue), colour = as.factor(sign(eigenvalue)), shape = as.factor(sign(eigenvalue)))) +
  geom_point(size = 0.7) +
  facet_grid(. ~ reg_lambda) +
  scale_x_continuous(limits = c(0, 200), expand = c(0.02, 0.02), breaks = breaks_extended(n = 5)) +
  scale_y_continuous(trans = log10_trans(), labels = fancy_scientific_short, breaks = breaks_log(n = 6)) +
  scale_shape_manual(values = c(5, 10), labels = c("Negative", "Non-negative")) +
  scale_colour_manual(values = c("red", "black"), labels = c("Negative", "Non-negative")) +
  labs(x = "Index", y = "Absolute value of eigenvalue", colour = "Sign of eigenvalue", shape = "Sign of eigenvalue") +
  theme_light() +
  theme(legend.position = "none")
```

### The Ledoit-Wolf TPDM estimator

Shrinkage is a regularisation technique dating back to @steinInadmissibilityUsualEstimator1956, whereby a sample estimate is shrunk towards some pre-specified target. The simplest shrinkage method is Ledoit-Wolf linear shrinkage [@ledoitImprovedEstimationCovariance2003]. Their approach to regularising the sample covariance matrix transfers easily to the TPDM. 

:::{#def-lw-tpdm}
Let $T$ be a $d\times d$ target matrix. For $\lambda\in[0,1]$, the Ledoit-Wolf TPDM is the $d\times d$ matrix
\begin{equation}\label{eq-lw-tpdm}
  \tilde{\Sigma}(\lambda) = \lambda T + (1-\lambda)\hat{\Sigma}.
\end{equation}
:::

The key idea is to bias the empirical TPDM $\hat{\Sigma}$ towards a highly structure target $T$ by forming a convex linear combination between the two. The shrinkage intensity $\lambda\in[0,1]$ determines the weight allocated to each matrix. No shrinkage occurs when $\lambda=0$. If $\lambda=1$ the estimator returns $T$, which is biased but perfectly stable (it has zero variance). The choice of target depends on the context at hand. We will always choose $T=I_d$, the $d\times d$ identity matrix. This means the off-diagonal entries are shrunk towards zero and the diagonal entries are left unaltered. Similar to soft-thresholding, the Ledoit-Wolf TPDM counters the bias by applying equal shrinkage to all entries. A slight difference is that the Ledoit-Wolf shrinkage intensity is applied multiplicatively, i.e. $\tilde{\sigma}_{ij}=(1-\lambda)\hat{\sigma}_{ij}$, whereas thresholding works additively. @fig-red-sea-tpdm (bottom right) shows the sequence of Ledoit-Wolf TPDMs for the Red Sea data, as described earlier. We observe that linear shrinkage behaves quite differently to any of the thresholding estimators; the general structure of the matrix does not change all that much as $\lambda$ increases. While this behaviour is not necessarily desirable from a modelling perspective, it explains why the Ledoit-Wolf TPDM retains many of the mathematical properties of the empirical TPDM.

:::{#prp-lw-tpdm-properties}
The Ledoit-Wolf TPDM $\tilde{\Sigma}(\lambda)$ is symmetric, positive semi-definite, and completely positive.
:::

:::{.proof}
For brevity, we suppress dependence on $\lambda$. Symmetry follows immediately because for any $i,j$,
\begin{equation*}
    \tilde{\sigma}_{ij} = \lambda \ind\{i=j\} + (1-\lambda)\hat{\sigma}_{ij} = \lambda \ind\{j=i\} + (1-\lambda) \hat{\sigma}_{ji} = \tilde{\sigma}_{ji}.
\end{equation*}
For positive semi-definiteness, recall that $\hat{\Sigma}$ is positive semi-definite and so for any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation*}
  \bm{y}^T\tilde{\Sigma}\bm{y} = \lambda \bm{y}^T\bm{y} + (1-\lambda)\bm{y}^T\hat{\Sigma}\bm{y} = \lambda \|\bm{y}\|_2^2 + (1-\lambda)\bm{y}^T\hat{\Sigma}\bm{y} \geq 0.
\end{equation*}
The identity matrix is trivially completely positive since $I_d=I_dI_d^T$ and $\hat{\Sigma}$ is completely positive by @prp-empirical-tpdm-completely-positive. The class of completely positive matrices is a convex cone [@bermanCompletelyPositiveMatrices2003], so complete positivity is preserved under convex combinations. \qedhere
:::

Positive semi-definiteness means the Ledoit-Wolf estimator may replace $\hat{\Sigma}$ in PCA techniques. Complete positivity implies that there exists a max-linear random vector $\bm{X}_{\lambda}$ whose TPDM is $\tilde{\Sigma}(\lambda)$. Therefore the Ledoit-Wolf TPDM is more straightforward to use in practice compared to the thresholded estimators. 

## Selecting the regularisation parameter

### Frobenius risk minimisation

Statistically, the main challenge is to select the regularisation parameter $\lambda$. A natural criterion is to choose $\lambda$ so as to minimise the discrepancy between the true TPDM $\Sigma$ and the regularised estimate $\tilde{\Sigma}(\lambda)$. We are generally only interested in the error in the off-diagonal entries, so we will tend to refer to the upper-half vectorised quantities 
\begin{equation*}
\bm{\sigma}=\mathrm{vecu}(\Sigma), \quad \hat{\bm{\sigma}}=\mathrm{vecu}(\hat{\Sigma}), \quad \tilde{\bm{\sigma}}(\lambda)=\mathrm{vecu}(\tilde{\Sigma}(\lambda)),
\end{equation*}
where the $\mathrm{vecu}$ operator was defined in \eqref{eq-vecu-sigma}. For a given class $\{\tilde{\Sigma}(\lambda):\lambda\geq 0\}$, the risk associated with $\lambda$ is defined as
\begin{equation}\label{eq-regularisation-frobenius-loss}
    \mathcal{R}(\lambda):=
    \mathbb{E}[\|\tilde{\bm{\sigma}}(\lambda) - \bm{\sigma}\|_2^2]
    =\mathbb{E}\left[\sum_{i<j}\left( \tilde{\sigma}_{ij}(\lambda) - \sigma_{ij} \right)^2\right].
\end{equation}
We refer to $\mathcal{R}(\lambda)$ as the Frobenius risk. The minimiser 
\begin{equation*}
    \lambda^\star := \argmin_{\lambda}\mathcal{R}(\lambda)
\end{equation*}
corresponds to the regularisation parameter which gives the TDPM estimate closest to the true TPDM in terms of the mean-square error. Of course, the true TPDM is always unknown in practice, so the risk $\mathcal{R}(\lambda)$ cannot be evaluated or minimised directly. We call $\lambda^\star$ the oracle parameter to signify that it is the value that would be chosen by an oracle with knowledge of the underlying true TPDM. Our goal is to devise a statistical procedure for estimating $\lambda^\star$.

The task of estimating $\lambda^\star$ becomes much simpler if we are privy to additional information about the data-generating process. For example, @fixSimultaneousAutoregressiveModels2021 infer a reasonable soft-thresholding parameter $\lambda=\beta_2$ in \eqref{eq-fix-bias-corrected-tpdm} by exploiting prior information about the spatial configuration and the spatial extent of the modelled phenomenon. Similarly, in supervised learning settings where an objective loss function and labelled training data are available, one may resort to standard hyperparameter tuning procedures such as cross validation. We study the problem in its most general, challenging form, where no additional structure/information is available.

For covariance matrices, a standard strategy for selecting the regularisation parameter is to minimise an empirical estimate of $\mathcal{R}$, i.e. an approximation of the risk function constructed from the data [@yiSUREtunedTaperingEstimation2013]. Unfortunately, this strategy does not transfer easily to the TPDM context. Upon noting that
\begin{equation*}
(\tilde{\sigma}_{ij}(\lambda) - \sigma_{ij})^2 = (\tilde{\sigma}_{ij}(\lambda) - \hat{\sigma}_{ij})^2 + 2 (\tilde{\sigma}_{ij}(\lambda) - \sigma_{ij})(\hat{\sigma}_{ij}(\lambda) - \sigma_{ij}) - (\hat{\sigma}_{ij} - \sigma_{ij})^2
\end{equation*}
the risk function \eqref{eq-regularisation-frobenius-loss} can be equivalently expressed as
\begin{equation}
    \mathcal{R}(\lambda) = \mathbb{E}[\|\tilde{\bm{\sigma}}(\lambda) - \hat{\bm{\sigma}}\|_2^2] + 2\sum_{i<j}\mathbb{E}[(\tilde{\sigma}_{ij}(\lambda) - \sigma_{ij})(\hat{\sigma}_{ij} - \sigma_{ij})] - \mathbb{E}[\|\hat{\bm{\sigma}} - \bm{\sigma}\|_2^2]
\end{equation}
These three terms are called apparent error, optimism, and constant [@yiSUREtunedTaperingEstimation2013]. The apparent error acts as a penalty against departing too far away from the empirical TPDM. This term can be estimated using the unbiased estimator $\|\tilde{\bm{\sigma}}(\lambda) - \hat{\bm{\sigma}}\|_2^2$. The constant term is irrelevant and can be ignored for the risk minimization as it does not depend on $\lambda$. The fundamental issue stems from the optimism term. For covariance matrices, the empirical estimator is unbiased and the summand may be replaced with $\mathrm{Cov}(\tilde{\sigma}_{ij}(\lambda),\hat{\sigma}_{ij})$, which may be estimated via bootstrapping [@fangTuningparameterSelectionRegularized2016]. This approach cannot be replicated for TPDMs, because the empirical estimator is biased. This impediment prevented us from finding a solution to the general problem of tuning arbitrary regularised TPDMs. However, we were able to devise a procedure for optimising Ledoit-Wolf estimators by taking a completely different approach to the one sketched above. 

### The optimal Ledoit-Wolf shrinkage intensity

@ledoitImprovedEstimationCovariance2003 provide a formula for the asymptotically optimal Ledoit-Wolf shrinkage intensity for covariance matrices. Inspired by their approach, we are able to obtain an analogous result for TPDMs. Suppose $\bm{X}_1,\bm{X}_2,\ldots$ is a sequence of independent copies of $\bm{X}$ and let $\tilde{\Sigma}(\lambda)$ denote a Ledoit-Wolf estimator of the TPDM. For any $\lambda\in[0,1]$, the associated Frobenius risk is
\begin{align*}
    \mathcal{R}(\lambda) 
    &= \mathbb{E}[\|(1-\lambda)\hat{\bm{\sigma}} - \bm{\sigma}\|_2^2] \\
    &= \sum_{i<j} \mathbb{E}\left[\left( (1-\lambda)\hat{\sigma}_{ij} - \sigma_{ij} \right)^2\right] \\
    &= \sum_{i<j} \left\lbrace (1-\lambda)^2\mathbb{E}[\hat{\sigma}_{ij}^2] - 2(1-\lambda)\sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}] + \sigma_{ij}^2 \right\rbrace.
\end{align*}
The first-order optimality condition gives
\begin{equation*}
    \mathcal{R}'(\lambda) = 0 
    \implies \sum_{i<j} 2(\lambda-1)\mathbb{E}[\hat{\sigma}_{ij}^2] + 2\sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}] = 0 
    \implies \lambda^\star := \frac{\sum_{i<j}\left\lbrace\mathbb{E}[\hat{\sigma}_{ij}^2] - \sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}]\right\rbrace}{\mathbb{E}[\|\hat{\bm{\sigma}}\|_2^2]}.
\end{equation*}
It is simple to check that $\mathcal{R}''(\lambda)=\sum_{i<j}2\mathbb{E}[\hat{\sigma}_{ij}^2]>0$, confirming that $\lambda^\star$ is indeed the risk minimiser. The formula for $\lambda^\star$ still depends on the true TPDM, so this is not helpful by itself. However, we may recognise the numerator of $\lambda^\star$ as being related to the variance of $\hat{\sigma}_{ij}$ and recall from @prp-empirical-tpdm-normality-entries that the asymptotic variance of $\sqrt{k}\hat{\sigma}_{ij}$ is $\nu_{ij}^2$. For clarity, we now include dependence on the sample size $n$ explicitly. Assume the conditions of @prp-empirical-tpdm-normality-entries hold and also that for all $i\neq j$ there exists a constant $C_{ij}$ such that
\begin{equation}\label{eq-sigma-rate-assumption}
\mathbb{E}[\hat{\sigma}_{ij}(n)] - \sigma_{ij} = \frac{C_{ij}}{k(n)} + \mathcal{O}(k(n)^{-2}),
\end{equation}
so that $k(n)(\mathbb{E}[\hat{\sigma}_{ij}(n)] - \sigma_{ij}) \to C_{ij}$ as $n\to\infty$. The condition \eqref{eq-sigma-rate-assumption} ensures the rate of convergence of $\hat{\sigma}_{ij}$ to $\sigma_{ij}$ is sufficiently rapid. Then we have that
\begin{align*}
    \lim_{n\to\infty} & k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n) \\
    &= \lim_{n\to\infty} \sum_{i<j}k(n)\left\lbrace \mathbb{E}[\hat{\sigma}_{ij}(n)^2] - \sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}(n)]\right\rbrace \\
    &= \lim_{n\to\infty} \sum_{i<j}k(n)\left\lbrace \mathbb{E}[\hat{\sigma}_{ij}(n)^2] - \mathbb{E}[\hat{\sigma}_{ij}(n)]^2 \right\rbrace + \lim_{n\to\infty}\sum_{i<j} k(n)\left\lbrace \mathbb{E}[\hat{\sigma}_{ij}(n)]^2 - \sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}(n)] \right\rbrace \\
    &= \lim_{n\to\infty} \sum_{i<j}k(n)\mathrm{Var}(\hat{\sigma}_{ij}(n)) + \lim_{n\to\infty} \sum_{i<j} \mathbb{E}[\hat{\sigma}_{ij}(n)] k(n) \left\lbrace \mathbb{E}[\hat{\sigma}_{ij}(n)] - \sigma_{ij} \right\rbrace \\
    &= \sum_{i<j}(\nu_{ij}^2 + \sigma_{ij}C_{ij}).
\end{align*}
Assuming (as we are) that the bias of the TPDM is non-negative, then $C_{ij}\geq 0$. Then, noting that $\|\hat{\bm{\sigma}}(n)\|_2^2>0$ almost surely, it follows that $\lambda^\star(n)>0$ for sufficiently large $n$. The above result gives that, asymptotically, the optimal shrinkage behaves as a constant divided by $k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]$. As the asymptotic constant is unknown, we eliminate it by taking ratios at different values of $n$. Given a fixed proportion $\kappa \in (0,1)$, it is easy to see that
\begin{equation*}
    \lim_{n\to\infty} k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n) = \sum_{i<j}(\nu_{ij}^2 + c_{ij}) =
    \lim_{n\to\infty} k(\left\lfloor \kappa n \right\rfloor)\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]\lambda^\star(\left\lfloor \kappa n \right\rfloor).
\end{equation*}
and the algebra of limits then yields
\begin{equation*}
    \lim_{n\to\infty} \frac{k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n)}{k(\left\lfloor \kappa n \right\rfloor)\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]\lambda^\star(\left\lfloor \kappa n \right\rfloor)}=1.
\end{equation*}
For sufficiently large $n$ all quantities in the denominator are non-zero, so the limit is well-defined. Moreover it holds that
\begin{equation*}
    \frac{\lambda^\star(\left\lfloor \kappa n \right\rfloor)}{\lambda^\star(n)} \approx \frac{k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]}{k(\left\lfloor \kappa n \right\rfloor)\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]}=:\zeta(n;\kappa).
\end{equation*}
In other words, the optimal shrinkage intensities at sample sizes $n$ and $\left\lfloor \kappa n \right\rfloor$ are $\lambda^\star(n)$ and $\zeta(n;\kappa)\lambda^\star(n)$, respectively. The ratio $\zeta(n;\kappa)$ describes the rate of decay of the optimal shrinkage as more data becomes available. It is easy to see that $\zeta(n,\kappa)\to 1$ when $\kappa\to 1$. Additionally $\zeta(n;\kappa)\to k(n)/k(\left\lfloor \kappa n \right\rfloor)$ as $n\to\infty$ due to consistency of $\hat{\bm{\sigma}}$. This limit will usually be some function of $\kappa$, e.g. choosing $k(n)\propto \sqrt{n}$ gives $\zeta(n;\kappa)\to \kappa^{-1/2}$ as $n\to\infty$. Crucially, $\zeta(n;\kappa)$ is defined in terms of quantities that are known or may be estimated without bias. Let $\hat{\zeta}=\hat{\zeta}(n;\kappa)$ be an estimate of $\zeta=\zeta(n;\kappa)$ to be defined later.

Knowing its asymptotic rate of decay is not sufficient to deduce $\lambda^\star(n)$. The missing piece of information is its magnitude. Consider the empirical TPDMs $\hat{\Sigma}(n)$ and $\hat{\Sigma}(\left\lfloor \kappa n \right\rfloor)$ based on samples of sizes $n$ and $\left\lfloor \kappa n \right\rfloor$, respectively. If these matrices are very similar (resp. different), then intuitively the absolute difference between $\lambda^\star(n)$ and $\lambda^\star(\left\lfloor \kappa n \right\rfloor)$ should be small (resp. large). Now, our large sample theory tells us that our best estimates of $\bm{\sigma}$ are of the form $(1-\lambda^\star(n))\hat{\bm{\sigma}}(n)$ and $(1-\hat{\zeta}\lambda^\star(n))\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)$. This hints at estimating $\lambda^\star(n)$ by minimising the discrepancy between these two estimates:
\begin{equation}\label{eq-lambda-lw-hat-initial}
    \hat{\lambda}(n) := \argmin_{\lambda\in[0,\min(1,\zeta^{-1})]} \left\lVert (1-\lambda)\hat{\bm{\sigma}}(n) -  (1-\hat{\zeta} \lambda)\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\right\rVert_2^2.
\end{equation}

### Practical and statistical considerations

Having outlined the general approach, we fill in some missing details regarding inference. First we address estimation of $\zeta$. To estimate $\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]$ and $\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]$, we produce a collection of bootstrapped samples $\mathcal{X}_n^{(b)}=\{\bm{X}_1^{(b)},\ldots,\bm{X}_n^{(b)}\}$ and $\mathcal{X}_{\left\lfloor \kappa n \right\rfloor}^{(b)}=\{\bm{X}_1^{(b)},\ldots,\bm{X}_{\left\lfloor \kappa n \right\rfloor}^{(b)}\}$ from $\bm{X}_1,\ldots,\bm{X}_n$, for $b=1,\ldots,B$. Next, we compute empirical TPDMs $\hat{\Sigma}^{(1)}(n),\ldots,\hat{\Sigma}^{(B)}(n)$ and $\hat{\Sigma}^{(1)}(\left\lfloor \kappa n \right\rfloor),\ldots,\hat{\Sigma}^{(B)}(\left\lfloor \kappa n \right\rfloor)$ using the $k(n)$ and $k(\left\lfloor \kappa n \right\rfloor)$ largest observations from $\mathcal{X}_n^{(1)},\ldots,\mathcal{X}_n^{(B)}$ and $\mathcal{X}_{\left\lfloor \kappa n \right\rfloor}^{(1)},\ldots,\mathcal{X}_{\left\lfloor \kappa n \right\rfloor}^{(B)}$, respectively. An estimate of $\zeta$ is then given by
\begin{equation}\label{eq-zeta-hat}
    \hat{\zeta}(n;\kappa) := \frac{k(n)\sum_{b=1}^B \|\hat{\bm{\sigma}}^{(b)}(n)\|_2^2}{k(\left\lfloor \kappa n \right\rfloor)\sum_{b=1}^B \|\hat{\bm{\sigma}}^{(b)}(\left\lfloor \kappa n \right\rfloor)\|_2^2}.
\end{equation}
The bootstrapped empirical TPDMs can be reused in the final step, so that instead of \eqref{eq-lambda-lw-hat-initial}, we actually set
\begin{equation}\label{eq-lambda-lw-hat-bootstrap}
    \hat{\lambda}(n) := \argmin_{\lambda\in[0,\zeta^{-1}]} \left\lVert \frac{1}{B}\sum_{b=1}^B \left[ (1-\lambda)\hat{\bm{\sigma}}^{(b)}(n) -  (1-\hat{\zeta} \lambda)\hat{\bm{\sigma}}^{(b)}(\left\lfloor \kappa n \right\rfloor)\right] \right\rVert_2^2.
\end{equation}
Bootstrapping is intended to reduce noise in the estimation of $\zeta$ and $\lambda^\star$, but it is not fundamental to our procedure. The only place where sub-sampling is mandatory is to obtain an empirical TPDM based on $\left\lfloor \kappa n \right\rfloor$ observations.

For fixed $n$, selection of $\kappa$ is subject to the familiar bias-variance trade-off. If $\kappa$ is small, then $\left\lfloor \kappa n \right\rfloor$ may be too small for the underlying asymptotic approximation to be valid. If $\kappa$ is very close to one, then the estimate $\hat{\lambda}$ will be quite sensitive to small changes in $\hat{\zeta}$. This is borne out in our simulation experiments. A formal analysis of how to select $\kappa$ is beyond the scope of our investigation.

## Simulation experiments

### Symmetric logistic {#sec-shrinkage-tpdm-experiments-sl}

We start by testing our regularised TPDM estimators in the simplest setting, where the data are generated from the symmetric logistic model. Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathrm{RV}_+^d(2)$ follows a symmetric logistic model with dependence parameter $\gamma\in(0,1]$. Then the true TPDM is of the form
\begin{equation*}
    \sigma_{ij} = \begin{cases}
        1,& i=j, \\
        \sigma,& i\neq j,
    \end{cases}
\end{equation*}
where $\sigma=\sigma(\gamma)$ is some constant -- see @exm-symmetric-logistic-tpdm. Recall from @fig-parametric-chi-tpdm-small-n that the empirical TPDM overestimates $\sigma$ when $\gamma\approx 1$. Since all off-diagonal entries of the TPDM are equal, the symmetric logistic model is one of the rare cases where penalising all entries equally is actually desirable. Assume for simplicity that $\hat{\bm{\sigma}}(n)=\hat{\sigma}(n)\bm{1}_{d\choose 2}$ for some scalar $\hat{\sigma}(n)>\sigma$. It is straightforward to show that the optimal Ledoit-Wolf shrinkage intensity is $\lambda_{\mathrm{LW}(n)}^\star=1-\sigma/\hat{\sigma}(n)$, since then
\begin{equation*}
    \tilde{\bm{\sigma}}(\lambda_{\mathrm{LW}}^\star(n)) = [1 - (1-\lambda_{\mathrm{LW}}^\star(n))] \hat{\bm{\sigma}} = \frac{\sigma}{\hat{\sigma}(n)} \hat{\sigma}(n) \bm{1}_{d\choose 2} = \bm{\sigma}.
\end{equation*}
Similarly, the optimal thresholds under soft-thresholding and adaptive lasso are 
\begin{align*}
    \lambda_\mathrm{S}^\star(n) &= \hat{\sigma}(n)-\sigma=\hat{\sigma}(n)\lambda_{\mathrm{LW}}^\star, \\ 
    \lambda_\mathrm{AL}^\star(n) &=[\hat{\sigma}(n)^\eta (\hat{\sigma}(n)-\sigma)]^{1/(\eta+1)}=\hat{\sigma}(n)[\lambda_{\mathrm{LW}}^\star(n)]^{1/(\eta+1)},
\end{align*}
respectively. Note that $\lambda_\mathrm{S}^\star$ and $\lambda_\mathrm{AL}^\star$ are expressed in terms of $\lambda_{\mathrm{LW}}^\star$. Thus, finding a good estimate for the Ledoit-Wolf shrinkage is sufficient to obtain reasonable estimates for the other regularisation parameters. On this basis, we allow ourselves to focus exclusively on testing the Ledoit-Wolf estimator. 

The setup for our experiment is as follows. We generate $n$ independent observations of $\bm{X}\in\mathrm{RV}_+^3(2)$ from a symmetric logistic model with dependence parameter $\gamma$. The intermediate sequence is set as $k(n)=\left\lfloor 4\sqrt{n} \right\rfloor$. This satisfies the rate conditions \eqref{eq-k-rate-conditions} and gives a reasonable number of threshold exceedances in practice. The ratio $\zeta(n;\kappa)$ is estimated using \eqref{eq-zeta-hat} with $B=10$ and $\hat{\lambda}$ computed via \eqref{eq-lambda-lw-hat-bootstrap} using the \texttt{optim()} function in R. Finally, we also compute the optimal shrinkage that minimises the Frobenius loss between the bootstrapped empirical TPDMs and the true TPDM; when averaged over many simulations these provide a good approximation to $\lambda^\star$. This experimental procedure is repeated for a sequence of sample sizes $5 \times 10^3 \leq n \leq 2 \times 10^5$, proportions $\kappa\in\{0.5, 0.75, 0.9\}$, and true dependence parameters $\gamma\in\{0.5, 0.8, 0.9\}$. The corresponding dependence strengths are $\sigma(0.5)=0.85$ (strong dependence), $\sigma(0.8)=0.48$ (moderately weak dependence) and $\sigma(0.9)=0.27$ (weak dependence). Results are based on 100 repetitions for each combination of parameters.

```{r make-fig-symmetric-logistic-ledoit-wolf-oracle}
#| label: fig-symmetric-logistic-ledoit-wolf-oracle
#| fig-cap: "Exploratory plots for understanding the performance of the Ledoit-Wolf TPDM in the symmetric logistic experiment. Left: the Frobenius risk $\\mathcal{R}(0)$ associated with the empirical TPDM $\\hat{\\Sigma}$ as a function of $n$. Right: $k(n)\\mathbb{E}[\\|\\hat{\\bm{\\sigma}}(n)\\|_2^2]\\lambda^\\star(n)$ as a function of $n$."
#| fig-scap: "Exploratory plots for the Ledoit-Wolf TPDM and symmetric logistic data."
#| fig-height: 3.5
#| warning: false

res <- readRDS(file.path("scripts", "shrinkage-tpdm", "results", "symmetric-logistic-ledoit-wolf.RDS"))

sl_tpdm <- function(a) { 
  sl_integrand <- function(x, a) (1 - a) / a * (x * (1-x))^(1/a - 3/2) * ((1-x)^(1/a) + x^(1/a))^(a-2) 
  if (a == 0) { 
    1 
  } else if (a == 1) { 
    0 
  } else { 
    integrate(function(x) sl_integrand(x, a = a), lower = 0, upper = 1, subdivisions = 500)$value 
  } 
} 
sl_tpdm <- Vectorize(sl_tpdm)

# Frobenius risk of Sigma hat
p1 <- ggplot(res, aes(x = n, y = E_sigma_hat_norm - (choose(d, 2) * sl_tpdm(gamma)^2), colour = as.factor(gamma))) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.02))) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression("Frobenius risk of" ~ hat(Sigma)), 
       colour = expression(gamma)) +
  theme_light()

# k * norm(Sigma_hat) * lambda_true against n
p2 <- ggplot(res, aes(x = n, y = k * E_sigma_hat_norm * lambda_star, colour = as.factor(gamma))) +
  stat_summary(fun = median, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(k(n) ~ lambda(n) ~ sum(hat(sigma)[ij](n)^2, "", "")),
       colour = expression(gamma)) +
  theme_light()

ggarrange(p1, p2, ncol = 2, legend = "right", common.legend = TRUE)
```

```{r make-fig-symmetric-logistic-ledoit-wolf-lambda-hat}
#| label: fig-symmetric-logistic-ledoit-wolf-lambda-hat
#| fig-cap: "The optimal Ledoit-Wolf shrinkage intensity $\\lambda^\\star(n)$ (solid lines) and the mean estimates $\\hat{\\lambda}(n)$ obtained from our tuning procedure (dashed/dotted lines) as a function of $n$ for various values of $\\gamma$ and $\\kappa$."
#| fig-scap: "Optimal/estimated Ledoit-Wolf shrinkage for symmetric logistic data."
#| fig-height: 4.5
#| warning: false

# lambda_true and lambda_hat against n
ggplot(res, aes(x = n, y = lambda_hat, colour = as.factor(gamma), linetype = as.factor(kappa))) +
  stat_summary(aes(y = lambda_star, colour = as.factor(gamma)), fun = median, geom = "line", linetype = 1) +
  stat_summary(fun = median, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.04))) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(hat(lambda)(n)),
       colour = expression(gamma),
       linetype = expression(kappa)) +
  theme_light()
```

The left plot in @fig-symmetric-logistic-ledoit-wolf-oracle shows the average Frobenius loss $\mathcal{R}(0)=\|\bm{\sigma} - \hat{\bm{\sigma}}(n)\|_2^2$ as a function of $n$ for each value of $\gamma$. This summarises the magnitude of the bias of the empirical TPDM; an error $\mathcal{R}(0)=\epsilon$ is equivalent to overestimating each entry of $\bm{\sigma}$ by $\epsilon/\sqrt{3}$. As expected, the magnitude of the bias is decreasing in $n$ and increasing in $\gamma$. However, we note that even at $n=200,000$ there is non-negligible bias present, showing that bias-corrected estimation has a role to play even when $n$ is large. This is an important point, considering that our procedure relies on large sample approximations. The right-hand plot shows the mean of $k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n)$ against $n$. Our tuning procedure requires that this quantity is approximately constant beyond $\left\lfloor \kappa n\right\rfloor$. For $\gamma=0.5$, the convergence is rapid, so we expect our procedure will perform well even when $n$ is small. When $\gamma=0.8$, the quantity approximately levels off at approximately $n=100,000$, while for $\gamma=0.9$ it is still increasing at $n=200,000$.

@fig-symmetric-logistic-ledoit-wolf-lambda-hat compares the mean estimates $\hat{\lambda}(n)$ for different values of $\kappa$ (dashed/dotted lines) against the oracle value $\lambda^{\star}(n)$ (solid line). The solid lines represent the true values of the optimal shrinkage parameter $\lambda(n)$. For $\gamma=0.5$, the optimal shrinkage is close to zero and the estimates $\hat{\lambda}(n)$ reflect this, even when $n$ is small. As predicted, the selection procedure performs well when $\gamma=0.8$ provided the sample size is sufficiently large. In particular, we obtain good estimates when $n\geq 150,000$ (i.e. $\left\lfloor \kappa n\right\rfloor\geq 75,000$), which aligns reasonably closely with our earlier comments concerning the convergence in @fig-symmetric-logistic-ledoit-wolf-oracle. Our tuning procedure does not work particularly well in the weakest dependence scenario ($\gamma=0.9$). While the intensities are sub-optimal, conservative estimates that err towards the empirical TPDM are probably preferable to over-shrinking. Our theoretical analysis provides assurance that these estimates would improve eventually as $n$ increases. The plot also shows the influence of the tuning parameter $\kappa$. When $\kappa$ is close to one the estimates $\hat{\lambda}(n)$ have high uncertainty, while conservative choices of $\kappa$ yield more stable but (negatively) biased estimates.

### EVA (2023) Data Challenge 4 {#sec-shrinkage-tpdm-experiments-eva-c4}

Recall from @sec-eva-data-challenge that our submission to Challenge 4 in the EVA (2023) Data Challenge utilised completely positive (CP) decompositions of the empirical TPDMs corresponding to five separate clusters. It transpired that our approach overestimated the true event probabilities of interest. Subsequent investigation revealed that we overestimated dependence. This was especially true in the first, fourth and fifth clusters, where dependence is weak [@rohrbeckEditorialEVAConference2023], leading us to suspect that the bias issue might be the root cause. In our post-hoc analysis (@sec-eva-c4-improvements), we proposed a remedy using sparse simplex projections. The intention and effect of this was to spread the mass of the empirical angular measure towards/onto the simplex boundary, thereby artificially 'weakening' the dependence. This approach showed great promise, but required wholesale changes to our overall methodology. Armed with the Ledoit-Wolf TPDM estimator, we are now in a position to propose yet another strategy. The new approach follows our original methodology exactly, except we replace the empirical TPDM with the Ledoit-Wolf counterpart. The substitution is seamless due to complete positivity of $\tilde{\Sigma}$ (@prp-lw-tpdm-properties).

Visual representations of the five clusters' empirical TPDMs are provided in @fig-eva-c4-ledoit-wolf-tpdm (left). From @tbl-eva-c4-cluster-summary in @sec-eva-data-challenge we know that the median values of $\{\hat{\sigma}_{ij}:i\neq j\}$ in each cluster are 0.33, 0.68, 0.67, 0.33, and 0.50. Crucially, within each cluster the range of $\{\hat{\sigma}_{ij}:i\neq j\}$ is at most 0.15, implying that the within-cluster pairwise dependence strengths are relatively equal. In such cases, the Ledoit-Wolf estimator is adequate. If this were not the case, more flexible regularisers such as adaptive lasso would be required. Let $\hat{\Sigma}^{(1)},\ldots,\hat{\Sigma}^{(5)}$ denote the clusters' empirical TPDMs based on the $k$ largest observations among the $n=10^4$ samples provided. We estimate shrinkage parameters $\hat{\lambda}^{(1)},\ldots,\hat{\lambda}^{(5)}$ for each cluster using our selection procedure with $k(n)=\left\lfloor k \sqrt{n}/100 \right\rfloor$, $\kappa=0.75$ and $B=10$. The remaining steps follow @sec-eva-c4-methodology: we apply the CP-factorisation algorithm of @kirilioukEstimatingProbabilitiesMultivariate2022 to each Ledoit-Wolf TPDM and compute probability estimates using the formula \eqref{eq-eva-prob-approx-formula}. 

```{r make-fig-eva-c4-ledoit-wolf-tpdm}
#| label: fig-eva-c4-ledoit-wolf-tpdm
#| fig-cap: "The empirical TPDM $\\hat{\\Sigma}=\\mathrm{diag}(\\hat{\\Sigma}^{(1)},\\ldots,\\hat{\\Sigma}^{(5)})$ (left) and the Ledoit-Wolf TPDM $\\tilde{\\Sigma}=\\mathrm{diag}(\\tilde{\\Sigma}^{(1)}(\\hat{\\lambda}_1),\\ldots,\\tilde{\\Sigma}^{(5)}(\\hat{\\lambda}_5))$ for the EVA (2023) Data Challenge based on $k=250$."
#| fig-scap: "Shrinkage parameters $\\hat{\\Sigma}$ and $\\tilde{\\Sigma}$ for the EVA (2023) Data Challenge data."
#| fig-height: 4

Sigma_hat_clusters <- readRDS("scripts/shrinkage-tpdm/results/eva-c4-Sigma-hat.RDS")
Sigma_tilde_clusters <- readRDS("scripts/shrinkage-tpdm/results/eva-c4-Sigma-tilde.RDS")

p1 <- plot_tpdm(Sigma_hat_clusters, x_labels = FALSE, y_labels = FALSE)
p2 <- plot_tpdm(Sigma_tilde_clusters, x_labels = FALSE, y_labels = FALSE)
grid.arrange(p1, p2, ncol = 2)
```

@fig-eva-c4-ledoit-wolf-lambda reveals the amount of each shrinkage for each cluster for $0.02 \leq k/n \leq 0.07$. As expected, clusters 1 and 4 are penalised the most and clusters 2 and 3 the least. The shrinkage intensities are sensitive to the choice of $k$. Counterintuitively the parameters $\hat{\lambda}^{(1)},\ldots,\hat{\lambda}^{(5)}$ seem to increase as $k$ decreases, despite the fact that the bias in the TPDM is usually smaller at higher thresholds [@fixSimultaneousAutoregressiveModels2021]. This warrants further investigation in future. For the present study, we focus on the case where $k=250$, since this is the number of threshold exceedances used in our competition entry. The corresponding Ledoit-Wolf TPDM estimate is shown in @fig-eva-c4-ledoit-wolf-tpdm (right). Comparing against the empirical TPDM on the left, there is a dramatic reduction in the entries in clusters 1 and 4. Consequently, this matrix is presumably closer to the (unknown) true TPDM than the raw estimate. The final probability estimates in @fig-eva-c4-ledoit-wolf-prob-estimates support this conclusion. In each panel, the horizontal dashed line shows the true value of the probabilities $p_1$ (left) and $p_2$ (right). The grey line corresponds to our original method based on CP-decompositions of the empirical TPDM. It drastically overestimates the event probabilities. The green and blue lines result from fitting a max-linear model using the $k$ largest angles defined via self-normalisation and Euclidean projections, respectively; these methods were described in detail in @sec-eva-multivariate. The red line is derived from our new procedure based on CP-decompositions of Ledoit-Wolf TPDMs. At $k=250$, it gives a near-perfect estimate of $p_1$ and a very good estimate of $p_2$. When $k$ is large the probability estimates are too large because the shrinkage intensities are too small, especially in the first, fourth and fifth clusters (see @fig-eva-c4-ledoit-wolf-lambda).

```{r make-fig-eva-c4-ledoit-wolf-lambda}
#| label: fig-eva-c4-ledoit-wolf-lambda
#| fig-cap: "Estimated shrinkage intensities $\\hat{\\lambda}^{(1)},\\ldots,\\hat{\\lambda}^{(5)}$ associated with the clusters' empirical TPDMs $\\hat{\\Sigma}^{(1)},\\ldots,\\hat{\\Sigma}^{(5)}$ for the EVA (2023) Data Challenge."
#| fig-scap: "$\\hat{\\lambda}^{(1)},\\ldots,\\hat{\\lambda}^{(5)}$ for the EVA (2023) Data Challenge data."
#| fig-height: 2.5
#| fig-width: 5
#| message: false

res <- readRDS(file.path("scripts", "shrinkage-tpdm", "results", "eva-c4-ledoit-wolf.RDS"))

res %>% 
  unnest_longer(lambda_hat, indices_to = "cluster", values_to = "lambda_hat_cluster") %>%
  select(k_frac, cluster, lambda_hat_cluster) %>%
  mutate(cluster = as.factor(cluster)) %>%
  ggplot(aes(x = k_frac, y = lambda_hat_cluster, colour = cluster)) +
  geom_smooth(se = FALSE) +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_percent()) +
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0.01, 0.01))) +
  scale_colour_manual(values = c("grey", "red", "blue", "darkgreen", "orange"), labels = 1:5) +
  scale_linetype_manual(values = 1:2, labels = c("p1", "p2")) +
  labs(x = expression(k/n),
       y = expression(hat(lambda)), 
       colour = "Cluster") +
  theme_light()
```


```{r make-fig-eva-c4-ledoit-wolf-prob-estimates}
#| label: fig-eva-c4-ledoit-wolf-prob-estimates
#| fig-cap: "Probability estimates for the EVA (2023) Data Challenge from all four proposed methods. The true probabilities $p_1$ (left) and $p_2$ (right) are indicated by the horizontal dashed lines. The methodologies associated with the grey, blue and green lines are described in @sec-eva-data-challenge. The red line shows the results based on CP decompositions of the Ledoit-Wolf estimates. The vertical dashed line is at $k=250$, the number of threshold exceedances used in our competition entry."
#| fig-scap: "Comparison of probability estimates for the EVA (2023) Data Challenge."
#| fig-height: 4.5
#| message: false

fancy_scientific_short <- function(l) {
  l %>%
    format(scientific = TRUE) %>%
    gsub("^(.*)e", "e", .) %>%
    gsub("e", "10^", .) %>%
    parse(text = .)
}

res %>% 
  pivot_longer(
    cols = empiricalstandard_p1:cplw_p2,
    names_to = c("method", "threshold"), 
    names_sep = "_",
    values_to = "estimate") %>%
  mutate(true_prob = case_when(threshold == "p1" ~ 8.4e-23,
                               threshold == "p2" ~ 5.4e-25)) %>%
  mutate(threshold = case_when(threshold == "p1" ~ "p[1]",
                               threshold == "p2" ~ "p[2]")) %>%
  ggplot(aes(x = k_frac, y = estimate, colour = method)) +
  geom_smooth(se = FALSE) +
  geom_hline(aes(yintercept = true_prob), linetype = "dashed") +
  geom_vline(xintercept = 0.025, linetype = "dashed") +
  facet_grid(. ~ threshold, labeller = label_parsed) +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_percent()) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.02)), transform = log10_trans(), breaks = breaks_log(n = 7), labels = fancy_scientific_short) +
  scale_colour_manual(values = c("grey", "red", "blue", "darkgreen"), labels = c("CP empirical TPDM", "CP Ledoit-Wolf TPDM", "Sparse empirical", "Empirical")) +
  scale_linetype_manual(values = 1:2, labels = c("p1", "p2")) +
  labs(x = expression(k/n),
       y = expression(hat(p)), 
       colour = "Method",
       linetype = "Threshold") +
  theme_light() +
  theme(legend.position = "top")
```

### Extremal SAR model {#sec-shrinkage-tpdm-experiments-sar}

Our final experiment emulates the simulation study in @fixSimultaneousAutoregressiveModels2021. The goal is to estimate the dependence parameter $\rho$ of the extremal SAR model (@sec-background-tpdm-fitting-max-linear), a sub-case of the max-linear model used for spatial extremes. This experiment will reinforce the effectiveness of our tuning procedure, while also highlighting the limitations of Ledoit-Wolf shrinkage compared to the more flexible thresholding estimators. Suppose $\bm{X}$ is as described in @sec-background-tpdm-fitting-max-linear, with (known) neighbourhood matrix $W$ and (unknown) dependence parameter $\rho$. Following @fixSimultaneousAutoregressiveModels2021, $\rho$ is to be estimated by minimising the Frobenius difference between the model TPDM (viewed as a function of $\rho$) and a bias-corrected estimate of the TPDM. This can be posed as a hierarchical/bilevel optimisation problem, whereby the upper-level objective function (estimating $\rho$) depends on the solution to a lower-level problem (estimating $\Sigma$). Specifically, we define
\begin{equation}\label{eq-extremal-sar-rho-bilevel}
    \hat{\rho} := \argmin_\varrho \hat{\mathcal{L}}(\varrho), \qquad \hat{\mathcal{L}}(\varrho) := \| \bm{\sigma}(\varrho) - \tilde{\bm{\sigma}}(\hat{\lambda}) \|_2^2.
\end{equation}
The loss function $\mathcal{L}(\varrho)$ depends on the estimate $\tilde{\bm{\sigma}}(\hat{\lambda})$ of $\bm{\sigma}$, so accurate estimation of the TPDM is critical. If $\tilde{\bm{\sigma}}(\hat{\lambda})$ is not representative of $\bm{\sigma}$, then the loss may not be informative for $\rho$. In @fixSimultaneousAutoregressiveModels2021, $\tilde{\bm{\sigma}}$ represents the soft-thresholded estimator and $\hat{\lambda}$ is obtained by fitting a non-linear model relating the pairwise dependence strengths and inter-site distances. The soft-thresholded estimator achieves a reasonable fit -- see Figure 3 (right) in @fixSimultaneousAutoregressiveModels2021.

Our simulation setup follows Section 4.2.1 in @fixSimultaneousAutoregressiveModels2021 with some minor adjustments. We consider $d=36$ sites arranged in an $6\times 6$ grid and generate $n$ independent realisations of a $6\times 6$ spatial field from an extremal SAR model with dependence parameter $\rho=0.15$. With this parameter choice, dependence vanishes at a distance of approximately 4 units. The empirical TPDM $\hat{\Sigma}$ is computed using the $k(n)=\left\lfloor 4\sqrt{n} \right\rfloor$ largest observations. Next, we use the true TPDM (@exm-max-linear-tpdm) to compute $\lambda^\star$ for the soft-threshold, Ledoit-Wolf, and adaptive lasso ($\eta=2$) TPDMs. From each oracle TPDM $\tilde{\bm{\sigma}}(\lambda^\star)$, we compute an estimate of $\rho$ by solving \eqref{eq-extremal-sar-rho-bilevel}. These values represent the estimates of $\rho$ that would be found by an oracle who is constrained to a particular type of regularisation. Finally, we repeat the same steps using regularisation parameters selected using our data-driven procedure (for Ledoit-Wolf with $\kappa=0.75$ and $B=5$) or the spatial method of @fixSimultaneousAutoregressiveModels2021 (for soft-threshold). Currently there are no available methods for tuning $\lambda$ and/or $\eta$ for the adaptive lasso. The procedure is repeated 100 times at a sequence of sample sizes varying between $10^4 \leq n \leq 2\times 10^5$. 

```{r make-fig-extremal-sar-regularised-tpdm}
#| label: fig-extremal-sar-regularised-tpdm
#| fig-cap: "Results for our experiments based on the extremal SAR model. Left: the mean Frobenius error of the oracle and estimated TPDMs. Middle: the values of the regularisation parameter selected by the oracle or tuning procedure. Right: the empirical bias of the estimates of $\\rho$ computed by solving \\eqref{eq-extremal-sar-rho-bilevel}."
#| fig-scap: "Results: TPDM estimation for fitting extremal SAR models."
#| fig-height: 4

res <- readRDS(file.path("scripts", "shrinkage-tpdm", "results", "extremal-sar-ledoit-wolf.RDS"))

p1 <- res %>% pivot_longer(
  cols = -c(n, k, d, rho, B, rep, kappa, k_kappa),
  names_to = c("param", "tpdm_estimator", "type"), 
  names_sep = "_",
  values_to = "value") %>%
  filter(param == "fl") %>%
  ggplot(aes(x = n, y = value, colour = tpdm_estimator, linetype = type)) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), breaks = breaks_extended(n = 5), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.02)), transform = log10_trans()) +
  scale_colour_manual(values = c("darkgreen", "grey", "red", "blue"), labels = c(expression("Adaptive lasso," ~ eta == 2), "Empirical", "Ledoit-Wolf", "Soft")) +
  scale_linetype_manual(values = 1:2, labels = c("Estimated", "Oracle")) +
  labs(x = expression(n %*% 10^{-3}),
       y = "Frobenius risk", 
       colour = "TPDM estimator",
       linetype = "Type") +
  theme_light()

p2 <- res %>% pivot_longer(
  cols = -c(n, k, d, rho, B, rep, kappa, k_kappa),
  names_to = c("param", "tpdm_estimator", "type"), 
  names_sep = "_",
  values_to = "value") %>%
  filter(param == "lambda") %>%
  ggplot(aes(x = n, y = value, colour = tpdm_estimator, linetype = type)) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), breaks = breaks_extended(n = 5), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.02))) +
  scale_colour_manual(values = c("darkgreen", "red", "blue"), labels = c(expression("Adaptive lasso," ~ eta == 1), "Ledoit-Wolf", "Soft")) +
  scale_linetype_manual(values = 1:2, labels = c("Estimated", "Oracle")) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(lambda(n)), 
       colour = "TPDM estimator",
       linetype = "Type") +
  theme_light()

p3 <- res %>% pivot_longer(
  cols = -c(n, k, d, rho, B, rep, kappa, k_kappa),
  names_to = c("param", "tpdm_estimator", "type"), 
  names_sep = "_",
  values_to = "value") %>%
  filter(param == "rho") %>%
  ggplot(aes(x = n, y = value - rho, colour = tpdm_estimator, linetype = type)) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), breaks = breaks_extended(n = 5), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_colour_manual(values = c("darkgreen", "grey", "red", "blue"), labels = c(expression("Adaptive lasso," ~ eta == 1), "Empirical", "Ledoit-Wolf", "Soft")) +
  scale_linetype_manual(values = 1:2, labels = c("Estimated", "Oracle")) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(hat(rho) - rho), 
       colour = "TPDM estimator",
       linetype = "Type") +
  theme_light()

ggarrange(p1, p2, p3, nrow = 1, ncol = 3, common.legend = TRUE, legend = "top")
```

The left-hand plot in @fig-extremal-sar-regularised-tpdm shows how the Frobenius loss (on a log scale) varies with $n$ for each oracle/estimated TPDM. The colours correspond to the regularisation method used; the line type indicates whether the regularisation parameter is estimated or an oracle based on knowledge of the true TPDM. By definition, the risk attained by the oracle losses provides a lower bound on the risk that can be attained for a given estimator class. If the solid line lies close to the oracle loss, then $\lambda$ has been well selected. In this respect, the blue lines show that the soft-threshold tuning procedure of @fixSimultaneousAutoregressiveModels2021 performs very well, with the usual caveat that it makes use of extra information to do so. Moreover, the soft-thresholded estimates are very close to the true TPDM in Frobenius norm. For Ledoit-Wolf (red) our data-driven procedure gives reasonably good results provided $n$ is large. The Ledoit-Wolf estimates are worse than the soft-threshold or adaptive lasso, but are still far superior to the empirical estimate. The middle plot provides more insight into the selected values of $\lambda$. The soft-threshold parameter is very close to the oracle value. Our shrinkage parameters also track the oracle quite closely, even for small $n$, though these small errors seem to translate into non-negligible errors in the overall Frobenius norm. The right-hand plot examines the empirical bias $\hat{\rho}-\rho$ in the final estimates of the spatial dependence parameter. The empirical TPDM (grey) overestimates dependence and wildly overestimates $\rho$. The soft-threshold estimate is interesting, with the estimator of @fixSimultaneousAutoregressiveModels2021 actually outperforming the oracle. This is not a contradiction: a smaller Frobenius loss does not guarantee a better estimate of $\rho$. @fixSimultaneousAutoregressiveModels2021 found that their estimator exhibited a slight negative bias for $\rho$. Our results confirm this and provide additional insight into why this occurs. Since the oracle suffers the same issue, we can infer that the bias is due to misspecification (i.e. being restricted to the soft-thresholded TPDMs) rather than a deficiency of their tuning procedure. As predicted, the Ledoit-Wolf estimator performs comparatively poorly and generally underestimates $\rho$, especially when $n$ is small. However, we reiterate that this is a shortcoming of using a restrictive shrinkage method and not due to our our tuning procedure, since the oracle estimates (dashed line) are no better than our estimated (solid line). Adaptive lasso achieves comparable results to soft-thresholding, except the sign of the bias is reversed. There is scope for further improvement by varying the down-weighting hyperparameter $\eta$. 

## Conclusions and outlook

The main contributions of this chapter are to counteract the bias issue of the empirical TPDM by proposing two classes of bias-corrected estimators. The first class, based on thresholding, generalises the estimator proposed in @fixSimultaneousAutoregressiveModels2021 and includes more flexible regularisers, such as the adaptive lasso. The second class employs linear shrinkage to reduce the bias. While the restrictive form of this estimator limits its applicability, a key advantage is that it possesses nice mathematical properties that enable it to be used immediately in existing TPDM-based modelling frameworks. Moreover, we constructed a data-driven statistical procedure for selecting the associated hyperparameter. The procedure is underpinned by asymptotic approximations, but our simulation experiments demonstrate reasonable performance with finite sample sizes in a wide range of settings.

There is ample opportunity to develop these methods further. Establishing a general solution to the problem of data-driven tuning for regularised estimators would be a significant breakthrough. Until this achieved, one might consider ways of enhancing the flexibility of the Ledoit-Wolf estimator, e.g. is it possible to apply different levels of shrinkage to groups of similarly sized entries in $\hat{\Sigma}$? Finally, one might take a broader perspective and consider alternative loss functions instead of the Frobenius risk. For example, if one is interested in applying the TPDM as part of a principal components analysis, a more appropriate criterion might be some measure of agreement between the principal eigenspaces of $\Sigma$ and $\tilde{\Sigma}(\lambda)$, such as the $K_p$ coefficient [@krzanowskiBetweenGroupsComparisonPrincipal1979]. 
