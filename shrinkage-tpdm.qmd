# Bias-corrected estimation of the TPDM

```{r shrinkage-tpdm-load-packages}
#| include: false
library(tidyverse)
library(tidyr)
library(magrittr)
library(scales)
library(ggh4x)
library(ggpubr)
library(colorspace)
library(kableExtra)
library(reshape2)
library(mev)
library(Matrix)
library(gridExtra)

options(dplyr.summarise.inform = FALSE)
options(knitr.kable.NA = "")
```


## Introduction and motivation

*To do*

## Regularised estimation: thresholding and shrinkage

In high-dimensional settings, where the number of variables is comparable to the number of observations, the empirical covariance matrix is noisy and poorly conditioned. Regularisation may be used to obtain more stable and accurate estimates. Shrinkage estimation achieves this by shrinking the empirical estimate towards a biased but highly structured target [@ledoitImprovedEstimationCovariance2003]. Thresholded estimators enforce sparsity by shrinking small values towards zero [@rothmanGeneralizedThresholdingLarge2009]. This improves stability and interpretability in cases where many variables are weakly correlated. We review these techniques before applying them for TPDM estimation.

### Thresholding

Thresholded estimators are founded on an underlying thresholding operator. Since we are interested in later applying these techniques to TPDMs, we define the various function classes assuming non-negative inputs.

:::{#def-thresholding-operator}
For $\lambda\geq 0$, a function $s_\lambda:\R_+\to\R_+$ is called a thresholding operator if it satisfies the following conditions for all $x\in\R_+$:

1. $s_{\lambda}(x)\leq x$;
2. $x\leq\lambda \implies s_\lambda(x)=0$;
3. $|s_\lambda(x)-x|\leq \lambda$.
:::

The parameter $\lambda$ controls the amount of shrinkage/regularisation applied. In the two extremes case, we have $s_0(x)=x$ and $s_\infty(x)=0$ for all $x\in\R_+$. Condition (i) states that $s_\lambda$ should shrink $x$ towards zero. Condition (ii) enforces a threshold below which values are mapped to zero. This induces sparsity in the outputs. Condition (iii) bounds the maximal amount of shrinkage. For simplicity, we assume the threshold and maximal shrinkage are equal. This condition may be relaxed by introducing separate parameters $\lambda_1$ and $\lambda_2$ if desired [@antoniadisRegularizationWaveletApproximations2001]. Thresholding operators can be equivalent formulated as solutions to regularised optimisation problems of the form
\begin{equation}\label{eq-thresholding-regularisation-problem}
    s_\lambda(x) = \argmin_{y\in\R_+}\left[\frac{1}{2}(y-x)^2 + p_\lambda(y;x)\right]
\end{equation}
for some penalty function $p_\lambda$. Describing threshold operators in this way provides insight into their behaviour.

We consider three popular thresholding operators: hard-thresholding, soft-thresholding, and adaptive lasso. The hard-thresholding rule 
\begin{equation}
    s_\lambda^H(x) = x\ind\{x > \lambda\}
\end{equation}
applies no shrinkage to values above $\lambda$. Its diametric opposite is the soft-thresholding operator
\begin{equation}
    s_\lambda^S(x) = (x-\lambda)_+,
\end{equation}
which induces the maximal amount of shrinkage permitted by condition (iii). Soft-thresholding 
corresponds to solving \eqref{eq-thresholding-regularisation-problem} with lasso penalty function $p_\lambda(y;x)=\lambda y$. Equal shrinkage is applied to large and small values (above the threshold) alike, since $p_\lambda(y;x)$ does not depend on $x$. Other thresholding operators offer a compromise between these two cases. We consider the adaptive lasso, whose penalty function $p_\lambda(y;x)=\lambda^{\eta+1}yx^{-\eta}$ is down-weighted for larger values. The thresholding rule is given by
\begin{equation}\label{eq-adaptive-lasso}
    s_{\lambda,\eta}^{AL}(x) = (x-\lambda^{\eta+1}x^{-\eta})_+, \qquad (\eta \geq 0).
\end{equation}
The degree of down-weighting is controlled by the tuning parameter $\eta\geq 0$. Setting $\eta=0$ results in no down-weighting, i.e. $s_{\lambda,0}^{AL}(x)=s_\lambda^S(x)$. On the other hand, $s_{\lambda,\eta}^{AL}(x)\to s_\lambda^H(x)$ as $\eta\to\infty$. Thus, adaptive lasso is sandwiched between soft- and hard-thresholding. The clipped $L_1$-penalty and the smoothly clipped absolute deviations (SCAD) penalty are other alternatives that work in a similar way [@antoniadisRegularizationWaveletApproximations2001].

Let $A=(a_{ij})\in\R_+^{m\times n}$ be a non-negative matrix and $s_\lambda:\R_+\to\R_+$ a thresholding operator. The thresholded matrix $s_\lambda(A):=(s_\lambda(a_{ij}))$ is simply the result of element-wise application of $s_\lambda$ to $A$. Thresholded matrices can be computed very efficiently. Note that thresholding does not preserve certain properties, including positive semi-definiteness.

@rothmanGeneralizedThresholdingLarge2009 analyse the properties of the thresholded empirical covariance matrix $s_\lambda(\hat{\Sigma})$. (Here the thresholding rule is suitably extended to cater for negative inputs.) Under certain assumptions, they prove that $s_\lambda(\hat{\Sigma})$ is `sparsistent'. This means that, as well as being consistent for the covariance matrix $\Sigma$, it identifies true (non-)zeros as (non-)zeros with probability tending to one.

### Ledoit-Wolf shrinkage

The simplest shrinkage method is Ledoit-Wolf linear shrinkage [@ledoitImprovedEstimationCovariance2003]. The key idea is to bias the sample covariance matrix $\hat{\Sigma}$ towards some pre-specified target matrix $T$. This is achieved by forming a convex linear combination $\lambda T + (1-\lambda)\hat{\Sigma}$ of $T$ and $\hat{\Sigma}$. The shrinkage intensity $\lambda\in[0,1]$ determines the weight allocated to each matrix. If $\lambda=0$, then no shrinkage occurs. In the context of covariance matrix estimation, the resulting matrix is noisy but unbiased. If $\lambda=1$, then the estimator reverts to $T$. This is biased but perfectly stable. The choice of target depends on the context at hand. 

## Regularised TPDM estimators

We are now ready to apply thresholding and shrinkage to the TPDM. Our objective is multiple: we wish to mitigate against variability/instability and reduce the bias. The importance of the former objective should not be disregarded, since in multivariate extremes the effective sample size $k$ is often of comparable size to $d$. However, we focus our discussion on tackling the bias. Let $\hat{\Sigma}$ be an empirical estimate of some TPDM $\Sigma$. Our regularised TPDM estimators are defined as follows.

:::{#def-thresholded-tpdm}
Let $s_\lambda$ be a thresholding operator. For $\lambda\geq 0$, the thresholded TPDM is
\begin{equation}\label{eq-thresholded-empirical-tpdm}
  \tilde{\Sigma}(\lambda) = (\tilde{\sigma}_{ij}(\lambda)), \qquad 
    \tilde{\sigma}_{ij} = 
    \begin{cases}
      \hat{\sigma}_{ij}, & i=j, \\
    s_\lambda(\hat{\sigma}_{ij}), & i\neq j.
    \end{cases}
\end{equation}
:::

:::{#def-lw-tpdm}
For $\lambda\in[0,1]$, the Ledoit-Wolf TPDM is
\begin{equation}\label{eq-thresholded-lw-tpdm}
  \tilde{\Sigma}(\lambda) = \lambda I + (1-\lambda)\hat{\Sigma}.
\end{equation}
:::

Both estimators are defined so as to target regularisation towards the off-diagonal entries. The thresholded estimator is explicitly instructed to leave the diagonal elements unaltered. For the Ledoit-Wolf estimator this occurs more organically. The diagonal elements are pushes towards the diagonal elements of the identity, i.e. one. Assuming as we do that the margins are standardised, this is a sensible choice. In any case, the performance of TPDM estimators will be measured with respect to the strictly upper-triangular entries only. Thus we will refer to the TPDM $\Sigma$ (resp. $\hat{\Sigma}$, $\tilde{\Sigma}(\lambda)$) and its upper-half vectorised counterpart $\bm{\sigma}$ (resp. $\hat{\bm{\sigma}}$, $\tilde{\bm{\sigma}}(\lambda)$) interchangeably.

Choosing between thresholding or shrinkage involves practical and mathematical considerations. The threshold-based estimators afford more flexibility via the specification of the thresholding rule. Ledoit-Wolf is very crude in contrast, reducing each entry by the same multiplicative factor $1-\lambda$. However, this simplicity admits nice properties and permits a more detailed study into its mathematical behaviour.

:::{#prp-lw-tpdm-properties}
The Ledoit-Wolf TPDM $\tilde{\Sigma}(\lambda)$ is symmetric, positive semi-definite, and completely positive.
:::

:::{.proof}
For brevity, we suppress dependence on $\lambda$. Symmetry is obvious: for any $i,j$,
\begin{equation*}
    \tilde{\sigma}_{ij} = \lambda \ind\{i=j\} + (1-\lambda)\hat{\sigma}_{ij} = \lambda \ind\{j=i\} + (1-\lambda) \hat{\sigma}_{ji} = \tilde{\sigma}_{ji}.
\end{equation*}
For positive semi-definiteness, recall that $\hat{\Sigma}$ is positive semi-definite and so for any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation*}
  \bm{y}^T\tilde{\Sigma}\bm{y} = \lambda \bm{y}^T\bm{y} + (1-\lambda)\bm{y}^T\hat{\Sigma}\bm{y} = \lambda \|\bm{y}\|_2^2 + (1-\lambda)\bm{y}^T\hat{\Sigma}\bm{y} \geq 0.
\end{equation*}
The class of completely positive matrices is a convex cone (CITE Berman \& Shaked-Monderer (2003)), so complete positivity is preserved under convex combinations. The identity matrix is trivially completely positive since $I=II^T$ and the empirical TPDM is completely positive by @prp-empirical-tpdm-completely-positive.
:::

This result says that the Ledoit-Wolf estimator possesses the properties of a TPDM. In particular, complete positivity implies that there exists a max-linear random vector whose TPDM is $\tilde{\Sigma}$. The same cannot be said of threshold estimators. For example, positive semi-definiteness is not preserved under thresholding.

## Selecting the regularisation parameter

Statistically, the main challenge is to select the regularisation parameter $\lambda$. A natural criterion is to choose $\lambda$ to minimise the discrepancy between the true TPDM $\Sigma$ and the estimate. For a given class of regularised estimators, the risk associated with the parameter $\lambda$ is defined as
\begin{equation}\label{eq-regularisation-frobenius-loss}
    \mathcal{R}(\lambda):=
    \mathbb{E}[\|\tilde{\bm{\sigma}}(\lambda) - \bm{\sigma}\|_2^2]
    =\mathbb{E}\left[\sum_{i<j}\left( \tilde{\sigma}_{ij}(\lambda) - \sigma_{ij} \right)^2\right].
\end{equation}
We call this the Frobenius risk. It is related to $\mathbb{E}[\|\tilde{\Sigma}(\lambda)-\Sigma \|_F^2]$ but restricted to the strictly upper-triangular entries. Minimising the Frobenius risk enforces that estimates are close to the truth in mean-squared error. This metric seems appropriate in general contexts where `accurate' TPDM estimates are sought. 

The ground truth is never known in practice, so the risk $\mathcal{R}(\lambda)$ cannot be evaluated or minimised directly. We emphasise that naively replacing $\Sigma$ with the known matrix $\hat{\Sigma}$ in \eqref{eq-regularisation-frobenius-loss} does not mitigate this issue: the minimiser would always be $\lambda=0$. We refer to
\begin{equation*}
    \lambda^\star := \argmin_{\lambda}\mathcal{R}(\lambda)
\end{equation*}
as the oracle parameter. The name signifies that it is the value that would be selected by an oracle with access to the ground truth. The goal is to devise statistical procedures for selecting $\lambda$, i.e. estimators $\hat{\lambda}$ of $\lambda^\star$.

The problem becomes much simpler if we are privy to additional information about the data-generating process. For example, @fixSimultaneousAutoregressiveModels2021 infer the soft-thresholding parameter $\lambda=\beta_2$ in \eqref{eq-fix-bias-corrected-tpdm} by exploiting prior information about the configuration of the spatial locations and the `range' of the modelled phenomenon. Clearly, this is only viable in spatial contexts. A similar argument can be made for supervised learning problems. With labelled training data and an objective loss function, we may resort to standard hyperparameter tuning procedures such as $k$-fold cross validation. We address the problem in its most general, challenging form, where no additional structure/information is available.

### Why are standard Frobenius risk minimisation approaches not viable?

For covariance matrices, a standard strategy for selecting the regularisation parameter is to minimise an empirical estimate of the Frobenius loss function, i.e. an approximation of $\mathcal{R}(\lambda)$ constructed using the data [@yiSUREtunedTaperingEstimation2013]. Unfortunately, this strategy does not transfer easily to the TPDM context. Let $\tilde{\Sigma}(\lambda)$ be an arbitrary regularised TPDM estimator. Using Stein's identity (CITE)
\begin{equation*}
(\tilde{\sigma}_{ij}(\lambda) - \sigma_{ij})^2 = (\tilde{\sigma}_{ij}(\lambda) - \hat{\sigma}_{ij})^2 + 2 (\tilde{\sigma}_{ij}(\lambda) - \sigma_{ij})(\hat{\sigma}_{ij}(\lambda) - \sigma_{ij}) - (\hat{\sigma}_{ij} - \sigma_{ij})^2
\end{equation*}
the risk function \eqref{eq-regularisation-frobenius-loss} can be equivalently expressed as
\begin{equation}
    \mathcal{R}(\lambda) = \mathbb{E}[\|\tilde{\bm{\sigma}}(\lambda) - \hat{\bm{\sigma}}\|_2^2] + 2\sum_{i<j}\mathbb{E}[(\tilde{\sigma}_{ij}(\lambda) - \sigma_{ij})(\hat{\sigma}_{ij} - \sigma_{ij})] - \mathbb{E}[\|\hat{\bm{\sigma}} - \bm{\sigma}\|_2^2]
\end{equation}
These three terms are called apparent error, optimism, and constant. The apparent error acts as a penalty against departing too far away from the empirical TPDM. It can be estimated using the unbiased estimator $\|\tilde{\bm{\sigma}}(\lambda) - \hat{\bm{\sigma}}\|_2^2$. The constant term does not depend on $\lambda$. It is irrelevant for risk minimisation and may be ignored. Ironically, the fundamental issue stems from the optimism term. The empirical covariance matrix is unbiased, so the summand may be replaced with $\mathrm{Cov}(\tilde{\sigma}_{ij}(\lambda),\hat{\sigma}_{ij})$. This quantity may be estimated using bootstrap methods [@fangTuningparameterSelectionRegularized2016]. Bias in the empirical TPDM precludes us from taking a similar course. 

Overcome this problem would enable tuning of any regularised estimator. Sadly, a solution eluded us. Admittedly, this somewhat restricts the scope of our study: without automated tuning procedures, it becomes difficult to perform meaningful analyses. However, we were to able to make progress in the special case of the Ledoit-Wolf estimator. Our approach is described in the following section and the Ledoit-Wolf TPDM becomes our primary focus.

### The asymptotically optimal Ledoit-Wolf shrinkage

@ledoitImprovedEstimationCovariance2003 derive a formula for the optimal Ledoit-Wolf shrinkage for covariance matrices. Optimality of their estimator holds with respect to the Frobenius risk and as $n\to\infty$. We are able to a similar result for TPDM shrinkage. Let $\tilde{\Sigma}(\lambda)$ be the Ledoit-Wolf estimator of the TPDM. The associated Frobenius risk is
\begin{align*}
    \mathcal{R}(\lambda) 
    &= \mathbb{E}[\|(1-\lambda)\hat{\bm{\sigma}} - \bm{\sigma}\|_2^2] \\
    &= \sum_{i<j} \mathbb{E}\left[\left( (1-\lambda)\hat{\sigma}_{ij} - \sigma_{ij} \right)^2\right] \\
    &= \sum_{i<j} \left\lbrace (1-\lambda)^2\mathbb{E}[\hat{\sigma}_{ij}^2] - 2(1-\lambda)\sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}] + \sigma_{ij}^2 \right\rbrace.
\end{align*}
The first-order optimality condition gives
\begin{equation*}
    \mathcal{R}'(\lambda) = 0 
    \implies \sum_{i<j} 2(\lambda-1)\mathbb{E}[\hat{\sigma}_{ij}^2] + 2\sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}] = 0 
    \implies \lambda^\star := \frac{\sum_{i<j}\left\lbrace\mathbb{E}[\hat{\sigma}_{ij}^2] - \sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}]\right\rbrace}{\mathbb{E}[\|\hat{\bm{\sigma}}\|_2^2]}.
\end{equation*}
It is simple to check that this is indeed a minimiser, since $\mathcal{R}''(\lambda)=\sum_{i<j}2\mathbb{E}[\hat{\sigma}_{ij}^2]>0$. As always, the oracle $\lambda^\star$ still depends on the true TPDM, so it cannot be computed directly. However, we may recognise the numerator of $\lambda^\star$ as being related to the variance of $\hat{\sigma}_{ij}$ and recall from @prp-empirical-tpdm-normality-entries that the asymptotic variance of $k\hat{\sigma}_{ij}$ is $\nu_{ij}^2$. For clarity, we now include dependence on $n$ explicitly. Under the conditions of @prp-empirical-tpdm-normality-entries, we have that
\begin{equation}\label{eq-lambda-lw-limit}
    \lim_{n\to\infty} k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n) = \lim_{n\to\infty} \sum_{i<j}k(n)\left\lbrace \mathbb{E}[\hat{\sigma}_{ij}(n)^2] - \sigma_{ij}\mathbb{E}[\hat{\sigma}_{ij}(n)]\right\rbrace = \sum_{i<j}\nu_{ij}^2.
\end{equation}
Since $\sum_{i<j}\nu_{ij}^2>0$ and $\|\hat{\bm{\sigma}}(n)\|_2^2>0$ almost surely, it follows that $\lambda^\star(n)>0$ for sufficiently large $n$. Equation \eqref{eq-lambda-lw-limit} says that, asymptotically, the optimal shrinkage behaves as a constant divided by $k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]$. The asymptotic constant is unknown. Naively, we might try estimating the asymptotic variances $\nu_{ij}^2$. A consistent estimator is given (and used) in Chapter XX. However, with finite samples $\hat{\nu}_{ij}^2$ is biased in a way that directly reflects the bias in the empirical TPDM. Therefore this is not a workable solution. (In the context of covariance matrices, the asymptotic constant can be estimated without bias, which greatly simplifies matters.)

Estimation of the asymptotic constant may be circumvented by taking ratios of the left-hand side in \eqref{eq-lambda-lw-limit} at different values of $n$. Fix a proportion $\kappa\in (0,1)$. Then by \eqref{eq-lambda-lw-limit},
\begin{equation*}
    \lim_{n\to\infty} k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n) = \sum_{i<j}\nu_{ij}^2 =
    \lim_{n\to\infty} k(\left\lfloor \kappa n \right\rfloor)\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]\lambda^\star(\left\lfloor \kappa n \right\rfloor).
\end{equation*}
The algebra of limits yields
\begin{equation*}
    \lim_{n\to\infty} \frac{k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n)}{k(\left\lfloor \kappa n \right\rfloor)\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]\lambda^\star(\left\lfloor \kappa n \right\rfloor)}=1.
\end{equation*}
Note that all quantities in the denominator are non-zero for sufficiently large $n$, so the limit is well-defined. For sufficiently large $n$, it holds that
\begin{equation*}
    \frac{\lambda^\star(\left\lfloor \kappa n \right\rfloor)}{\lambda^\star(n)} \simeq \frac{k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]}{k(\left\lfloor \kappa n \right\rfloor)\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]}=:\zeta(n;\kappa).
\end{equation*}
This implies that the optimal shrinkage intensities at sample sizes $n$ and $\left\lfloor \kappa n \right\rfloor$ are $\lambda^\star(n)$ and $\zeta(n;\kappa)\lambda^\star(n)$, respectively. The ratio $\zeta(n;\kappa)$ describes the rate of decay of the optimal shrinkage as more data becomes available. It is easy to see that $\zeta(n,\kappa)\to 1$ when $\kappa\to 1$. Additionally $\zeta(n;\kappa)\to k(n)/k(\left\lfloor \kappa n \right\rfloor)$ as $n\to\infty$ due to consistency of $\hat{\bm{\sigma}}$. This limit will usually be some function of $\kappa$, e.g. choosing $k(n)\propto \sqrt{n}$ gives $\zeta(n;\kappa)\to \kappa^{-1/2}$ as $n\to\infty$. Crucially, $\zeta(n;\kappa)$ is defined in terms of quantities that are known or may be estimated without bias. Let $\hat{\zeta}=\hat{\zeta}(n;\kappa)$ be an estimate of $\zeta=\zeta(n;\kappa)$ (to be defined later).

Knowing its rate of decay is not sufficient to infer $\lambda^\star$. The missing piece is information about its magnitude. Consider the empirical TPDMs $\hat{\Sigma}(n)$ and $\hat{\Sigma}(\left\lfloor \kappa n \right\rfloor)$ based on samples of sizes $n$ and $\left\lfloor \kappa n \right\rfloor$, respectively. The difference between these matrices provides some information about the difference between $\lambda^\star(n)$ and $\lambda^\star(\left\lfloor \kappa n \right\rfloor)$. Intuitively, if these matrices are very similar (resp. different), then the absolute difference between $\lambda^\star(n)$ and $\lambda^\star(\left\lfloor \kappa n \right\rfloor)$ is probably small (resp. large). For any $\lambda\in[0,\min\{1,\zeta^{-1}\}]$, the corresponding regularised estimates of $\bm{\sigma}$ are $(1-\lambda)\hat{\bm{\sigma}}(n)$ and $(1-\zeta \lambda)\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)$. If $\lambda$ is well-selected, then both estimates should be close to $\bm{\sigma}$, and therefore close to each other. (Accounting for misspecification error, it is perhaps more accurate to say that both estimates should be close the best representation of $\bm{\sigma}$ among the class of Ledoit-Wolf estimators, and therefore close to each other.) It suggests itself to estimate $\lambda^\star(n)$ by minimising the discrepancy between the two estimates:
\begin{equation}\label{eq-lambda-lw-hat-initial}
    \hat{\lambda} := \argmin_{\lambda\in[0,\min\{1,\zeta^{-1}\}]} \left\lVert (1-\lambda)\hat{\bm{\sigma}}(n) -  (1-\hat{\zeta} \lambda)\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\right\rVert_2^2
\end{equation}
Having outlined the general approach, we fill in some missing details regarding inference. First we address estimation of $\zeta$. To estimate $\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]$ and $\mathbb{E}[\|\hat{\bm{\sigma}}(\left\lfloor \kappa n \right\rfloor)\|_2^2]$, we create a collection of bootstrapped samples $\mathcal{X}_n^{(b)}=\{\bm{X}_1^{(b)},\ldots,\bm{X}_n^{(b)}\}$ and $\mathcal{X}_{\left\lfloor \kappa n \right\rfloor}^{(b)}=\{\bm{X}_1^{(b)},\ldots,\bm{X}_{\left\lfloor \kappa n \right\rfloor}^{(b)}\}$ from $\bm{X}_1,\ldots,\bm{X}_n$, for $b=1,\ldots,B$. Next, we compute empirical TPDMs $\hat{\Sigma}^{(1)}(n),\ldots,\hat{\Sigma}^{(B)}(n)$ and $\hat{\Sigma}^{(1)}(\left\lfloor \kappa n \right\rfloor),\ldots,\hat{\Sigma}^{(B)}(\left\lfloor \kappa n \right\rfloor)$ using the $k(n)$ and $k(\left\lfloor \kappa n \right\rfloor)$ largest observations from $\mathcal{X}_n^{(1)},\ldots,\mathcal{X}_n^{(B)}$ and $\mathcal{X}_{\left\lfloor \kappa n \right\rfloor}^{(1)},\ldots,\mathcal{X}_{\left\lfloor \kappa n \right\rfloor}^{(B)}$, respectively. We estimate $\zeta$ as
\begin{equation}\label{eq-zeta-hat}
    \hat{\zeta}(n;\kappa) := \frac{k(n)\sum_{b=1}^B \|\hat{\bm{\sigma}}^{(b)}(n)\|_2^2}{k(\left\lfloor \kappa n \right\rfloor)\sum_{b=1}^B \|\hat{\bm{\sigma}}^{(b)}(\left\lfloor \kappa n \right\rfloor)\|_2^2}.
\end{equation}
The bootstrapped empirical TPDMs can be reused in the final step. Instead of \eqref{eq-lambda-lw-hat-initial}, we set
\begin{equation}\label{eq-lambda-lw-hat-bootstrap}
    \hat{\lambda} := \argmin_{\lambda\in[0,\zeta^{-1}]} \left\lVert \frac{1}{B}\sum_{b=1}^B \left[ (1-\lambda)\hat{\bm{\sigma}}^{(b)}(n) -  (1-\hat{\zeta} \lambda)\hat{\bm{\sigma}}^{(b)}(\left\lfloor \kappa n \right\rfloor)\right] \right\rVert_2^2.
\end{equation}
Bootstrapping is intended to reduce noise in the estimation of $\zeta$ and $\lambda^\star$, but it is not fundamental to our procedure. The only place where sub-sampling is mandatory is to obtain an empirical TPDM based on $\left\lfloor \kappa n \right\rfloor$ observations.

For fixed $n$, selection of $\kappa$ is subject to the familiar bias-variance trade-off. If $\kappa$ is too small, then $\left\lfloor \kappa n \right\rfloor$ will be small and the underlying asymptotic approximation may not be valid. If $\kappa$ is very close to one, then $\zeta$ is close to one and needs to be estimated very precisely. In this case, small differences in $\hat{\zeta}$ translate into large fluctuations in $\hat{\zeta}$. This will translate into high variability in $\hat{\lambda}$. This is borne out in our simulation experiments. A formal analysis of how to select $\kappa$ is beyond the scope of our investigation.


## Simulation experiments

### Symmetric logistic

We start by testing our regularised TPDM estimators in the simplest setting, where the data are generated from the symmetric logistic model. Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathrm{RV}_+^d(2)$ follows a symmetric logistic model with dependence parameter $\gamma\in[0,1)$. Then the true TPDM is of the form
\begin{equation*}
    \sigma_{ij} = \begin{cases}
        1,& i=j, \\
        \sigma,& i\neq j,
    \end{cases}
\end{equation*}
where $\sigma=\sigma(\gamma)$ is some constant given by \eqref{ex-symmetric-logistic-tpdm}. When $\gamma\approx 1$, the empirical TPDM overestimates $\sigma$. This is one of the rare instances where applying equal shrinkage to all off-diagonal entries is a sensible approach. Therefore the soft-threshold, adaptive lasso, and Ledoit-Wolf estimators are all suitable choices. Assume for simplicity that $\hat{\bm{\sigma}}(n)=\hat{\sigma}(n)\bm{1}$ for some scalar $\hat{\sigma}(n)>\sigma$. For Ledoit-Wolf, optimal shrinkage occurs at $\lambda_{\mathrm{LW}}^\star(n)=1-\sigma/\hat{\sigma}(n)$, since then
\begin{equation*}
    \tilde{\bm{\sigma}}(\lambda_{\mathrm{LW}}^\star(n)) = [1 - (1-\lambda_{\mathrm{LW}}^\star(n))] \hat{\bm{\sigma}} = \frac{\sigma}{\hat{\sigma}(n)} \hat{\sigma}(n) \bm{1} = \bm{\sigma}.
\end{equation*}
Similarly, the optimal thresholds under soft-thresholding and adaptive lasso are 
\begin{align*}
    \lambda_\mathrm{S}^\star &= \hat{\sigma}(n)-\sigma=\hat{\sigma}(n)\lambda_{\mathrm{LW}}^\star, \\ 
    \lambda_\mathrm{AL}^\star &=[\hat{\sigma}(n)^\eta (\hat{\sigma}(n)-\sigma)]^{1/(\eta+1)}=\hat{\sigma}(n)[\lambda_{\mathrm{LW}}^\star(n)]^{1/(\eta+1)},
\end{align*}
respectively. Hence, a good estimate for the Ledoit-Wolf shrinkage parameter gives good estimates for the other regularisation parameters for free. Therefore, we may focus our attention on the Ledoit-Wolf case.

We now describe our experimental procedure. We generate $n$ independent observations of $\bm{X}\in\mathrm{RV}_+^3(2)$ from a symmetric logistic model with dependence parameter $\gamma$. We set $k(n)=\left\lfloor 4\sqrt{n} \right\rfloor$. This satisfies the rate conditions \eqref{eq-k-rate-conditions} and gives a reasonable number of threshold exceedances in practice. Next, we estimate $\zeta(n;\kappa)$ using \eqref{eq-zeta-hat} with $B=10$ and compute $\hat{\lambda}$ via \eqref{eq-lambda-lw-hat-bootstrap} using the \texttt{optim()} function in R. We also compute the `true' optimal shrinkage by minimising the Frobenius loss between the bootstrapped empirical TPDMs and the true TPDM. This process is repeated for a sequence of values $5 \times 10^3 \leq n \leq 2 \times 10^5$ and $\kappa\in\{0.5, 0.75, 0.9\}$. The true dependence parameters are $\gamma\in\{0.5, 0.8, 0.9\}$. The corresponding dependence strengths $\sigma(\gamma)$ are 0.85 (strong dependence), 0.48 (moderately weak dependence) and 0.27 (weak dependence). All reported values are based on 100 repetitions.

```{r make-fig-symmetric-logistic-ledoit-wolf}
#| label: fig-symmetric-logistic-ledoit-wolf
#| fig-cap: "Blah"
#| fig-scap: "Blah"
#| fig-height: 7
#| warning: false

res <- readRDS(file.path("scripts", "shrinkage-tpdm", "results", "symmetric-logistic-ledoit-wolf.RDS"))

sl_tpdm <- function(a) { 
  sl_integrand <- function(x, a) (1 - a) / a * (x * (1-x))^(1/a - 3/2) * ((1-x)^(1/a) + x^(1/a))^(a-2) 
  if (a == 0) { 
    1 
  } else if (a == 1) { 
    0 
  } else { 
    integrate(function(x) sl_integrand(x, a = a), lower = 0, upper = 1, subdivisions = 500)$value 
  } 
} 
sl_tpdm <- Vectorize(sl_tpdm)

# lambda_true and lambda_hat against n
p1 <- ggplot(res, aes(x = n, y = lambda_hat, colour = as.factor(gamma), linetype = as.factor(kappa))) +
  stat_summary(aes(y = lambda_star, colour = as.factor(gamma)), fun = median, geom = "line", linetype = 1) +
  stat_summary(fun = median, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0.01, 0.04))) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(hat(lambda)(n)),
       colour = expression(gamma),
       linetype = expression(kappa)) +
  theme_light()

# mean entry of Sigma_hat against n
p2 <- ggplot(res, aes(x = n, y = E_sigma_hat_norm - (choose(d, 2) * sl_tpdm(gamma)^2), colour = as.factor(gamma))) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.02))) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(sum((hat(sigma)[ij](n) - sigma[ij](n))^2, "", "")), 
       colour = expression(gamma)) +
  theme_light()

# zeta_hat against n
p3 <- ggplot(res, aes(x = n, y = zeta_hat, colour = as.factor(gamma), linetype = as.factor(kappa))) +
  stat_summary(fun = median, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(hat(zeta)(n,kappa)),
       colour = expression(gamma),
       linetype = expression(kappa)) +
  theme_light()

# k * norm(Sigma_hat) * lambda_true against n
p4 <- ggplot(res, aes(x = n, y = k * E_sigma_hat_norm * lambda_star, colour = as.factor(gamma))) +
  stat_summary(fun = median, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_colour_manual(values = c("blue", "red", "darkgreen")) +
  scale_linetype_manual(values = c(2, 3, 6)) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(k(n) ~ lambda(n) ~ sum(hat(sigma)[ij](n)^2, "", "")),
       colour = expression(gamma)) +
  theme_light()

ggarrange(p1, p2, p3, p4, nrow = 2, ncol = 2, legend = "right", common.legend = TRUE)
```

The top-right panel of \autoref{fig-ledoit_wolf_symmetric_logistic} plots the mean of $\|\hat{\bm{\sigma}}(n)-\bm{\sigma}\|_2^2$ against $n$, based on all empirical TPDMs computed across all simulations. An error $\|\hat{\bm{\sigma}}(n)-\bm{\sigma}\|_2^2=\epsilon$ is equivalent to overestimating each entry by $\epsilon/\sqrt{3}$. The magnitude of the bias is decreasing in $n$ and increasing in $\gamma$. However, we note that even at $n=200,000$ there is non-negligible bias present. This illustrates that bias-corrected estimation still has a role to play at large sample sizes.

The bottom-right panel depicts the mean of $k(n)\mathbb{E}[\|\hat{\bm{\sigma}}(n)\|_2^2]\lambda^\star(n)$ against $n$. Our method is underpinned by an asymptotic approximation that assumes this quantity is constant above $\left\lfloor \kappa n\right\rfloor$. By inspecting this plot (for different $\gamma$), we can gauge whether the necessary convergences have occurred. For $\gamma=0.5$ and $\gamma=0.8$, the quantity appears to be approximately constant beyond $n\approx X$ and $n\approx 100,000$, respectively. For $\gamma=0.9$, convergence does not seem to have occurred by $n=200,000$. Thus we cannot guarantee that our procedure will produce good estimates for the regularisation parameter when $\gamma=0.9$ and $n\leq 200,000$.

The main results are shown in the top-left panel of @fig-symmetric-logistic-ledoit-wolf. The solid lines represent the true values of the optimal shrinkage parameter $\lambda(n)$. The dashed/dotted lines represent the mean estimate $\hat{\lambda}(n)$ obtained via our procedure for different values of $\kappa$ (listed in the plot legend). For $\gamma\in \{0.5,0.8\}$, our procedure works very well. The estimated values are very close to the true ones, provided the sample is large enough. When $n$ is large, the estimates are good for all values of $\kappa$. This agrees with the theory: good estimates may be obtained at a sample size $n$ provided $k(m)\mathbb{E}[\|\hat{\bm{\sigma}}(m)\|_2^2]\lambda^\star(m)$ is approximately constant for $m\geq \left\lfloor \kappa n\right\rfloor$. When $n$ is small, the estimates are rather conservative. This is sub-optimal but under-shrinking and erring towards the empirical TPDM is probably preferable to over-shrinking. For $\gamma=0.9$ we predicted that the estimates may be poor owing to the slow convergence rate. Indeed, the shrinkage parameter is nearly always significantly underestimated. The plot also shows the influence of the tuning parameter $\kappa$. Estimates based on $\kappa=0.9$ are highly variable but exhibit less bias; choosing $\kappa$ more conservatively results in stable but (negatively) biased estimates.

### EVA (2023) Data Challenge 4

Recall from Chapter XX that our submission to sub-challenge 4 for the EVA (2023) Data Challenge utilised CP decompositions of the empirical TPDMs corresponding to five clusters. It transpired that our approach overestimated the true probabilities. Subsequent investigation suggested that we overestimated dependence, especially in the first, fourth and fifth clusters. According to the Editorial, dependence in these clusters is weak [@rohrbeckEditorialEVA20232023]. In our post-hoc analysis, we proposed a remedy using sparse simplex projections. This effectively weakens dependence by spreading the mass of the empirical angular measure towards and onto the simplex boundary. This approach worked exceptionally well, but required making wholesale changes to our approach. Now, armed with our Ledoit-Wolf TPDM estimator, we are able to propose another alternative strategy. The new approach follows our original methodology exactly the Ledoit-Wolf TPDM replacing the raw empirical TPDM. The substitution is straightforward since the Ledoit-Wolf estimator is completely positive (@prp-ledoit-wolf-tpdm-properties).

Table XX in Chapter XX shows the summary statistics for the empirical TPDMs of the five clusters. The median values of $\{\hat{\sigma}_{ij}:i\neq j\}$ in each cluster are found to be 0.33, 0.68, 0.67, 0.33, and 0.50. Within each cluster, the range of $\{\hat{\sigma}_{ij}:i\neq j\}$ is at most 0.15. This means that the pairwise dependence strengths are relatively equal in each cluster. The previous experiment demonstrates that the Ledoit-Wolf estimator is adequate in such cases. Let $\hat{\Sigma}^{(1)},\ldots,\hat{\Sigma}^{(5)}$ denote the clusters' empirical TPDMs based on the $k$ largest observations among the $n=10^4$ samples provided. We estimate shrinkage parameters $\hat{\lambda}^{(1)},\ldots,\hat{\lambda}^{(5)}$ for each cluster using our tuning procedure with $k(n)=\left\lfloor k\sqrt{n}/100 \right\rfloor$, $\kappa=0.75$ and $B=10$. We then apply the CP-factorisation algorithm of @kirilioukEstimatingProbabilitiesMultivariate2022 to each TPDM and compute the resulting probability. This procedure is repeated for $0.02 \leq k \leq 0.07$. We also rerun the three other methods described in Chapter XX for comparison. 

```{r make-fig-eva-c4-ledoit-wolf-lambda}
#| label: fig-eva-c4-ledoit-wolf-lambda
#| fig-cap: "Blah"
#| fig-scap: "Blah"
#| fig-height: 4
#| message: false

res <- readRDS(file.path("scripts", "shrinkage-tpdm", "results", "eva-c4-ledoit-wolf.RDS"))

res %>% 
  unnest_longer(lambda_hat, indices_to = "cluster", values_to = "lambda_hat_cluster") %>%
  select(k_frac, cluster, lambda_hat_cluster) %>%
  mutate(cluster = as.factor(cluster)) %>%
  ggplot(aes(x = k_frac, y = lambda_hat_cluster, colour = cluster)) +
  geom_smooth(se = FALSE) +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_percent()) +
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0.01, 0.01))) +
  scale_colour_manual(values = c("grey", "red", "blue", "darkgreen", "orange"), labels = 1:5) +
  scale_linetype_manual(values = 1:2, labels = c("p1", "p2")) +
  labs(x = expression(k/n),
       y = expression(hat(lambda)), 
       colour = "Cluster") +
  theme_light()
```

@fig-eva-c4-ledoit-wolf-lambda reveals the amount of each shrinkage each cluster receives. As expected, the intensity is greatest and clusters 1 and 4 and smallest in clusters 2 and 3. For reference, @fig-eva-c4-ledoit-wolf-tpdm provides a visualisation of the raw and regularised estimates with $k=250$ (the number of threshold exceedances used in our submitted entry). To be precise, these are block-diagonal matrices formed from the raw/shrunk cluster-wise TPDMs. There is a stark contrast between these matrices, which will influence the results downstream. The parameters $\hat{\lambda}$ are quite sensitive to the choice of $k$. Surprisingly, the shrinkage increases as $k$ decreases. This is somewhat counter-intuitive: we expect that bias is largest when the radial threshold is lowered. \textit{Not actually sure why this is happening -- will think about it.}

```{r make-fig-eva-c4-ledoit-wolf-tpdm}
#| label: fig-eva-c4-ledoit-wolf-tpdm
#| fig-cap: "Blah"
#| fig-scap: "Blah"
#| fig-height: 4

Sigma_hat_clusters <- lapply(2:4, function(i) matrix(runif(i^2), nrow = i, ncol = i))
Sigma_tilde_clusters <- lapply(2:4, function(i) matrix(runif(i^2), nrow = i, ncol = i))

colpal <- colorspace::sequential_hcl(n = 100, "Viridis")
breaks <- lattice::do.breaks(c(0, 1.3), length(colpal))

Sigma_hat_bdiag <- bdiag(Sigma_hat_clusters) %>% as.matrix()
S1 <- t(Sigma_hat_bdiag[nrow(Sigma_hat_bdiag):1, ])
p1 <- lattice::levelplot(S1, col.regions = colpal, at = breaks,
                         xlab = "", ylab = "",
                         colorkey = TRUE,
                         scales = list(x = list(draw = FALSE),
                                       y = list(draw = FALSE)))

Sigma_tilde_bdiag <- bdiag(Sigma_tilde_clusters) %>% as.matrix()
S2 <- t(Sigma_tilde_bdiag[nrow(Sigma_tilde_bdiag):1, ])
p2 <- lattice::levelplot(S2, col.regions = colpal, at = breaks,
                         xlab = "", ylab = "",
                         colorkey = TRUE,
                         scales = list(x = list(draw = FALSE),
                                       y = list(draw = FALSE)))
grid.arrange(p1, p2, ncol = 2)
```

The final probability estimates are given in @fig-eva-c4-ledoit-wolf-prob-estimates. In each panel, the horizontal dashed line shows the true probability. The grey line corresponds to our original method based on CP-decompositions of the empirical TPDM. It yields drastic overestimates. The green and blue lines result from fitting a max-linear model using the $k$ largest angles defined via self-normalisation and Euclidean projections, respectively. The red line is derived from CP-decompositions of the Ledoit-Wolf TPDMs. At $k=250$, it gives the best estimate of $p_1$ and a very good estimate of $p_2$. The results indicate that the shrinkage parameters are conservative when $k/n$ is large. 

```{r make-fig-eva-c4-ledoit-wolf-prob-estimates}
#| label: fig-eva-c4-ledoit-wolf-prob-estimates
#| fig-cap: "Blah"
#| fig-scap: "Blah"
#| fig-height: 4
#| message: false



fancy_scientific_short <- function(l) {
  l %>%
    format(scientific = TRUE) %>%
    gsub("^(.*)e", "e", .) %>%
    gsub("e", "10^", .) %>%
    parse(text = .)
}

res %>% 
  pivot_longer(
    cols = empiricalstandard_p1:cplw_p2,
    names_to = c("method", "threshold"), 
    names_sep = "_",
    values_to = "estimate") %>%
  mutate(true_prob = case_when(threshold == "p1" ~ 8.4e-23,
                               threshold == "p2" ~ 5.4e-25)) %>%
  mutate(threshold = case_when(threshold == "p1" ~ "p[1]",
                               threshold == "p2" ~ "p[2]")) %>%
  ggplot(aes(x = k_frac, y = estimate, colour = method)) +
  geom_smooth(se = FALSE) +
  geom_hline(aes(yintercept = true_prob), linetype = "dashed") +
  geom_vline(xintercept = 0.025, linetype = "dashed") +
  facet_grid(. ~ threshold, labeller = label_parsed) +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_percent()) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.02)), transform = log10_trans(), breaks = breaks_log(n = 7), labels = fancy_scientific_short) +
  scale_colour_manual(values = c("grey", "red", "blue", "darkgreen"), labels = c("CP empirical TPDM", "CP Ledoit-Wolf TPDM", "Sparse empirical", "Empirical")) +
  scale_linetype_manual(values = 1:2, labels = c("p1", "p2")) +
  labs(x = expression(k/n),
       y = expression(hat(p)), 
       colour = "Method",
       linetype = "Threshold") +
  theme_light() +
  theme(legend.position = "top")
```

### Extremal SAR model

Our final experiment emulates the simulation study of @fixSimultaneousAutoregressiveModels2021. The goal is to estimate the dependence parameter $\rho$ in the extremal SAR model. This experiment will reinforce the effectiveness of our tuning procedure, while also highlighting the limitations of being restricted to Ledoit-Wolf shrinkage. Suppose $\bm{X}$ is as described in Section XX, with (known) neighbourhood matrix $W$ and (unknown) dependence parameter $\rho$. Following @fixSimultaneousAutoregressiveModels2021, we shall estimate $\rho$ by minimising the Frobenius difference between the model TPDM (viewed as a function of $\rho$) and a bias-corrected estimate of the TPDM. This can be posed as a hierarchical/bilevel optimisation problem, whereby the upper-level objective function depends on the solution to a lower-level problem. Specifically, we define
\begin{equation}\label{eq-extremal-sar-rho-bilevel}
    \hat{\rho} := \argmin_\varrho \hat{\mathcal{L}}(\varrho), \qquad \hat{\mathcal{L}}(\varrho) := \| \bm{\sigma}(\varrho) - \tilde{\bm{\sigma}}(\hat{\lambda}) \|_2^2.
\end{equation}
The loss function $\mathcal{L}(\varrho)$ depends on the estimate of $\tilde{\bm{\sigma}}(\hat{\lambda})$ of $\bm{\sigma}$. Accurate estimation of $\rho$ relies on accurate estimation of TPDM. If $\tilde{\bm{\sigma}}(\hat{\lambda})$ is a poor estimate of $\bm{\sigma}$, then the loss $\mathcal{L}(\varrho)$ may not be informative for $\rho$. In @fixSimultaneousAutoregressiveModels2021, $\tilde{\bm{\sigma}}$ represents the soft-thresholded estimator and $\hat{\lambda}$ is obtained by fitting a non-linear model relating the pairwise dependence strengths and inter-site distances. The soft-thresholded estimator gives a reasonable fit -- see Figure 3 (right) in @fixSimultaneousAutoregressiveModels2021. This suggests that the bias is approximately constant in \textit{absolute} terms, hinting that the Ledoit-Wolf estimator is ill-suited to this problem. 

Our simulation setup is based on Section 4.2.1 in @fixSimultaneousAutoregressiveModels2021. We consider $d=36$ sites arranged in an $6\times 6$ grid. We generate $n$ independent realisations of a $6\times 6$ spatial field based on the extremal SAR model with dependence parameter $\rho=0.15$. Dependence vanishes at a distance of approximately 4 units. The empirical TPDM $\hat{\Sigma}$ is computed using the $k(n)=\left\lfloor 4\sqrt{n} \right\rfloor$ largest observations. Next, we compute the oracle parameters $\lambda^\star$ for the soft-threshold, Ledoit-Wolf, and adaptive lasso ($\eta=2$) TPDMs. These are subtly different to our earlier notion of oracle; they minimise the Frobenius difference between the true TPDM and the *observed* empirical TPDM. Averaged other many simulations, they should converge to the original notion of the oracle. We then compute the oracle TPDMs $\tilde{\bm{\sigma}}(\lambda^\star)$ and estimate $\rho$ by \eqref{eq-extremal-sar-rho-bilevel}. This process is repeated using estimates of the regularisation parameter for Ledoit-Wolf (using our data-driven procedure with $\kappa=0.75$ and $B=5$) and soft-threshold (using the proximity-based method of @fixSimultaneousAutoregressiveModels2021). There is no available method for estimating the adaptive lasso threshold. The procedure is repeated 100 times at a sequence of sample sizes varying between $10^4 \leq n \leq 2\times 10^5$. 


```{r make-fig-extremal-sar-regularised-tpdm}
#| label: fig-extremal-sar-regularised-tpdm
#| fig-cap: "Blah"
#| fig-scap: "Blah"
#| fig-height: 4

res <- readRDS(file.path("scripts", "shrinkage-tpdm", "results", "extremal-sar-ledoit-wolf.RDS"))

p1 <- res %>% pivot_longer(
  cols = -c(n, k, d, rho, B, rep, kappa, k_kappa),
  names_to = c("param", "tpdm_estimator", "type"), 
  names_sep = "_",
  values_to = "value") %>%
  filter(param == "fl") %>%
  ggplot(aes(x = n, y = value, colour = tpdm_estimator, linetype = type)) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.02)), transform = log10_trans()) +
  scale_colour_manual(values = c("darkgreen", "grey", "red", "blue"), labels = c(expression("Adaptive lasso," ~ eta == 1), "Empirical", "Ledoit-Wolf", "Soft")) +
  scale_linetype_manual(values = 1:2, labels = c("Estimated", "Oracle")) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(sum((tilde(sigma)[ij] - sigma[ij])^2, "", "")), 
       colour = "TPDM estimator",
       linetype = "Type") +
  theme_light()

p2 <- res %>% pivot_longer(
  cols = -c(n, k, d, rho, B, rep, kappa, k_kappa),
  names_to = c("param", "tpdm_estimator", "type"), 
  names_sep = "_",
  values_to = "value") %>%
  filter(param == "lambda") %>%
  ggplot(aes(x = n, y = value, colour = tpdm_estimator, linetype = type)) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.02))) +
  scale_colour_manual(values = c("darkgreen", "red", "blue"), labels = c(expression("Adaptive lasso," ~ eta == 1), "Ledoit-Wolf", "Soft")) +
  scale_linetype_manual(values = 1:2, labels = c("Estimated", "Oracle")) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(hat(lambda)), 
       colour = "TPDM estimator",
       linetype = "Type") +
  theme_light()

p3 <- res %>% pivot_longer(
  cols = -c(n, k, d, rho, B, rep, kappa, k_kappa),
  names_to = c("param", "tpdm_estimator", "type"), 
  names_sep = "_",
  values_to = "value") %>%
  filter(param == "rho") %>%
  ggplot(aes(x = n, y = value - rho, colour = tpdm_estimator, linetype = type)) +
  stat_summary(fun = mean, geom = "line") +
  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01)), labels = label_number(scale = 1/1000)) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.02))) +
  scale_colour_manual(values = c("darkgreen", "grey", "red", "blue"), labels = c(expression("Adaptive lasso," ~ eta == 1), "Empirical", "Ledoit-Wolf", "Soft")) +
  scale_linetype_manual(values = 1:2, labels = c("Estimated", "Oracle")) +
  labs(x = expression(n %*% 10^{-3}),
       y = expression(hat(rho) - rho), 
       colour = "TPDM estimator",
       linetype = "Type") +
  theme_light()

ggarrange(p1, p2, p3, nrow = 1, ncol = 3, common.legend = TRUE, legend = "top")
```

The left-hand plot in fig-extremal-sar-regularised-tpdm shows how the Frobenius loss (on a log scale) varies with $n$ for each TPDM estimate. The colours correspond to the regularisation method used (if any); the linetype indicates whether the regularisation parameter is an oracle or has been estimated. The oracle losses represent a lower limit on the Frobenius loss that can be attained by each class of estimator. For this reason, the dashed lines lie below the solid lines. If the solid line lies close to the oracle loss, then $\lambda$ has been selected optimally. In this respect, the soft-threshold tuning procedure of @fixSimultaneousAutoregressiveModels2021 performs very well. This should be caveated by noting that it requires extra information to achieve this. For Ledoit-Wolf, our data-driven procedure also appears to be working relatively well. Comparing across estimators, the plot suggests that soft-threshold is the best performing and Ledoit-Wolf the worst, but all are markedly superior to the empirical estimate. 

The middle plot provides more insight into the selected values of $\lambda$. The soft-threshold parameter is very close to the oracle value. Our estimator also performs well, following the the oracle very closely. This hold true even for small samples despite it being based on large-sample approximations. For this example, the Frobenius loss seems to be quite sensitive to errors in the shrinkage intensity, especially when $n$ is small. Thus, even though we estimate $\lambda$ rather accurately, the Frobenius loss can still be non-negligible. 

The right-hand plot displays the bias $\hat{\rho}-\rho$ in the spatial dependence parameter estimates. The empirical TPDM overestimates dependence and therefore overestimates $\rho$. The soft-threshold estimate is interesting: the estimator of @fixSimultaneousAutoregressiveModels2021 actually outperforms the oracle. This is not a contradiction. A smaller Frobenius loss does not guarantee a better estimate of $\rho$ (unless the loss equals zero). @fixSimultaneousAutoregressiveModels2021 found that their estimator exhibited a slight negative bias for $\rho$. Our results confirm this but shed more light on its root cause. Since the oracle suffers the same issue, we infer that the bias arises from working in the soft-thresholding class, *not* from their tuning procedure. As predicted, the Ledoit-Wolf estimator performs comparatively poorly. It has a significant downwards bias, especially when $n$ is small. However, we reiterate that this is a shortcoming of using a restricted class of estimator, not our procedure. The estimates of $\rho$ obtained via our method are as good as one could hope for. The adaptive lasso oracle achieves relatively good results; there is scope to improve on this by varying the additional parameter $\eta$. 

## Conclusions and outlook

**Achievements:**

- First data-driven approach for reducing bias in the TPDM. 
- Based on asymptotic approximations, but finite-sample performance is good across multiple experiments.
- Shown how properties of proposed estimator make it easy to plug-in to other methodologies. As an example, we applied it to EVA method because it is completely positive. Would be easy to do the same with @rohrbeckSimulatingFloodEvent2023, say, because it is positive definite). 
- Proposed a broader class of TPDM estimators; final experiments shows that these can be useful.

**Future work:**

- More investigation into the procedure, e.g. choice of $\kappa$ and $B$.
- Is there a way to make the LW more flexible, e.g. estimating the shrinkage separately for groups of similarly sized entries in $\hat{\Sigma}$.
- Derive similar procedures for tuning the more flexible estimators. Is there any way to estimate the optimism in Section 6.3.1, or is this a non-starter?
- Once the previous objective is achieved, a framework for choosing the $\lambda$ *and* the type of estimator.
- Results from EVA application suggest that a good estimate of the dependence structure can be obtained by (i) estimating something and then correcting it, or (ii) modifying the data (i.e. the angles) themselves and then estimating things. Can these two disparate approaches be compared/unified in any way?
- Minimisation with respect to other loss functions [@rothmanGeneralizedThresholdingLarge2009]. E.g. how well it detects the sparsity structure, 
\begin{align*}
    \mathrm{TPR}(\lambda) &:= \frac{|\{(i,j) : i<j,\, \tilde{\sigma}_{ij}(\lambda)\neq 0,\, \sigma_{ij}\neq 0\}|}{|\{(i,j) : i<j,\, \sigma_{ij}\neq 0\}|}, \\
    \mathrm{FPR}(\lambda) &:= \frac{|\{(i,j) : i<j,\, \tilde{\sigma}_{ij}(\lambda)\neq 0,\, \sigma_{ij} = 0\}|}{|\{(i,j) : i<j,\, \sigma_{ij} = 0\}|},
\end{align*} 
or a measure of agreement between the principal eigenspaces (CITE Krzanowski (1979))
\begin{equation*}
K_p(\lambda) := \sum_{i=1}^p \sum_{j=1}^p \left\langle \tilde{\bm{u}}_i(\lambda),\bm{u}_j\right\rangle^2, \qquad (1\leq p\leq d). 
\end{equation*}
  These may be useful if using TPDM for clustering or PCA, respectively. For the sparsity, could use the fact that TPDM based on sparse simplex projections has the same zeroes as the TPDM (and these zeroes will occur in finite samples). Can then plot the ROC curve based on TPR/FPR as above with the true $\sigma_{ij}$ replaced with the entries $\hat{\sigma}_{ij}^\star$ of the sparse projection-based estimate. Can then see whether this data-driven ROC curve is representative of true (oracle) ROC curve. Note that sparsity only makes sense for threshold estimates, LW does not produce zeroes.
