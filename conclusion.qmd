# Conclusion {#sec-conclusion}

## Summary

This thesis explored how the TPDM can be used to enhance our understanding and prediction of extreme events. Such events are of interest across a broad range of domains, such as climatology and finance. Accordingly, we worked in a general framework so that our methods are widely applicable. Our work reiterates the fact that the TPDM is a powerful and versatile tool in multivariate EVT. Specifically, our contributions include: a hypothesis test for detecting changing dependence in high dimensions; an exploration of the link between multivariate EVT and compositional data analysis and concrete examples where leveraging this connection reaps performance benefits; data-driven strategies for predicting extreme events using sparse projections and matrix factorisations; flexible TPDM estimators with reduced bias and a strategy for tuning them. Using our methods will enable practitioners to work in higher dimensions, enjoy reduced computation times, validate their assumptions, and obtain more accurate measures of risk. Cutting-edge performance was evidenced via simulation studies providing direct comparisons against current state-of-the-art as well as achieving a first-placed ranking in the third sub-challenge of the EVA (2023) Data Challenge. The contributions of each chapter are outlined in more detail below.

@sec-changing-ext-dep introduced a test for the premise that extremal dependence is constant over time. This is a key assumption across multivariate EVT that may be violated due to climate change or new financial regulations. Previously, the only existing method for validating it has been the test of @dreesStatisticalInferenceChanging2023, which is limited to low dimensions for computational and theoretical reasons. Our main contribution was to develop an alternative procedure that is viable in high dimensions. To achieve this, we defined time-dependent extensions to the TPDM: the local TPDM and integrated TPDM. By studying the asymptotic behaviour of their empirical estimators, we derived the asymptotic null distributions of two test statistics. These are fast to compute and critical values are readily available. Simulation studies examined our test's performance with realistic sample sizes under a range of dependence scenarios. Under the null, our test approximately attained its nominal Type I error rate; under alternatives, it was more powerful than @dreesStatisticalInferenceChanging2023, except in the very specific case described in @sec-changing-ext-dep-experiments-tpdm-invariant. An application to Red Sea temperature extremes (@sec-changing-ext-dep-red-sea) illustrated how our method can be used as an exploratory tool for understanding the effects of climate change and as a preliminary step to determine whether stationary methods are adequate.

Inference in multivariate EVT is generally based on the $k$ largest pseudo-angles, points lying on the simplex whose (limiting) distribution characterises extremal dependence. @sec-compositional explored ways in which tools from compositional data analysis (CoDA), which is specifically designed for data on the simplex, can be utilised in this context. We drew general connections between the two fields (@sec-compositional-connections-to-evt), before applying CoDA tools to two problems in multivariate EVT: principal components analysis (@sec-compositional-pca) and extreme event classification (@sec-compositional-classification). Our experiments revealed that working in the Aitchison geometry often yields more efficient dimension reduction -- both in terms of Euclidean reconstruction error and tail event prediction -- and reduces the asymptotic classification error of extreme events. While these improvements to existing methods represent novel contributions in themselves, we hope our research sparks further consideration of how CoDA might be applied in other areas of multivariate EVT. 

@sec-eva-data-challenge contained our submission to the EVA (2023) Data Challenge. For Challenge 3 (@sec-eva-c3), the key difficulty was modelling the extremal directions of a low-dimensional random vector. Our main contribution was to show how the sparse simplex projections of @meyerSparseRegularVariation2021 can be used to estimate a max-linear model whose angular measure is supported on sub-faces of the simplex. The benefits of our approach are evident from the accuracy of the resulting probability estimates and our first-placed ranking in this task. Our Challenge 4 (@sec-eva-c4) methodology extended the framework of @kirilioukEstimatingProbabilitiesMultivariate2022, e.g. by incorporating clustering. We identified deficiencies with the procedure, namely the tendency of the empirical TPDM to overestimate weak dependencies, and showed how these can be addressed by again turning to sparse projections. Thus, a key contribution is to illustrate the practical utility of incorporating Euclidean simplex projections into the TPDM/max-linear modelling framework. We hope the theoretical aspects of this approach are explored further in future.

@sec-background-bias-issue and @sec-shrinkage-tpdm collectively entailed an in-depth study of the bias of the empirical TPDM. Our first key contribution was to apply thresholding techniques to construct flexible TPDM estimators that mitigate against the bias. In particular, the adaptive lasso estimator is able to target shrinkage towards small entries, unlike the soft-thresholded TPDM proposed by @fixSimultaneousAutoregressiveModels2021. Our second contribution is to propose and study the Ledoit-Wolf TPDM. In contrast with alternative estimators proposed elsewhere [@fixSimultaneousAutoregressiveModels2021; @jiangPrincipalComponentAnalysis2020], it satisfies the key properties of a TPDM. Moreover, we were able to devise a purely data-driven procedure for selecting the regularisation parameter, whereas all previous bias-correction tuning procedures rely on heuristics or additional *a priori* knowledge. Compared to the empirical estimator, our methods: permit accurate estimation of the TPDM of a symmetric logistic random vector (@sec-shrinkage-tpdm-experiments-sl); correct for the overestimation that hindered our performance in Challenge 4 of the EVA (2023) Data Challenge (@sec-shrinkage-tpdm-experiments-eva-c4); facilitate improved inference a spatial extremes model (@sec-shrinkage-tpdm-experiments-sar). 

## Future research ideas

### Change-point detection {#sec-future-work-changepoint}

@sec-changing-ext-dep detailed a procedure for detecting whether the extremal dependence structure is constant or changing over time. If a change is detected, a natural follow-up question is: when did the change occur? This question belongs to the realm of change-point detection and is the subject of an ongoing project in collaboration with Euan McGonigle (University of Southampton), Jordan Richards (University of Edinburgh) and Christian Rohrbeck (University of Bath). The high-level goal is to devise a change-point algorithm for estimating changes in extremal dependence, by extending the methods in @sec-changing-ext-dep or otherwise. Existing work on this topic is scarce and restricted to likelihood-ratio tests based on parametric models: @diasChangepointAnalysisDependence2004 consider the bivariate symmetric logistic model; @hazraEstimatingChangepointsExtremal2024 detect change-points in the HÃ¼sler-Reiss parameter $\Lambda$. By leveraging the TPDM, we are able to operate in high dimensions and avoid restrictive parametric assumptions. 

Assume there exists a change-point $\tau\in[0,1]$ such that $\Sigma(t)$ is constant up to the change-point and remains constant thereafter. Given samples $\{\mathbf{x}_l\}_{l/n \in (0,\tau]}$ and $\{\mathbf{x}_l\}_{l/n \in (\tau,n]}$ and noting that
\begin{align*}
\psi_{ij}(t)-t\psi_{ij}(1)
&= \int^t_0 \sigma_{ij}(s)\,\dee s - t \int^1_0 \sigma_{ij}(s)\,\dee s \\
&= t(1-t) \left[\frac{1}{t}\int^t_0\sigma_{ij}(s)\,\dee s- \frac{1}{1-t} \int^1_{t}\sigma_{ij}(s)\,\dee s\right],
\end{align*}
a CUSUM-type estimator for the change-point is given by
\begin{equation*}
\hat{\tau}= \argmax_{t\in[0,1]}(\hat{\bm{\psi}}(t)-t\hat{\bm{\psi}}(1)).
\end{equation*}
Alternatively, one can construct MOSUM-type statistics and look for local maxima of these; the main challenge is deriving the behaviour of such statistics near change-points in order to obtain statistical guarantees on the estimation accuracy of the change-points.

### Incorporating covariate information

Presently, the TPDM framework does not permit the inclusion of covariates. This limitation was raised by an anonymous reviewer of our EVA (2023) Data Challenge paper (@sec-eva-data-challenge), since we neglected to use the available covariate information for Challenge 3. Incorporating covariate information would allow the dependence structure of extreme events to adapt dynamically based on external factors or conditions. Covariates such as time, climate conditions, or economic indicators might influence how extreme values in one variable relate to another. A covariate-adjusted TPDM can better capture varying relationships in different contexts, leading to more accurate predictions of joint extreme events. We extend the definition of the TPDM to include covariate dependence as follows.

:::{#def-covariate-adjusted-tpdm}
Suppose $\bm{x}\in\R^m$ is a vector of covariates and $\bm{Y}(\bm{x})\in\mathcal{RV}_+^d(2)$ has angular measure $H(\bm{x})$. The covariate-adjusted TPDM is the $d\times d$ matrix
\begin{equation}\label{eq-covariate-tpdm}
    \Sigma(\bm{x}) = (\sigma_{ij}(\bm{x})), \qquad \sigma_{ij}(\bm{x}) = \int_{\mathbb{S}_{+(2)}^{d-1}} \theta_i \theta_j \,\dee H(\bm{\theta};\bm{x}) = \mathbb{E}_{H(\bm{x})} [\Theta_i\Theta_j].
\end{equation}
:::

The main challenge involved in using a covariate-adjusted TPDM lies in performing inference, due to the scarcity of extreme observations. The semi-parametric spectral density ratio model of @decarvalhoSpectralDensityRatio2014 might provide a useful starting point. They consider a family of angular measures $\{H_l=H(\bm{x}_l):l=1,\ldots,L\}$ associated with $L$ sub-populations corresponding to covariate vectors $\bm{x}_1,\ldots,\bm{x}_L\in\R^m$. The authors simultaneously estimate $H_1,\ldots,H_L$ by assuming that they are related to a unspecified baseline angular measure $H_0$ via a weight/link function. This setup allows information to be borrowed across the sub-populations, allowing reliable estimation even for sub-populations whose samples are too small to be individually informative about their tails. From this, one could define sub-population TPDMs $\Sigma_1,\ldots,\Sigma_L$ by substituting $H_1,\ldots,H_L$ into \eqref{eq-covariate-tpdm}. The theoretical and computational details outlined in @decarvalhoSpectralDensityRatio2014 should carry over fairly naturally. By comparing $\Sigma_1,\ldots,\Sigma_L$ against each other (or the baseline TPDM $\Sigma_0$), one can ascertain the influence of the covariates on the dependence structure. 

Once a covariate-adjusted TPDM has been developed (using the approach described above or otherwise), it can then be incorporated into TPDM-based modelling frameworks. For example, completely positive factorisations $\Sigma(\bm{x})=A(\bm{x})A(\bm{x})^T$ may be used to fit covariate-dependent max-linear models, from which tail event probabilities can be estimated under different conditions. Another application involves plugging $\Sigma(\bm{x})$ into the generative framework of @rohrbeckSimulatingFloodEvent2023 in order to construct hazard event sets under different scenarios. 

### TPDM based on sparse simplex projections

In multivariate extremes, an important preliminary step in the model selection process is to determine whether sets of variables are asymptotically dependent or asymptotically independent. While the TPDM is a useful exploratory tool, it is limited for this purpose: pairs of variables that are asymptotically independent are difficult to identify from the empirical TPDM due to the bias issue; even if $\sigma_{ij}=0$ the corresponding entry $\hat{\sigma}_{ij}$ is typically not very close to zero. The bias-correction procedure in @sec-shrinkage-tpdm might solve this problem, but an alternative solution utilises the sparse simplex projections from @sec-eva-data-challenge.

:::{#def-sparse-tpdm}
Suppose $\bm{X}\in\mathcal{RV}_+^d(1)$ and let $\pi$ denote the Euclidean projection onto the simplex (@def-eva-euclidean-projection). The sparse TPDM of $\bm{X}$ is the $d\times d$ matrix
\begin{equation}\label{eq-sparse-tpdm}
    \Sigma^\star = (\sigma_{ij}^\star), \qquad \sigma_{ij}^\star = \lim_{t\to\infty} \mathbb{E}\left[[\pi(\bm{X}/t)]_i^{1/2} [\pi(\bm{X}/t)]_j^{1/2} \mid \|\bm{X}\|_1 > t \right].
\end{equation}
:::

Theorem 2 in @meyerSparseRegularVariation2021 states that the limiting distribution of the Euclidean angles only places mass on simplicial subspaces where the angular measure places mass or subspaces containing them. This implies that $\sigma_{ij}^\star\neq \sigma_{ij}$ in general, but $\sigma_{ij}^\star=0$ if and only if $\sigma_{ij}=0$. This relation becomes useful when we consider finite-sample estimates of $\Sigma^\star$ and $\Sigma$. The estimator of the sparse TPDM is defined as follows.

:::{#def-sparse-empirical-tpdm}
Suppose $\bm{X}\in\mathcal{RV}_+^d(1)$ and let $\bm{x}_1,\ldots,\bm{x}_n$ be iid observations of $\bm{X}$. The empirical sparse TPDM is the $d\times d$ matrix
\begin{equation*}
  \hat{\Sigma}^\star = (\hat{\sigma}_{ij}^\star), \qquad 
  \hat{\sigma}_{ij}^\star := \hat{E}_{\hat{H}^\star}[\Theta_i^{1/2}\Theta_j^{1/2}]=\frac{d}{k}\sum_{l=1}^k [\pi(\bm{x}_{(l)}/r_{(k+1)})]_i^{1/2}[\pi(\bm{x}_{(l)}/r_{(k+1)})]_j^{1/2},
\end{equation*}
where $\hat{H}^\star$ is defined below @def-eva-sparse-empirical-A and $\pi$ is the Euclidean projection onto the simplex (@def-eva-euclidean-projection).
:::

While the entries of the ordinary empirical TPDM are non-zero almost surely, the Euclidean projection yields projections lying on the simplex boundary. Consequently, the estimate $\hat{\Sigma}^\star$ is a sparse matrix. Due to the theoretical connection between $\Sigma^\star$ and $\Sigma$, the zero entries of $\hat{\Sigma}^\star$ can be used to estimate the zero entries of $\Sigma$. 

From here, the research could proceed in any number of directions (e.g. clustering, graphical modelling, fitting max-linear models). We describe one possible application, whereby $\hat{\Sigma}^\star$ is used to select the regularisation parameter for the thresholded TPDM estimates (@def-thresholded-tpdm). Recall that the goal is to choose the threshold $\lambda>0$ so that the thresholded TPDM $\tilde{\Sigma}(\lambda)=s_\lambda(\hat{\Sigma})$ is close to the true TPDM. In @sec-shrinkage-tpdm, we measured closeness using the Frobenius error \eqref{eq-regularisation-frobenius-loss}, but one might consider alternative metrics such as
\begin{align*}
    \mathrm{TPR}(\lambda) &:= \frac{|\{(i,j) : i<j,\, \tilde{\sigma}_{ij}(\lambda)\neq 0,\, \sigma_{ij} \neq 0\}|}{|\{(i,j) : i<j,\, \sigma_{ij}\neq 0\}|}, \\
    \mathrm{FPR}(\lambda) &:= \frac{|\{(i,j) : i<j,\, \tilde{\sigma}_{ij}(\lambda)\neq 0,\, \sigma_{ij} = 0\}|}{|\{(i,j) : i<j,\, \sigma_{ij} = 0\}|}.
\end{align*} 
These criteria assess how accurately the regularised TPDM captures the true sparsity structure of $\Sigma$. The threshold $\lambda$ would typically be selected by inspecting a receiver operating characteristic (ROC) curve and/or maximising the area under the curve (AUC). However, $\mathrm{TPR}(\lambda)$ and $\mathrm{FPR}(\lambda)$ depend on the unknown TPDM. To overcome this, one might replace $\sigma_{ij}$ with $\hat{\sigma}_{ij}^\star$ and compute
\begin{align*}
    \widehat{\mathrm{TPR}}(\lambda) &:= \frac{|\{(i,j) : i<j,\, \tilde{\sigma}_{ij}(\lambda)\neq 0,\, \hat{\sigma}_{ij}^\star \neq 0\}|}{|\{(i,j) : i<j,\, \hat{\sigma}_{ij}^\star \neq 0\}|},\\
    \widehat{\mathrm{FPR}}(\lambda) &:= \frac{|\{(i,j) : i<j,\, \tilde{\sigma}_{ij}(\lambda)\neq 0,\, \hat{\sigma}_{ij}^\star  = 0\}|}{|\{(i,j) : i<j,\, \hat{\sigma}_{ij}^\star = 0\}|}.
\end{align*} 
Through theoretical analysis and simulation experiments, one can ascertain whether the pseudo-ROC curve based on $\widehat{\mathrm{TPR}}$ and $\widehat{\mathrm{FPR}}$ resembles the true ROC curve. Provided the conclusion is positive, this yields a data-driven procedure for selecting the threshold $\lambda$. 
