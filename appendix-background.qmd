# Supplementary Material for @sec-background

## Properties of the TPDM {#sec-app-tpdm-properties}

### Equivalence of TPDM definitions {#sec-app-tpdm-definitions}

The TPDM was originally defined by @cooleyDecompositionsDependenceHighdimensional2019 for multivariate random vectors with tail index $\alpha=2$ as in @def-tpdm. This was later generalised by @kirilioukEstimatingProbabilitiesMultivariate2022 to permit an arbitrary tail index $\alpha\geq 1$ (@def-tpdm-alpha). Beyond the fact that these definitions coincide when $\alpha=2$, it is not obvious why @def-tpdm-alpha is the natural generalisation to @def-tpdm. In this section, we provide some insight into this matter. In doing so, we show that the TPDM is independent of $\alpha$, enabling the tail index to be chosen without loss of generality. For example, in @sec-compositional and @sec-changing-ext-dep we fix $\alpha=1$ and $\alpha=2$, respectively. Moreover, the resulting formulae will be used throughout the thesis to compute the true TPDM under various parametric models.


The following lemma provides a formula for the angular density (for arbitrary $\alpha$) in terms of the angular density when $\alpha=1$. We extend a result in the Supplementary Material of @fixSimultaneousAutoregressiveModels2021, which outlines how to transform from $\alpha=1$ to $\alpha=2$. 

:::{#lem-angular-density-transformation}
Suppose $\bm{X}=(X_i,X_j)\in\mathcal{RV}_+^2(\alpha)$ for some $\alpha\geq 1$. Let $H_\alpha$ denote the normalised angular measure with respect to $\|\cdot\|_\alpha$ and $h_\alpha:\mathbb{S}_{+(\alpha)}\to\R_+$ the corresponding angular density (assuming it exists). Moreover, we define the re-parametrised density as
\begin{equation*}
    \tilde{h}_\alpha:[0,1]\to\R_+, \qquad \theta \mapsto h_\alpha\left(\left(\theta,(1-\theta^\alpha)^{1/\alpha}\right)\right).
\end{equation*}
Then
\begin{equation}
    \tilde{h}_\alpha(\theta) = \alpha \theta^{\alpha-1} \tilde{h}_1(\theta^\alpha).
\end{equation}
:::

::: {.proof}
The proof generalises the procedure described in Section 3.2 of the Supplementary Material of @fixSimultaneousAutoregressiveModels2021. First, we transform from $L_1$ polar coordinates $(r,\bm{\theta})$ to Cartesian coordinates $\bm{z}=(z_i,z_j)=(r\theta_i,r\theta_j)$. The Jacobian of the transformation is $\|\bm{z}\|_1^{-1}$. Using \eqref{eq-nu-H-relation} with $\alpha=1$ and $H_1(\dee \bm{\theta})=h_1(\bm{\theta})\dee \bm{\theta}$,
    \begin{align*}
        \nu(\dee r \times \dee \bm{\theta}) 
        &= r^{-2} h_1(\bm{\theta}) \, \dee r\,\dee\bm{\theta} \\
        &= \|\bm{z}\|_1^{-2} h_1(\bm{z}/\|\bm{z}\|_1) \|\bm{z}\|_1^{-1} \dee\bm{z} \\
        &= \|\bm{z}\|_1^{-3} h_1(\bm{z}/\|\bm{z}\|_1) \dee\bm{z} \\
        &= \nu(\dee \bm{z}).
    \end{align*}
    Next, we transform from tail index $\alpha=1$ to arbitrary $\alpha$. Let $\bm{y}=(y_i,y_j)=(z_i^{1/\alpha},z_j^{1/\alpha})$. The Jacobian of this transformation is $\alpha^2 y_i^{\alpha-1}y_j^{\alpha-1}$. Note that $\|\bm{z}\|_1=y_i^\alpha+y_j^\alpha=\|\bm{y}\|_\alpha^\alpha$.
    \begin{equation*}
        \nu(\bm{z}) 
        = \left[\|\bm{y}\|_\alpha^{\alpha}\right]^{-3} h_1\left(\frac{y_i^\alpha}{\|\bm{y}\|_\alpha^{\alpha}},\frac{y_j^\alpha}{\|\bm{y}\|_\alpha^{\alpha}}\right) \alpha^2 y_i^{\alpha-1}y_j^{\alpha-1} \dee\bm{y} 
        = \nu(\dee \bm{y}).
    \end{equation*}
    Finally, we transform to $L_\alpha$ polar coordinates $(s,\bm{\phi})$ with $s=\|\bm{y}\|_\alpha$ and $\bm{\phi}=(\phi_i,\phi_j)=\bm{y}/s$. By Lemma 1.1 in @songL_pnormUniformDistribution1997, the Jacobian is $s(1 - \phi_i^\alpha)^{(1-\alpha)/a} = s\phi_j^{1-\alpha}$. We now have
    \begin{align*}
        \nu(\dee \bm{y}) 
        &= \left[s^{\alpha}\right]^{-3} h_1\left(\phi_i^\alpha,\phi_j^\alpha \right) \alpha^2 (s\phi_i)^{\alpha-1}(s\phi_j)^{\alpha-1} s\phi_j^{1-\alpha} \,\dee s \,\dee\bm{\phi} \\
        &= \alpha s^{-\alpha-1} \alpha \phi_i^{\alpha-1} h_1\left(\phi_i^\alpha,\phi_j^\alpha \right) \,\dee s\,\dee \bm{\phi} \\
        &= \alpha s^{-\alpha-1} h_\alpha(\bm{\phi}) \,\dee s\,\dee \bm{\phi} \\
        &= \nu(\dee s \times \dee \bm{\phi}),
    \end{align*}
    where $h_\alpha(\bm{\phi}):=\alpha \phi_i^{\alpha-1} h_1\left(\phi_i^\alpha,\phi_j^\alpha \right)$. The final step is to compute $\tilde{h}_\alpha$ by projecting the density $h_\alpha$, which lives on $\mathbb{S}_{+(\alpha)}^1$, down to $[0,1]$. Writing $\bm{\phi}$ as $(\phi,(1-\phi^\alpha)^{1/\alpha})$ gives
    \begin{equation*}
        \tilde{h}_{\alpha}(\phi) = h_\alpha\left(\left(\phi, (1-\phi^\alpha)^{1/\alpha}\right)\right) = \alpha \phi^{\alpha-1}h_1\left((\phi^\alpha, 1-\phi^\alpha)\right) = \alpha \phi^{\alpha-1} \tilde{h}_1(\phi^\alpha). \qedhere
    \end{equation*}
:::

In the trivial case $\alpha=1$ the formula reduces to $\tilde{h}_1(\theta) = \tilde{h}_1(\theta)$, as one would hope. Setting $\alpha=2$ yields $\tilde{h}_2(\theta) = 2\theta\tilde{h}_1(\theta^2)$, which matches the formula in @fixSimultaneousAutoregressiveModels2021. Note that $\tilde{h}_\alpha$ is well-defined (i.e. is a normalised density), since
\begin{equation*}
    \int_0^1 \tilde{h}_\alpha (\theta)\,\dee \theta = \int_0^1 \alpha\theta^{\alpha-1}\tilde{h}_1 (\theta^\alpha )\,\dee \theta = \int_0^1 \tilde{h}_1(\phi)\,\dee \phi = 1.
\end{equation*}
We now apply the transformation formula to express the TPDM for any $\alpha\geq 1$ in terms of the angular density $\tilde{h}_1$.

:::{#prp-tpdm-h1-formula}
Using the notation of @lem-angular-density-transformation, the off-diagonal entry in the TPDM of $\bm{X}$ is
\begin{equation}\label{eq-tpdm-h1-formula}
    \sigma_{ij} = m \int_0^1 \sqrt{u(1-u)} \, \tilde{h}_1(u)\,\dee u.
\end{equation}
:::

::: {.proof}
The relation between the normalised measure $H_\alpha$ and the measure $H$ in @def-tpdm-alpha is $H_\alpha=m^{-1}H$, where $m$ is the mass of $H$. Therefore, \eqref{eq-tpdm-alpha} can be equivalently restated as
    \begin{equation*}
        \sigma_{ij} = m \int_{\mathbb{S}_{+(\alpha)}} \theta_i^{\alpha/2}\theta_j^{\alpha/2} \,\dee H_\alpha (\bm{\theta})
    \end{equation*}
    Rewriting this in terms of the angular density and re-parametrising yields
    \begin{align*}
        \sigma_{ij} 
        &= m \int_{\mathbb{S}_{+(\alpha)}} \theta_i^{\alpha/2}\theta_j^{\alpha/2} h_{\alpha}(\bm{\theta})\,\dee \bm{\theta} \\
        &= m \int_{\mathbb{S}_{+(\alpha)}} \theta_i^{\alpha/2}[(1-\theta_i^\alpha)^{1/\alpha}]^{\alpha/2} h_\alpha(\bm{\theta})\,\dee \bm{\theta} \\
        &= m \int_0^1 \theta^{\alpha/2} (1-\theta^\alpha)^{1/2} \tilde{h}_\alpha(\theta)\,\dee \theta.
    \end{align*}
    Finally, we apply @lem-angular-density-transformation and substitute $u=\theta^\alpha$ to obtain the final result
    \begin{equation*}
        \sigma_{ij} 
        = m \int_0^1 \theta^{\alpha/2} (1-\theta^\alpha)^{1/2} \alpha \theta^{\alpha-1}\tilde{h}_1(\theta^\alpha)\,\dee \theta 
        = m \int_0^1 \sqrt{u(1-u)}\,\tilde{h}_1(u)\,\dee u. \qedhere
    \end{equation*}
:::

Substituting the angular density of the bivariate symmetric logistic distribution
\begin{equation*}
\tilde{h}_1(\theta;\gamma) = \frac{1-\gamma}{2\gamma}[\theta(1-\theta)]^{\frac{1}{\gamma}-2}[\theta^{1/\gamma} + (1-\theta)^{1/\gamma}]^{\gamma-2}
\end{equation*}
or Hüsler-Reiss distribution
\begin{equation*}
\tilde{h}_1(\theta;\lambda) = \frac{\exp\left(-\lambda/4\right)}{4\lambda [\theta(1-\theta)]^{3/2}}\phi\left(\frac{1}{2\lambda}\log\left(\frac{\theta}{1-\theta}\right)\right)
\end{equation*}
into \eqref{eq-tpdm-h1-formula} yields the expressions given in @exm-symmetric-logistic-tpdm and @exm-husler-reiss-tpdm.

### Formula for the asymptotic variance $\nu_{ij}^2$ {#sec-app-tpdm-asymptotic-var}

In this section, we derive a formula for the asymptotic variance $\nu_{ij}^2$ of $\hat{\sigma}_{ij}$ -- see @prp-empirical-tpdm-normality-entries in terms of the angular density $\tilde{h}_1$ of $(X_i,X_j)$. Using the identity $\mathrm{Var}(Y)=\mathbb{E}[Y^2]-\mathbb{E}[Y]^2$ and @prp-tpdm-h1-formula, we have
\begin{equation*}
    \nu_{ij}^2 
    = m^2 \int_{\mathbb{S}_{+(\alpha)}^{d-1}}(\theta_i\theta_j)^\alpha \,\dee H_\alpha(\bm{\theta}) - \sigma_{ij}^2 
    = m^2 \int_0^1 \theta^\alpha (1-\theta^\alpha) \tilde{h}_\alpha(\theta) \,\dee \theta - \sigma_{ij}^2.
\end{equation*}
Substituting $u=\theta^\alpha$ and using @prp-tpdm-h1-formula gives the final expression
\begin{equation}\label{eq-nu-squared-h1-formula}
    \nu_{ij}^2 = m^2 \int_0^1 u(1-u)\,\tilde{h}_1(u)\,\dee u - \left[m\int_0^1 \sqrt{u(1-u)}\,\tilde{h}_1(u)\,\dee u \right]^2.
\end{equation}
Again, we note that the result, in this case the asymptotic variance (and therefore distribution) of $\hat{\sigma}_{ij}$, does not depend on $\alpha$.

### Proof of @prp-empirical-tpdm-normality {#sec-app-tpdm-asy-normality-proof}

::: {.proof}
We follow the proof of Theorem 5.23 in @kraliCausalityEstimationMultivariate2018 but adapt it to the general $\alpha$ case. By the Cramér-Wold device, it is sufficient to show asymptotic normality of $\sqrt{k}\bm{\beta}^T(\hat{\bm{\sigma}}-\bm{\sigma})$ for all $\bm{\beta}\in\R^{d\choose 2}$. For convenience, the components of $\bm{\beta}$ are indexed to match the sub-indices of $\bm{\sigma}$. Then
\begin{equation*}
    \bm{\beta}^T\bm{\sigma}
        = \sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\sigma_{ij} = \mathbb{E}_{H}\left[\sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}\right] =: \mathbb{E}_{H}[g(\bm{\Theta};\bm{\beta})],
    \end{equation*}
where 
\begin{equation*}
        g(\bm{\theta};\bm{\beta}):=\sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\theta_i^{\alpha/2}\theta_j^{\alpha/2}
\end{equation*}
The corresponding empirical estimator is
\begin{equation*}
        \hat{\mathbb{E}}_{H}[g(\bm{\Theta};\bm{\beta})] = \frac{m}{k} \sum_{l=1}^{k}\sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2} = \sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\left(\frac{m}{k}\sum_{l=1}^k\Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2} \right) = \bm{\beta}^T\hat{\bm{\sigma}}.
\end{equation*}
Noting that $g(\cdot\,;\bm{\beta})$ is continuous and applying @thm-clt-extremes, we have
\begin{equation*}
        \sqrt{k}\bm{\beta}^T (\hat{\bm{\sigma}}-\bm{\sigma}) = \sqrt{k}\left(\hat{\mathbb{E}}_{H}[g(\bm{\Theta};\bm{\beta})] - \mathbb{E}_{H}[g(\bm{\Theta};\bm{\beta})]\right) \to N(0,v(\bm{\beta})).
\end{equation*}
where $v(\bm{\beta}):=\mathrm{Var}_{H}(g(\bm{\Theta};\bm{\beta}))$. The asymptotic normality of $\hat{\bm{\sigma}}$ follows by the Cramér-Wold device. The diagonal elements of the covariance matrix $V$ are as in @prp-empirical-tpdm-normality-entries. The off-diagonal entries are given by
\begin{align*}
        2\mathrm{Cov}\left(\sqrt{k}(\hat{\sigma}_{ij} - \sigma_{ij}),\sqrt{k}(\hat{\sigma}_{lm} - \sigma_{lm})\right) 
        &=2k\,\mathrm{Cov}(\hat{\sigma}_{ij}, \hat{\sigma}_{lm}) \\
        &= k\left[\mathrm{Var}(\hat{\sigma}_{ij} + \hat{\sigma}_{lm}) - \mathrm{Var}(\hat{\sigma}_{ij}) - \mathrm{Var}(\hat{\sigma}_{lm})\right] \\
        &\to \mathrm{Var}_{H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) - \nu_{ij}^2 - \nu_{lm}^2. \qedhere
\end{align*}
:::

### Derivation of $V$ under the max-linear model {#sec-app-tpdm-V-max-linear}

Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is max-linear with $q$ factors and parameter matrix $A\in\R_+^{d\times q}$. Then, for any $i,j=1,\ldots,d$, we have $\sigma_{ij}=\sum_{l=1}^q a_{il}^{\alpha/2}a_{jl}^{\alpha/2}$ and
\begin{equation*}
        \nu_{ij}^2 
        = d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} (\theta_i\theta_j)^\alpha \,\dee H(\bm{\theta}) - \sigma_{ij}^2 
        = d\sum_{s=1}^q \|\bm{a}_s\|_\alpha^\alpha \left(\frac{a_{is}a_{js}}{\|\bm{a}_s\|_\alpha^2}\right)^\alpha - \sigma_{ij}^2 
        = d \sum_{s=1}^q \frac{(a_{is}a_{js})^\alpha}{\|\bm{a}_s\|_\alpha^\alpha} - \sigma_{ij}^2.
\end{equation*}
For any pair of upper-triangular index pairs $(i,j)$ and $(l,m)$, we have
\begin{align*}
     \mathrm{Var}_{H} & (\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) \\
     &= d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} [(\theta_i\theta_j)^\alpha + 2(\theta_i\theta_j\theta_l\theta_m)^{\alpha/2} +(\theta_l\theta_m)^\alpha] \,\dee H(\bm{\theta}) - \left[ \sigma_{ij} + \sigma_{lm} \right]^2 \\
     &= d \sum_{s=1}^q \frac{(a_{is}a_{js})^\alpha + 2(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2} + (a_{ls}a_{ms})^\alpha}{\|\bm{a}_s\|_\alpha^\alpha} - \left[ \sigma_{ij} + \sigma_{lm} \right]^2 \\
     &= \nu_{ij}^2 + \nu_{lm}^2 + d \sum_{s=1}^q \frac{2(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2}}{\|\bm{a}_s\|_\alpha^\alpha} - 2\sigma_{ij}\sigma_{lm}
\end{align*}
and therefore
\begin{equation*}
    2\rho_{ij,lm}
        = d \sum_{s=1}^q \frac{2(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2}}{\|\bm{a}_s\|_\alpha^\alpha} - 2\sigma_{ij}\sigma_{lm}.
\end{equation*}
The expressions for $\nu_{ij}^2$ and $\rho_{ij,lm}$ can be summarised as
\begin{equation}\label{eq-max-linear-V}
        v_{ij,lm} = d \sum_{s=1}^q \frac{(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2}}{\|\bm{a}_s\|_\alpha^\alpha} - \sigma_{ij}\sigma_{lm}.
\end{equation}


## PCA in general finite-dimensional Hilbert spaces {#sec-app-pca-hilbert}

In classical multivariate analysis, principal component analysis (PCA) is the flagship method for reducing the dimension of a random vector. PCA identifies linear subspaces that minimise the distance between the data and its low-dimensional projections. This implicitly assumes an underlying algebraic-geometric structure. Specifically, PCA requires one to work in a Hilbert space $\mathcal{H}$. Without this theoretical foundation, it is meaningless to speak of principal components as orthogonal basis vectors or consider low-rank reconstructions as unique projections onto a subspace. A Hilbert space comprises a $d$-dimensional vector space with operations $\oplus$ and $\odot$ endowed with an inner product $\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}$. The induced norm and metric are $\|\cdot\|_{\mathcal{H}}=\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}^{1/2}$ and $d_{\mathcal{H}}(\bm{x},\bm{y})=\|\bm{x}\ominus\bm{y}\|_{\mathcal{H}}$, respectively. In most applications $\mathcal{H}=\R^d$ with the usual Euclidean geometry. This thesis will additionally consider PCA in alternative spaces, including $\R_+^d$ and $\mathbb{S}_{+(1)}^{d-1}$. However, in each case, the Hilbert space in question will be isometric to the usual Euclidean space $(\R^d, \left\langle\cdot,\cdot\right\rangle)$. That is, there exists an isomorphism $h:\mathcal{H}\to\R^d$ such that for any $\bm{x},\bm{y}\in\mathcal{H}$,
\begin{equation*}
    \left\langle\bm{x},\bm{y}\right\rangle_{\mathcal{H}} = \left\langle h(\bm{x}),h(\bm{y})\right\rangle, \qquad \|\bm{x}\ominus \bm{y}\|_{\mathcal{H}} = \|h(\bm{x})-h(\bm{y})\|_2.
\end{equation*}
We present PCA for random vectors in $\R^d$, with the understanding that the data may have undergone an isometric transformation in pre-processing and outputs may need to be back-transformed to lie in the original space. This transform/back-transform approach is equivalent to conducting the analysis in the original space with appropriately generalised notions of mean, variance, etc. [@pawlowsky-glahnGeometricApproachStatistical2001].

\begin{table}[]
\small
\begin{tabular}{@{}llll@{}}
\toprule
$\mathcal{H}$ & $\R^d$ & $\R_+^d$ & $\mathbb{S}_{+(1)}^{d-1}$ \\ \midrule
$h:\mathcal{H}\to\R^d$ & $h(\bm{x})=\bm{x}$ & $h(\bm{x}) = \tau^{-1}(\bm{x}) = \log[\exp(\bm{x})-1]$ & $h(\bm{x})=\mathrm{clr}(\bm{x})=\log[\bm{x}/\bar{g}(\bm{x})]$ \\
$h^{-1}:\R^d\to\mathcal{H}$ & $h^{-1}(\bm{y})=\bm{y}$ & $h^{-1}(\bm{y}) = \tau(\bm{y}) = \log[1+\exp(\bm{y})]$ & $h^{-1}(\bm{y}) = \mathrm{clr}^{-1}(\bm{y})=\mathcal{C}\exp(\bm{y})$ \\
$\bm{x}\oplus\bm{y}$ & $\bm{x} + \bm{y}$ & $\tau[\tau^{-1}(\bm{x})+\tau^{-1}(\bm{y})]$ & $\mathcal{C}(x_1y_1,\ldots,x_dy_d)$ \\
$\alpha\odot\bm{x}$ & $\alpha \bm{x}$ & $\tau[\alpha \tau^{-1}(\bm{x})]$ & $\mathcal{C}(x_1^\alpha,\ldots,x_d^\alpha)$ \\
$\left\langle \bm{x},\bm{y}\right\rangle_{\mathcal{H}}$ & $\sum_{i=1}^d x_iy_i$ & $\sum_{i=1}^d \tau^{-1}(x_i)\tau^{-1}(y_i)$ & $\sum_{i=1}^d \log[x_i/\bar{g}(\bm{x})]\log[y_i/\bar{g}(\bm{x})]$ \\ \bottomrule
\end{tabular}
\end{table}

Suppose $\bm{Y}=(Y_1,\ldots,Y_d)$ is a random vector in $\R^d$ satisfying $\mathbb{E}[\|\bm{Y}\|_2^2]<\infty$. Let $\bm{Y}_1,\ldots,\bm{Y}_n$ be independent copies of $\bm{Y}$. The reconstruction error of a subspace $\mathcal{S}\subseteq\R^d$ is measured as
\begin{equation}\label{eq-pca-true-risk}
    R(\mathcal{S}) := \mathbb{E}[\|\bm{Y}-\Pi_{\mathcal{S}}\bm{Y}\|_2^2] 
\end{equation}
Fundamental to PCA are the eigenvectors $\bm{u}_1,\ldots,\bm{u}_d\in\R^d$ and respective eigenvalues $\lambda_1\geq \ldots \geq \lambda_d \geq 0$ of the positive semi-definite matrix
\begin{equation*}
    \Sigma=\mathbb{E}[\bm{Y}\bm{Y}^T].
\end{equation*}
The entries of $\Sigma$, herein referred to as the non-centred covariance matrix, are the second-order moments of $\bm{Y}$.  By a change of basis, the random vector $\bm{Y}$ may be equivalently decomposed as 
\begin{equation*}
    \bm{Y} = \sum_{j=1}^d \left\langle \bm{Y},\bm{u}_j\right\rangle \bm{u}_j.
\end{equation*}
The scores $V_j:=\left\langle \bm{Y},\bm{u}_j \right\rangle$ represent the stochastic basis coefficients when $\bm{Y}$ is decomposed into the basis $\{\bm{u}_1,\ldots,\bm{u}_d\}$. They satisfy $\mathbb{E}[V_iV_j]=\lambda_i\ind\{i=j\}$. For $1\leq p < d$, the truncated expansion 
\begin{equation*}
    \hat{\bm{Y}}^{[p]} := \sum_{j=1}^p V_j \bm{u}_j = \Pi_{\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_p\}}\bm{Y}.
\end{equation*}
produces the optimal $p$-dimensional projection of $\bm{Y}$. In other words, the subspace $\mathcal{S}_p=\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_p\}$ minimises the criterion \eqref{eq-pca-true-risk} over $\mathcal{V}_p$, the set of all linear subspaces of dimension $p$ of $\R^d$. It is the unique minimiser provided the multiplicity of $\lambda_p$ is one. The corresponding risk is determined by the eigenvalues of the discarded components via $R(\mathcal{S}_p)=\sum_{j>p}\lambda_j$. 

In practice, the covariance matrix is unknown so \eqref{eq-pca-true-risk} cannot be minimised directly. Instead we resort to an empirical risk minimisation (ERM) approach, whereby the risk is replaced by
\begin{equation}\label{eq-pca-empirical-risk}
    \hat{R}(\mathcal{S}) := \frac{1}{n} \sum_{i=1}^n \|\bm{Y}_i-\Pi_{\mathcal{S}}\bm{Y}_i\|_2^2
\end{equation} 
Minimisation of the empirical risk follows analogously based on the empirical non-centred covariance matrix 
\begin{equation*}
    \hat{\Sigma}=\frac{1}{n}\sum_{i=1}^n \bm{Y}_i\bm{Y}_i^T
\end{equation*}
and its ordered eigenpairs $(\hat{\lambda}_j,\hat{\bm{u}}_j)$ for $j=1,\ldots,d$. For $p=1,\ldots,d$ and $i=1,\ldots,n$, the rank-$p$ reconstruction of $\bm{Y}_i$ is given by
\begin{equation*}
    \hat{\bm{Y}}_i^{[p]} := \sum_{j=1}^p \hat{V}_{ij} \bm{u}_j = \Pi_{\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\}}\bm{Y},
\end{equation*}
where $\hat{V}_{ij}:=\left\langle \bm{Y}_i,\bm{u}_j\right\rangle$.
The subspace $\hat{\mathcal{S}}_p=\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\}$ minimises \eqref{eq-pca-empirical-risk} in $\mathcal{V}_p$; the objective at the minimum is $\hat{R}(\hat{\mathcal{S}}_p)=\sum_{j>p}\hat{\lambda}_j$.

Usually the dimension of the target subspace (if it exists) is unknown, so the number of retained components $p$ must be selected according to some criterion. At the heart of this choice is a trade-off between dimension reduction and approximation error. Selecting $p=\max\{j:\hat{\lambda}_j>0\}$ results in perfect reconstructions but the reduction in dimension will be minimal if any. Excessive compression incurs information loss and destroys key features of the data. Several criteria for selecting the number of retained components based on the eigenvalues have been proposed. These include stopping when the reconstruction error $\sum_{j>p}\hat{\lambda}_j$ is acceptably small, cutting off components with $\lambda_j<1$, or retaining components based on where the `scree plot' forms an elbow.  

If $\bm{Y}$ is mean-zero (or the $n\times d$ data matrix is column-centred in pre-processing), then $\Sigma$ is the covariance matrix of $\bm{Y}$ and the procedure is termed centred PCA. In this case, PCA can be equivalently reformulated in terms of finding low-dimensional projections that maximally preserve variance. In the non-centred case this interpretation is not valid, the projections merely maximise variability around the origin. A detailed comparison between centred PCA and non-centred PCA is conducted in @cadimaRelationshipsUncentredColumnCentred2009. They obtain relationships between and bounds on the eigenvectors/eigenvalues of the non-centred and standard covariance matrices. Based on their theoretical analysis and a series of example, they conclude that both types of PCA generally produce similar results. In particular, the leading eigenvector (up to sign and scaling) of the non-centred covariance matrix is very often close to the vector of the column means of the data matrix. Thus the first non-centred principal component essentially relates to the centre of the data.

## Literature review: clustering with the TPDM {#sec-app-tpdm-clustering}

While clustering methods are not central to this thesis, only being used once in @sec-eva-c4, we consider it worthwhile to describe some TPDM-related clustering algorithms. The two algorithms described in this section are closely connected with other topics discussed in this thesis, such as principal components analysis and support identification. Therefore, reviewing these algorithms can enrich the reader's understanding of these other aspects.

Within multivariate extremes, the umbrella term `clustering' has many meanings. To avoid confusion, we briefly describe these and clarify which type we are referring to. 

- **Prototypical events.** Assume that the angular measure concentrates at/near a small number of points in $\mathbb{S}_{+}^{d-1}$. Then one might wish to identify cluster centres $\bm{w}_1,\ldots\bm{w}_K$ minimising some objective function of the form
\begin{equation}\label{eq-clustering-objective-function}
    \mathbb{E}_{H}\left[\min_{l=1,\ldots,K}\varrho(\bm{\Theta},\bm{w}_l)\right],
\end{equation}
where $\varrho:\mathbb{S}_+^{d-1}\times \mathbb{S}_+^{d-1}\to[0,1]$ is some distance/dissimilarity function. The cluster centres can be interpreted as the directions of prototypical extremes events. See @chautruDimensionReductionMultivariate2015, @janssenKmeansClusteringExtremes2020 and @medinaSpectralLearningMultivariate2021 for further details.
- **Identification of concomitant extremes.** Suppose that angular measure is supported on a set of $K\ll 2^{d-1}$ subspaces (faces) of the simplex $C_{\beta_1},\ldots,C_{\beta_K}$, where $\beta_1,\ldots,\beta_K\in\mathcal{P}(\{1,\ldots,d\})\setminus\emptyset$ and
\begin{equation*}
    C_\beta = \{\bm{\theta}\in\mathbb{S}_+^{d-1}:\theta_i>0 \iff i\in\beta\}.
\end{equation*}
Only those groups (`clusters') of components indexed by $\beta_1,\ldots,\beta_K$ may be simultaneously extreme. Identification of the support of the angular measure is notoriously challenging because the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ lie (almost surely) in the interior of the simplex. @goixSparseRepresentationMultivariate2017 and @simpsonDeterminingDependenceStructure2020 identify clusters according to whether observations fall within appropriately sized rectangular/conic neighbourhoods of the corresponding axis in $\R_+^d$. @meyerSparseRegularVariation2021 take a different approach, whereby the angular component is defined with respect to the Euclidean projection [@liuEfficientEuclideanProjections2009] rather than usual projection based on self-normalisation. The geometry of the projection is such that the projected data lie on subfaces of the simplex. The price paid is that the limiting conditional distribution of the angles is related to, but not identical to, the angular measure.
- **Partitioning into AD/AI groups components.** This notion of clustering is related to the previous type. We assume that the variables $X_1,\ldots,X_d$ can be partitioned into $K$ clusters, such that $X_i$ and $X_j$ are asymptotically dependent if and only if they belong to the same cluster. In other words, there exists $2\leq K \leq d$ and a partition $\beta_1,\ldots,\beta_K$ of $\{1,\ldots,d\}$ such that the angular measure is supported on $C_{\beta_1},\ldots,C_{\beta_K}$ or lower-dimensional subspaces thereof, i.e.
\begin{equation*}
    H\left(\bigcup_{l=1}^K \bigcup_{\beta'_l\subseteq \beta_l} C_{\beta'_l}\right) = m.
\end{equation*}
The task of modelling the dependence structure of $\bm{X}$ can be divided into lower-dimensional sub-problems involving the random sub-vectors $\bm{X}_{\beta_1},\ldots,\bm{X}_{\beta_K}$. If $K=d$, then all variables are asymptotically independent. The underlying hypothesis is very strong and unlikely to hold in practice. Nevertheless, it is often a useful simplifying modelling assumption. @bernardClusteringMaximaSpatial2013 propose grouping components using the $k$-medoids algorithm [@kaufmanFindingGroupsData1990] with a dissimilarity matrix populated with pairwise measures of tail dependence, similar to $\chi_{ij}$ and $\sigma_{ij}$. The approaches of @fomichovSphericalClusteringDetection2023 and @richardsModernExtremeValue2024 involve the TPDM; these are reviewed in greater detail below.

@fomichovSphericalClusteringDetection2023 show that the latter kind of clustering may be performed using the framework of the first kind. They provide a link between the principal eigenvector $\bm{u}_1$ of the TPDM and the minimiser of the objective \eqref{eq-clustering-objective-function} with quadratic cost $\varrho(\bm{\theta},\bm{\phi})=\left\langle\bm{\theta},\bm{\phi}\right\rangle^2$ and $K=1$:
\begin{equation*}
    \min_{\bm{\theta}\in\mathbb{S}_{+(2)}^{d-1}} \mathbb{E}_{H}\left[\varrho(\bm{\Theta},\bm{\theta})\right] = \mathbb{E}_{H}\left[\varrho(\bm{\Theta},\bm{u}_1)\right].
\end{equation*}
Note that $\bm{u}_1\in\mathbb{S}_{+(2)}^{d-1}$ is assumed to be suitably normalised with all entries being non-negative; the Perron-Frobenius theorem guarantees this is possible. This result informs an iterative clustering procedure called spherical $k$-principal-components. Consider a set of extremal angles $\bm{\theta}_{(1)},\ldots,\bm{\theta}_{(k)}\in\mathbb{S}_{+(2)}^{d-1}$ and current centroids $\hat{\bm{w}}_{1},\ldots,\hat{\bm{w}}_{K}\in\mathbb{S}_{+(2)}^{d-1}$. A single iteration of their procedure yields new centroids $\hat{\bm{w}}_{1}^\star,\ldots,\hat{\bm{w}}_{K}^\star\in\mathbb{S}_{+(2)}^{d-1}$ given by the respective principal eigenvectors of
\begin{equation*}
    \hat{\Sigma}^{[i]} = \sum_{l=1}^k \bm{\theta}_{(l)}\bm{\theta}_{(l)}^T\ind\{\argmin_{j=1,\ldots,K}\varrho(\bm{\theta}_{(l)},\bm{w}_j)=i\}, \qquad (i=1,\ldots,K).
\end{equation*}
The matrix $\hat{\Sigma}^{[i]}$ represents the empirical TPDM (up to some multiplicative constant) based on the nearest neighbours of the $i$th centroid. @fomichovSphericalClusteringDetection2023 prove that, under certain conditions, the limiting centroids lie in a neighbourhood of the faces of interest $C_{\beta_1},\ldots,C_{\beta_K}$. Thresholding the centroid vectors yields the final partition $\beta_1,\ldots,\beta_K$.

@richardsModernExtremeValue2024 apply hierarchical clustering using the empirical TPDM as the underlying similarity matrix. The clustering method constitutes a minor aspect of their submission to the EVA (2023) Data Challenge. Few methodological details are provided, so the following explanation constitutes our interpretation of their method, drawing on Figure 4 in @richardsModernExtremeValue2024 and the accompanying code made available at [https://github.com/matheusguerrero/yalla](https://github.com/matheusguerrero/yalla). Define the dissimilarity between $X_i$ and $X_j$ as $\varrho_{ij} = 1 - \sigma_{ij}$. This satisfies the properties of a dissimilarity measure:
\begin{equation*}
    \varrho_{ij} \geq 0, \qquad \varrho_{ii} = 0, \qquad \varrho_{ij}=\varrho_{ji}.
\end{equation*}
The $d\times d$ dissimilarity matrix $\mathcal{D}=1-\Sigma=(\varrho_{ij})$ can be fed into standard hierarchical clustering algorithms. Agglomerative hierarchical clustering initially assigns each variable belongs to its own cluster, i.e. $\beta_i=\{i\}$ for $i=1,\ldots,d$. The algorithm proceeds iteratively, repeatedly joining together the two closest clusters until some stopping criterion is satisfied. Under complete-linkage clustering, the distance between clusters $\beta\neq\beta'$ is given by $\max\{\varrho_{ij} : i\in\beta,j\in\beta'\}$. The merging process may be stopped when there is a sufficiently small number of clusters or when the clusters are sufficiently separated.
