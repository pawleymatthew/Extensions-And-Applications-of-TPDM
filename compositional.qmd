# A compositional perspective on multivariate extremes

```{r compositional-load-packages}
#| include: false
library(tidyverse)
library(tidyr)
library(magrittr)
library(scales)
library(ggh4x)
library(ggpubr)
library(colorspace)
library(pbapply)
library(kableExtra)
library(reshape2)
library(compositions)
library(Ternary)
library(SpatialExtremes)
library(caret)
library(lattice)

options(dplyr.summarise.inform = FALSE)
options(knitr.kable.NA = "")
```

```{r compositional-source-functions}
#| include: false
sapply(list.files(path = "R/compositional", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
sapply(list.files(path = "R/general", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
sapply(list.files(path = "R/background", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

## Motivation

In multivariate extreme value statistics, the analysis of angular distributions is central to understanding the tail dependence structure. The angular measure describes the direction of large observations, focussing on the relative proportions of variables at extreme levels rather than their absolute values. Compositional data analysis (CoDA) provides tools to handle data that carry only relative information, enabling a more robust inference by accounting for the constraints imposed by their simplex structure [@aitchisonStatisticalAnalysisCompositional1986]. EVA and CoDA have been connected on several occasions: @colesModellingExtremeMultivariate1991 remark on the similarity between parametric models for CoDA and models for the angular measure; @decarvalhoStatisticsExtremesChallenges2016 sketch a possible approach to extremal PCA that involves applying "PCA for compositional data to the pseudo-angles" to "disentangle dependence into components of practical interest"; @serranoSemiparametricBivariateExtremevalue2022 construct bivariate extreme value copulas using compositional splines. Apart from these sporadic cases, the connection between these two disciplines remains unexplored in the current literature. This chapter aims to demonstrate this untapped potential. 

First, we will review the core CoDA principles and concepts and draw high-level connections to MEV analysis. Then, we provide some tangible examples that illustrate our point more concretely. Specifically, we apply a CoDA lens to two statistical learning problems within multivariate extremes: tail dimension reduction via principal components analysis [@clemenconRegularVariationHilbert2024; @cooleyDecompositionsDependenceHighdimensional2019; @dreesPrincipalComponentAnalysis2021] and binary classification in extreme regions [@jalalzaiBinaryClassificationExtreme2018]. We demonstrate that off-the-shelf CoDA methods are readily applicable to these problems and are competitive against the current state-of-the-art.

## Compositional data analysis

### Compositions and the Aitchison geometry

Compositional data analysis is a relatively modern statistical discipline that has been adopted across various fields including economics, geology, biology, political science, and many others [@alenaziReviewCompositionalData2023]. Its widespread applicability stems from the ubiquity of compositional data, i.e. data whose components represent parts of a whole and convey only relative information. For example, a geologist analysing the chemical composition of rock samples is interested in the relative abundances of its constituent elements rather than the absolute amounts (which ultimately depend on the size of the rock sample). Two vectors $\bm{x},\bm{y}\in\R_+^d:=(0,\infty)^d$ are compositionally equivalent, denoted $\bm{x}\sim\bm{y}$, if there exists $c>0$ such that $\bm{y}=c\bm{x}$. The equivalence relation $\sim$ defines equivalence classes on $\R_+^d$. For $\bm{x}\in\R_+^d$, the compositional class $[\bm{x}]:=\{c\bm{x}:c>0\}$ may be represented by a closed composition on the $d$-part unitary simplex,
\begin{equation*}
    \mathcal{C}\bm{x} := \frac{\bm{x}}{\|\bm{x}\|_1}\in\mathbb{S}_{+(1)}^{d-1}.
\end{equation*}
Thus the natural sample space of compositional random vectors is the $L_1$-simplex $\mathbb{S}_{+(1)}^{d-1}$. In his seminal paper, @aitchisonStatisticalAnalysisCompositional1982 contends that standard multivariate analysis techniques are inappropriate for modelling compositions because they are intended for unconstrained data. Neglecting the compositional constraint causes an array of difficulties, including detecting spurious correlations [@pearsonMathematicalContributionsTheory1897; @aitchisonStatisticalAnalysisCompositional1982], difficulties representing the structure of the data [@aitchisonPrincipalComponentAnalysis1983], and contradictory conclusions between analyses [@aitchisonStatisticalAnalysisCompositional1986]. The consensus solution is to work in a statistical framework that satisfies the core CoDA principles laid out by @aitchisonStatisticalAnalysisCompositional1986:

1. **(Scale invariance)** Analyses based on the raw observations $\bm{x}_1,\ldots,\bm{x}_n\in\R_+^d$ and on the closed compositions $\mathcal{C}\bm{x}_1,\ldots,\mathcal{C}\bm{x}_n\in\mathbb{S}_{+(1)}^{d-1}$ should yield the same results. In other words, the magnitudes of $\bm{x}_1,\ldots,\bm{x}_n$ are not relevant for inference. 
2. **(Subcompositional coherence)** Analyses based on the full composition and any subcomposition should yield consistent conclusions.
3. **(Permutation invariance)** Analyses should yield equivalent results when parts in the composition are permuted. 

Adherence to the foundational CoDA principles is achieved by discarding Euclidean geometry and instead working in the so-called Aitchison geometry [@aitchisonStatisticalAnalysisCompositional1986]. Formally, this involves constructing a Hilbert space structure on the interior of $\mathbb{S}_{+(1)}^{d-1}$ [@pawlowsky-glahnGeometricApproachStatistical2001]. Suppose $\bm{x},\bm{y}\in\mathbb{S}_{+(1)}^{d-1}\setminus \partial\mathbb{S}_{+(1)}^{d-1}$, where
\begin{equation*}
\partial\mathbb{S}_{+(1)}^{d-1} := \{\bm{x}\in \mathbb{S}_{+(1)}^{d-1} : \exists i \, x_i=0\}
\end{equation*}
is the simplex boundary. For $\alpha\in\R$, the perturbation and power operations are defined by
\begin{equation*}
    \bm{x} \oplus \bm{y} = \mathcal{C}(x_1y_1,\ldots,x_dy_d),\qquad \alpha \odot \bm{x} = \mathcal{C}(x_1^\alpha,\ldots,x_d^\alpha).
\end{equation*}
The real vector space $(\mathbb{S}_{+(1)}^{d-1},\odot,\oplus$) has additive identity $\bm{e}_a:=\mathcal{C}(1,\ldots,1)$ and inverse elements $\ominus \bm{x} := \mathcal{C}(x_1^{-1},\ldots,x_d^{-1})$. Central to the Aitchison geometry is the centred log-ratio (CLR) transformation
\begin{equation*}
    \mathrm{clr}:\mathbb{S}_{+(1)}^{d-1} \to \R^d,\qquad \bm{x}\mapsto \log\left(\frac{\bm{x}}{\bar{g}(\bm{x})}\right),
\end{equation*}
where $\bar{g}(\bm{x}):=(\prod_{i=1}^d x_i)^{1/d}$ denotes the geometric mean of the components of $\bm{x}$. The inverse transformation is $\mathrm{clr}^{-1}(\bm{v})=\mathcal{C}\exp(\bm{v})$. With this transformation, the inner product may be defined in terms of $\langle \cdot,\cdot\rangle_e$, the Euclidean inner product in $\R^d$, as
\begin{equation*}
      \langle \bm{x},\bm{y}\rangle_a := \langle \mathrm{clr}(\bm{x}),\mathrm{clr}(\bm{y})\rangle_e,
\end{equation*}
The Aitchison norm and distance are the metric elements induced by $\langle \cdot,\cdot \rangle_a$, given by
\begin{equation}
      \|\bm{x}\|_a := \langle \bm{x},\bm{x} \rangle_a^{1/2} = \|\mathrm{clr}(\bm{x})\|_2, \qquad d_a(\bm{x},\bm{y}) := \|\bm{x} \ominus \bm{y}\|_a = \|\mathrm{clr}(\bm{x}) - \mathrm{clr}(\bm{y})\|_2.
\end{equation} 
The CLR transformation is an isometry between the Aitchison simplex and the Euclidean hyperplane
\begin{equation*}
  \mathcal{T}^{d-1}:=\{\bm{y}\in\R^d : y_1+\ldots+y_d=0 \}\subset\R^d
\end{equation*}
passing through the origin and parallel to the simplex in $\R^d$. Therefore CoDA methods can be formulated in two equivalent ways: in terms of compositions in the Aitchison geometry (the in-the-simplex approach) or based on the CLR-transformed vectors in the Euclidean geometry (the out-of-the-simplex approach). If one chooses the in-the-simplex route, statistical and probabilistic concepts must be appropriately generalised. For example, @pawlowsky-glahnGeometricApproachStatistical2001 define measures of central tendency and variability for compositions as
\begin{align*}
\mathrm{cen}_a(\bm{X}) &:= \argmin_{\bm{y}\in\mathbb{S}_+^{d-1}}\mathbb{E}[d_a^2(\bm{X},\bm{y})] = \mathcal{C}(\exp(\mathbb{E}[\log(\bm{X})]), \\
\mathrm{totVar}_a(\bm{X}) &:= \mathbb{E}[d_a^2(\bm{X},\mathrm{cen}_a(\bm{X}))] = \sum_{j=1}^d \mathrm{Var}([\mathrm{clr}(\bm{X})]_j).
\end{align*}

### Connections between CoDA and multivariate extremes

Suppose $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ and let $\bm{X}_1,\ldots,\bm{X}_n$ be independent copies of $\bm{X}$. Inference for $H$ is typically based on the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ associated with radial threshold exceedances. This process is rigorously justified by the MRV property. Extreme events may be represented by their angles with zero loss of information since the radii $R_{(1)},\ldots,R_{(k)}$ are uninformative for $H$, except insofar as they must exceed a high threshold. Extremal dependence modelling therefore proceeds under an assumption of scale invariance, the first axiom of CoDA. Moreover, choosing the $L_1$-norm implies that $H$ is a distribution on $\mathbb{S}_{+(1)}^{d-1}$ and $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ are random compositions in $\mathbb{S}_{+(1)}^{d-1}\setminus \partial\mathbb{S}_{+(1)}^{d-1}$ almost surely. Thus CoDA techniques are readily applicable to the kind of data that arise in such analyses. 

### Comparison between Euclidean and Aitchison distances on the simplex

The Aitchison distance is a simplicial metric (i.e. satisfies the CoDA axioms) and is therefore an appropriate measure of distance between compositions [@aitchisonCriteriaMeasuresCompositional1992]. In general, the Aitchison dissimilarity $\|\bm{x}\ominus\bm{y}\|_a$ between two compositions $\bm{x},\bm{y}\in\mathbb{S}_{+(1)}^{d-1}\setminus \partial\mathbb{S}_{+(1)}^{d-1}$ behaves very differently to $\|\bm{x}-\bm{y}\|_2$. @fig-aitchison-distance provides further insight into this. Each ternary plot graphs the distance between $d_{\bullet}(\bm{x},\bm{y})$ as a function of $\bm{y}$ with respect to some fixed point $\bm{x}$, where $\bullet\in\{a,e\}$. The fixed points are $\bm{x}=(1/3,1/3,1/3)$ (top row) and $\bm{x}=(0.1, 0.45, 0.45)$ (bottom row). The left- and right-hand plots correspond to Euclidean and Aitchison distances, respectively. Consider the plots in the top row. The Euclidean distance is bounded by $\sqrt{2/3}\approx 0.82$, the distance between the centroid and any vertex, since the simplex is a bounded subspace of $\R^d$. In contrast, the Aitchison distance $d_{a}(\bm{x},\bm{y})$ is unbounded. CLR transforms are based on log-ratios and $\log y$ diverges to $\pm\infty$ as $y\to 0$ or $y\to\infty$. Thus, points close to the boundary should be regarded as being close to infinity [@parkKernelMethodsRadial2022]. In the top-left plot, distance decays in a concentric fashion. Consequently, the point labelled A, which may be interpreted as being arbitrarily close to its neighbouring edge, is deemed closer to the centre than point B. On the other hand, the Aitchison contours are decidedly non-concentric and the points C and D are approximately equidistant to the centre. In the bottom plots, the fixed point $\bm{x}=(0.1, 0.45, 0.45)$ is located at the 'hotspot' near the right-hand edge. Intuitively, the Euclidean metric (bottom left) says that point E is very close to $\bm{x}$, while F is much further away. However, in the Aitchison geometry (bottom right) the distance between G and $\bm{x}$ becomes arbitrarily large as G is nudged towards the boundary, so that eventually H is actually closer to $\bm{x}$ than G! 

These examples demonstrate that, when analysing data on the simplex, the choice of distance metric is of huge practical importance and should not dismissed as merely a mathematical technicality. 

```{r make-fig-aitchison-distance}
#| label: fig-aitchison-distance
#| fig-cap: "Visualisation of the Euclidean distance (left) and Aitchison distance (right) between points on $\\mathbb{S}_{+(1)}^{d-1}$ and the reference points $(1/3, 1/3, 1/3)$ (top) and $(0.1, 0.45, 0.45)$ (bottom). The labelled points A-H are referred to in the body text."
#| fig-scap: "Blah."
#| fig-height: 5
#| message: false

library(ggtern)
res <- readRDS(file.path("scripts", "compositional", "results", "aitchison_distance.RDS"))

fancy_scientific_long <- function(l) { 
  l %>%
    format(scientific = TRUE) %>%
    gsub("^(.*)e", "'\\1'e", .) %>%
    gsub("e", "%*%10^", .) %>%
    parse(text = .) 
}

p1 <- ggtern(filter(res, distance_type == "euclidean_distance", ref_point == "centre"), aes(A, B, C)) +
  geom_point(aes(color = distance), size = 2, alpha = 0.6) +
  scale_color_gradientn(colors = sequential_hcl(100, palette = "Inferno", rev = TRUE),
                        trans = "log10", breaks = breaks_log(n = 6), labels = fancy_scientific_long) +
  geom_mask() +
  annotate(geom = "text", x = 0.48, y = 0.48, z = 0.04, label = "A", colour = "white") +
  annotate(geom = "text", x = 0.8, y = 0.1, z = 0.1, label = "B", colour = "white") +
  labs(color = "") + xlab("") + ylab("") + Llab("") + Tlab("") + Rlab("") +
  guides(color = guide_colorbar(barheight = unit(4, "cm"), barwidth = unit(0.6, "cm"))) +
  theme_hidegrid() +
  theme_hidelabels()

p2 <- ggtern(filter(res, distance_type == "aitchison_distance", ref_point == "centre"), aes(A, B, C)) +
  geom_point(aes(color = distance), size = 2, alpha = 0.6) +
  scale_color_gradientn(colors = sequential_hcl(100, palette = "Inferno", rev = TRUE),
                        trans = "log10", breaks = breaks_log(n = 6), labels = fancy_scientific_long) +
  geom_mask() +
  annotate(geom = "text", x = 0.46, y = 0.46, z = 0.08, label = "C", colour = "white") +
  annotate(geom = "text", x = 0.8, y = 0.1, z = 0.1, label = "D", colour = "white") +
  labs(color = "") + xlab("") + ylab("") + Llab("") + Tlab("") + Rlab("") +
  guides(color = guide_colorbar(barheight = unit(4, "cm"), barwidth = unit(0.6, "cm"))) +
  theme_hidegrid() +
  theme_hidelabels()

p3 <- ggtern(filter(res, distance_type == "euclidean_distance", ref_point == "off_centre"), aes(A, B, C)) +
  geom_point(aes(color = distance), size = 2, alpha = 0.6) +
  scale_color_gradientn(colors = sequential_hcl(100, palette = "Inferno", rev = TRUE),
                        trans = "log10", breaks = breaks_log(n = 6), labels = fancy_scientific_long) +
  geom_mask() +
  annotate(geom = "text", x = 0.02, y = 0.49, z = 0.49, label = "E", colour = "white") +
  annotate(geom = "text", x = 0.6, y = 0.2, z = 0.2, label = "F", colour = "white") +
  labs(color = "") + xlab("") + ylab("") + Llab("") + Tlab("") + Rlab("") +
  guides(color = guide_colorbar(barheight = unit(4, "cm"), barwidth = unit(0.6, "cm"))) +
  theme_hidegrid() +
  theme_hidelabels()

p4 <- ggtern(filter(res, distance_type == "aitchison_distance", ref_point == "off_centre"), aes(A, B, C)) +
  geom_point(aes(color = distance), size = 2, alpha = 0.6) +
  scale_color_gradientn(colors = sequential_hcl(100, palette = "Inferno", rev = TRUE),
                        trans = "log10", breaks = breaks_log(n = 6), labels = fancy_scientific_long) +
  geom_mask() +
  annotate(geom = "text", x = 0.02, y = 0.49, z = 0.49, label = "G", colour = "white") +
  annotate(geom = "text", x = 0.6, y = 0.2, z = 0.2, label = "H", colour = "white") +
  labs(color = "") + xlab("") + ylab("") + Llab("") + Tlab("") + Rlab("") +
  guides(color = guide_colorbar(barheight = unit(4, "cm"), barwidth = unit(0.6, "cm"))) +
  theme_hidegrid() +
  theme_hidelabels()

ggtern::grid.arrange(p1, p2, p3, p4, ncol = 2)
detach(package:ggtern, unload = TRUE)
```

## Compositional PCA for extremes

### Motivation

Compositional principal component analysis (CoDA-PCA) aims at finding low-dimensional representations of compositional data via projections onto linear subspaces of the Aitchison simplex [@aitchisonPrincipalComponentAnalysis1983]. It is inadvisable to apply classical PCA to compositions for three reasons. First, spurious correlations arising from the compositional constraint mean it is unreliable as an exploratory tool for analysing the correlation structure between variables [@aitchisonStatisticalAnalysisCompositional1982]. Second, compositional data often exhibit curvature that cannot be captured by traditional linear methods, leading to a loss of efficiency [@aitchisonPrincipalComponentAnalysis1983]. Third, interpretability is hindered by the fact that low-dimensional projections of the data are not guaranteed to lie in the simplex. These problems are solved by performing PCA in the Aitchison Hilbert space. 

### Methodology

Suppose $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ and let $H$ be the angular measure with respect to the $L_1$-norm $\|\cdot\|_1$. Let $\bm{X}_1,\ldots,\bm{X}_n$ be independent random vectors with the same distribution as $\bm{X}$. Following the spirit of @dreesPrincipalComponentAnalysis2021, we seek to minimise the limiting mean-squared reconstruction error associated with the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ upon projecting onto low-dimensional linear subspace. For any Aitchison subspace $\mathcal{S}\subset\mathbb{S}_{+(1)}^{d-1}$, let $\Pi_{\mathcal{S}}$ denote the orthogonal projection (matrix) onto $\mathcal{S}$ and define the true asymptotic risk
\begin{equation*}
    R(\mathcal{S}) := \mathbb{E}_{H}[\| \bm{\Theta} \ominus \Pi_{\mathcal{S}} \bm{\Theta} \|_a^2].
\end{equation*}
For some $1\leq p \leq d-1$, let $\mathcal{V}_p$ be the class of linear subspaces of dimension $p$ in $\mathbb{S}_{+(1)}^{d-1}$. As usual, the minimisers of $R(\mathcal{S})$ over $\mathcal{V}_p$ corresponds to the principal eigenspace of a covariance matrix. The following result explains how the CoDA-PCA procedure outlined by @aitchisonStatisticalAnalysisCompositional1982 applies to our setting. 

:::{#prp-coda-pca-true-risk-minimiser}
Let $\tilde{\bm{\Theta}}\sim d^{-1}H$ be a random composition. Assume $\tilde{\bm{\Theta}}$ is centred (without loss of generality) and has finite total variance. Let $\Gamma=\mathrm{Cov}(\mathrm{clr}(\tilde{\bm{\Theta}}))$ be the CLR-covariance matrix of $\tilde{\bm{\Theta}}$ and $\Gamma=\Omega D\Omega^T$ its eigendecomposition, where $D$ is a diagonal matrix of eigenvalues $\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_{d-1} \geq \lambda_d = 0$ and $\Omega$ is an orthonormal $d\times d$ matrix whose columns are the eigenvectors $\bm{\omega}_1,\ldots,\bm{\omega}_{d-1}\in\mathcal{T}^{d-1}$ and $\bm{\omega}_d\propto\bm{1}_d$. The back-transformed eigenvectors $\{\bm{u}_1,\ldots,\bm{u}_{d-1}\}:=\{\mathrm{clr}^{-1}(\bm{\omega}_1),\ldots,\mathrm{clr}^{-1}(\bm{\omega}_{d-1})\}$ form an orthonormal basis of $\mathbb{S}_{+(1)}^{d-1}$ and for any $1\leq p \leq d-1$, 
\begin{equation*}
\mathcal{S}_p^\star := \mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_{p}\} = \left\lbrace \bigoplus_{j=1}^p (\tau_j \odot \bm{u}_j) : \tau_1,\ldots,\tau_p \in\R \right\rbrace
\end{equation*}
minimises $R$ over $\mathcal{V}_p$.
:::

::: {.proof}
Let $\tilde{\bm{Y}}:=\mathrm{clr}(\tilde{\bm{\Theta}})$. By assumption, $\tilde{\bm{Y}}$ has zero mean and finite second order moments, since
\begin{equation*}
\mathbb{E}[\tilde{\bm{Y}}] 
= \argmin_{\bm{\mu}}\mathbb{E}[d_e^2(\tilde{\bm{Y}},\bm{\mu})] 
= \argmin_{\bm{\mu}}\mathbb{E}[d_a^2(\tilde{\bm{\Theta}},\mathrm{clr}^{-1}(\bm{\mu}))] 
= \mathrm{clr}(\bm{e}_a) 
= \bm{0}
\end{equation*}
and 
\begin{equation*}
\mathrm{Var}(\tilde{Y}_j) = \mathrm{Var}([\mathrm{clr}(\tilde{\bm{\Theta}})]_j)) < \infty.
\end{equation*}
Using well-known results from classical PCA -- e.g. Theorem 5.3 in @seberMultivariateObservations1984 -- we have that 
\begin{equation*}
\{\bm{\omega}_1,\ldots,\bm{\omega}_p\} = \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\R^d} \mathbb{E}\left[\|\tilde{\bm{Y}} - \Pi_{\mathrm{span}\{\bm{v}_1,\ldots,\bm{v}_p\}}\tilde{\bm{Y}}\|_2^2\right].
\end{equation*}
Observe that for any scalar $a\in\R$ and compositional vectors $\bm{x},\bm{y}\in\mathbb{S}_{+(1)}^{d-1}$,
\begin{equation*}
\mathrm{clr}((a\odot\bm{x}) \oplus \bm{y}) = a\,\mathrm{clr}(\bm{x}) + \mathrm{clr}(\bm{y})
\end{equation*}
and therefore
\begin{align*}
\{\bm{\omega}_1,\ldots,\bm{\omega}_p\} 
&=\argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\R^d} \mathbb{E}\left[\|\tilde{\bm{Y}} - \Pi_{\mathrm{span}\{\bm{v}_1,\ldots,\bm{v}_p\}}\tilde{\bm{Y}}\|_2^2\right] \\
&= \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\R^d} \mathbb{E}[d_e^2(\tilde{\bm{Y}},\Pi_{\mathrm{span}\{\bm{v}_1,\ldots,\bm{v}_p\}}\tilde{\bm{Y}})] \\
&= \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\R^d} \mathbb{E}[d_a^2(\tilde{\bm{\Theta}},\mathrm{clr}^{-1}(\Pi_{\mathrm{span}\{\bm{v}_1,\ldots,\bm{v}_p\}}\tilde{\bm{Y}}))] \\
&= \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\R^d} \mathbb{E}\left[d_a^2\left(\tilde{\bm{\Theta}},\mathrm{clr}^{-1}\left(\sum_{j=1}^p \langle \tilde{\bm{Y}},\bm{v}_j \rangle \bm{v}_j\right)\right)\right] \\
&= \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\R^d} \mathbb{E}\left[d_a^2\left(\tilde{\bm{\Theta}},\bigoplus_{j=1}^p \langle \tilde{\bm{\Theta}},\mathrm{clr}^{-1}(\bm{v}_j) \rangle_a \odot \mathrm{clr}^{-1}(\bm{v}_j)\right)\right].
\end{align*}
This optimisation depends on each vector $\bm{v}_j$ only through $\mathrm{clr}^{-1}(\bm{v}_j)$, so can map the problem to the simplex, yielding
\begin{align*}
\{\mathrm{clr}^{-1}(\bm{\omega}_1),\ldots,\mathrm{clr}^{-1}(\bm{\omega}_p)\} 
&= \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\mathbb{S}_{+(1)}^{d-1}} \mathbb{E}\left[d_a^2\left(\tilde{\bm{\Theta}},\bigoplus_{j=1}^p \langle \tilde{\bm{\Theta}},\bm{v}_j \rangle_a \odot \bm{v}_j\right)\right] \\
&= \argmin_{\bm{v}_1,\ldots,\bm{v}_p\in\mathbb{S}_{+(1)}^{d-1}} R(\mathrm{span}\{\bm{v}_1,\ldots,\bm{v}_p\})
\end{align*}
Since $\mathcal{S}_p^\star = \mathrm{span}\{\mathrm{clr}^{-1}(\bm{\omega}_1),\ldots,\mathrm{clr}^{-1}(\bm{\omega}_p)\}$, this completes the proof.
:::

The finite total variance condition prohibits the angular measure from placing mass on the simplex boundary and restricts how much mass may be concentrated near the boundary. A sufficient condition is $\mathrm{Var}_H(\log (\Theta_i/\Theta_j))<\infty$ for all $i\neq j$. No such condition is required by @dreesPrincipalComponentAnalysis2021 because finite variances are guaranteed by boundedness of the simplex in $\R^d$. The assumption that $\tilde{\bm{\Theta}}$ is centred at the simplex barycentre is not equivalent to the moment constraint \eqref{eq-H-mean-constraints} on $H$. Informally, the moment constraints dictate the arithmetic mean of $H$, whereas the compositional centre represents the geometric mean. Finally, we point out that $\mathrm{rank}(\Gamma)=d-1$ in accordance with the dimensions of $\mathbb{S}_{+(1)}^{d-1}$ and $\mathcal{T}^{d-1}$. For compositional PCA we need only consider reconstructions up to rank $d-1$, whereas @dreesPrincipalComponentAnalysis2021 and @cooleyDecompositionsDependenceHighdimensional2019 require all $d$ components to be retained to guarantee perfect reconstruction (in general).

In practice $H$ is unknown, so we must resort to minimising an empirical risk based on the $k$ most extreme data points. The empirical risk is defined by
\begin{equation*}
    \hat{R}(\mathcal{S}) 
    := \hat{\mathbb{E}}_{H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_a^2] 
    = \frac{d}{k}\sum_{i=1}^k \|\bm{\Theta}_{(i)}-\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}\|_a^2.
\end{equation*}
By replacing $H$ with $\hat{H}$ in @prp-coda-pca-true-risk-minimiser, one can show that the minimiser of $\hat{R}$ in $\mathcal{V}_p$ is simply the principal subspace
\begin{equation*}
\hat{S}_p=\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\},
\end{equation*}
where $\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_{d-1}\in \mathbb{S}_{+(1)}^{d-1}$ are the back-transformed principal eigenvectors of the empirical CLR-covariance matrix [@egozcueModellingCompositionalData2018]
\begin{equation*}
\hat{\Gamma} = \frac{1}{k-1} \sum_{i=1}^k \mathrm{clr}(\bm{\Theta}_{(i)} \ominus \bar{\bm{\Theta}})\mathrm{clr}(\bm{\Theta}_{(i)} \ominus \bar{\bm{\Theta}})^T, \qquad \bar{\bm{\Theta}} = \frac{1}{k}\bigoplus_{i=1}^{k} \bm{\Theta}_{(i)}.
\end{equation*}

Our setup is identical to @dreesPrincipalComponentAnalysis2021, except that all algebraic-geometric concepts (linear, projection, orthogonal, etc.) are interpreted in the Aitchison sense. This subtle change fundamentally alters the problem in two different ways. While we still optimise over the class of linear subspaces, we refer to a different notion of linearity to @dreesPrincipalComponentAnalysis2021. Linear trends are now represented by compositional straight lines $\{\bm{a} \oplus (\tau \odot \bm{b}):\tau\in\R\}\subset\mathbb{S}_{+(1)}^{d-1}$ for some $\bm{a},\bm{b}\in\mathbb{S}_{+(1)}^{d-1}$. From a Euclidean perspective these 'lines' are curved, so compositional PCA is able to capture features of the data that appear non-linear -- see Figure 1b in @aitchisonPrincipalComponentAnalysis1983 for an illustration of this phenomenon. The second crucial difference is that our objective criterion perceives reconstruction errors in the Aitchison rather than Euclidean metric. Section XX showed that these two measures can produce wildly different assessments of how close two points are; in fact, for any $\epsilon>0$ and $\varepsilon>0$, there exists $\bm{x},\bm{y}\in\mathbb{S}_{+(1)}^{d-1}$ such that $d_e(\bm{x},\bm{y})<\epsilon$ and $d_a(\bm{x},\bm{y})>\epsilon$. The Aitchison loss is especially sensitive to reconstruction errors near the boundary, steering the optimiser to target these regions more closely.

### Simulation experiments

We now compare the performance of our proposed procedure against @dreesPrincipalComponentAnalysis2021, Herein referred to as CoDA-PCA and DS-PCA, respectively. Let $\bm{X}\in\mathcal{RV}_+^{d}(1)$ be on 1-Fréchet margins with angular measure $H$. We generate independent realisations $\bm{x}_1,\ldots,\bm{x}_n$ of $\bm{X}$ and, for a fixed level $k$, compute the optimal linear subspaces $\hat{\mathcal{S}}_1,\ldots,\hat{\mathcal{S}}_{p-1}$ using CoDA-PCA and DS-PCA. 

#### Performance metrics

We assess performance in two ways: mean-squared reconstruction error (MSRE) and tail event probability estimation. To quantify reconstruction error we use the asymptotic risk of $\mathcal{S}$ under the Aitchison and Euclidean metrics, i.e.
\begin{align*}
\mathcal{L}_a(\mathcal{S}) &= \mathbb{E}_H[\|\bm{\Theta} \ominus \Pi_{\mathcal{S}}\bm{\Theta}\|_a^2 \ind\{\Pi_{\mathcal{S}}\bm{\Theta}\in(0,\infty)^d\}], \\
\mathcal{L}_e(\mathcal{S}) &= \mathbb{E}_H[\|\bm{\Theta} - \Pi_{\mathcal{S}}\bm{\Theta}\|_2^2\}].
\end{align*}
These quantities can be computed to arbitrary precision using Monte Carlo estimation. To sample from the true angular measure we employ the \texttt{rmevspec} function in the \texttt{mev} package. The inclusion of $\ind\{\Pi_{\mathcal{S}}\bm{\Theta}\in(0,\infty)^d\}$ ensures that $\mathcal{L}_a$ is well-defined in cases where DS-PCA yields reconstructed vectors that lie outside of the positive orthant. In such cases, the Aitchison distance is undefined. Naively, one might consider projecting invalid reconstructions on to the positive orthant, but this does not really resolve the core issue. Naturally such points should be sent somewhere near the coordinate axes, but by adjusting *how close* you place the point the Aitchison loss can be made arbitrarily large. Instead, we simply ignore such points. Arguably this choice favours DS-PCA, since we do not penalise its failure to produce valid reconstructions. 

Measuring reconstruction error gives a high-level summary of how closely the low-dimensional approximation of $H$ resembles the true measure over its entire support. The second class of performance metrics focusses on assessing performance in specific regions of the simplex that are relevant for estimating tail event probabilities. Consider the estimands
\begin{align}
p_{\min} &:= \lim_{u\to\infty}\mathbb{P}(\min\bm{X} > u \mid \|\bm{X}\|_1 > u) = \mathbb{E}_{H}\left[\min_{j=1,\ldots,d}\Theta_j\right], \label{eq-prob-min-H} \\
p_{\max} &:= \lim_{u\to\infty}\mathbb{P}(\max\bm{X} > u \mid \|\bm{X}\|_1 > u) = \mathbb{E}_{H}\left[\max_{j=1,\ldots,d}\Theta_j\right]. \label{eq-prob-max-H}
\end{align}
These probabilities specify the minimal and maximal contributions of variable to the norm. The true probabilities may be computed via Monte Carlo simulation or analytically. To assess a principal subspace $\hat{\mathcal{S}}_p$, we compute estimates of $p_{\min}$ and $p_{\max}$ using the empirical angular measure *after* projecting the data onto $\hat{\mathcal{S}}_p$. Specifically, for any subspace $\mathcal{S}$ we define
\begin{equation*}
\hat{H}_{\mathcal{S}}(\cdot) := \frac{d}{\sum_{i=1}^k \ind\{\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}\in(0,\infty)^d\}} \sum_{i=1}^k \delta_{\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}}(\cdot)\ind\{\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}\in(0,\infty)^d\}
\end{equation*}
and compute
\begin{equation*}
\hat{p}_{\min}(\mathcal{S}) := \mathbb{E}_{\hat{H}_{\mathcal{S}}}\left[\min_{j=1,\ldots,d}\Theta_j\right], \qquad
\hat{p}_{\max}(\mathcal{S}) := \mathbb{E}_{\hat{H}_{\mathcal{S}}}\left[\max_{j=1,\ldots,d}\Theta_j\right]. 
\end{equation*}
The performance of each dimension reduction method is assessed by examining the sequence of estimates $\hat{p}_{\min}(\hat{\mathcal{S}}_1),\ldots,\hat{p}_{\min}(\hat{\mathcal{S}}_{p-1})$ and comparing against the true probability $p_{\min}$ and the (uncompressed) empirical estimate $\hat{p}_{\min}(\mathbb{S}_{+(1)}^{d-1})$. 

#### Max-linear model with compositionally colinear factors

Our first experiment involves max-linear random vectors. The parameter matrix $A$ will be constructed in a particular way so as to nicely illustrates the benefits of our proposed approach. We stock the factor matrix $A\in\R_+^{d\times q}$ with power-perturbation combinations of linearly independent compositions $\bm{v}_1,\ldots,\bm{v}_s\in\mathbb{S}_{+(1)}^{d-1}$, that is $A=(\bm{a}_1,\ldots,\bm{a}_q)$ and for each $j=1,\ldots,q$,
\begin{equation*}
\mathcal{C}\bm{a}_j = \bigoplus_{l=1}^s (\beta_{jl}\odot \bm{v}_l) 
\end{equation*}
for some scalars $\beta_{j1},\ldots,\beta_{js}\in\R$. Then the angular measure of the associated max-linear random vector $\bm{X}\in\mathcal{RV}_+^d(1)$ is
\begin{equation*}
    H(\cdot) = \sum_{j=1}^q \|\bm{a}_j\|_1 \delta_{\bigoplus_{l=1}^s (\beta_{jl}\odot \bm{v}_l)}(\cdot)
\end{equation*}
The support of $H$ is spanned (in the Aitchison geometry) by $\bm{v}_1,\ldots,\bm{v}_s$. If $s<d-1$, then the angular measure may be represented by a low-dimensional object with no information loss incurred.

We start with a low-dimensional example to facilitate visualisation. Let $d=3$, $s=1$, $q=50$ and $\bm{v}_1=(0.12,0.58,0.3)$. The values $\beta_{11},\ldots,\beta_{q1}$ are sampled uniformly between -4 and 4. The resulting matrix is shown in @fig-sim-pca-example-data (left). Since the angular measure is discrete, the true probabilities \eqref{eq-prob-min-H} and \eqref{eq-prob-max-H} can be computed analytically as $p_{\min}\approx 0.636$ and $p_{\max}\approx 0.068$. Realisations of $\bm{X}$ are generated using the max-stable (MS) construction $\bm{X} = A \times_{\max} \bm{Z}$ and the transformed-linear (TL) construction $\bm{X} = A \otimes \bm{Z}$ defined in \eqref{eq-max-linear-X} and \eqref{eq-max-linear-X-cooley}. 
For both processes, we generate 50 datasets of size $n=5\times 10^3$ and run the PCA methods with $k/n=0.01$. *The results for other values of $k$ and $n$ are very similar -- see Appendix XX.* 

```{r make-fig-sim-pca-example-A}
#| label: fig-sim-pca-example-A
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 4.5
#| warning: false

A <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-az-A-matrix.RDS"))
p1 <- plot_tpdm(A, x_labels = FALSE, y_labels = FALSE) %>% update(aspect = 0.6)

A <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-az-highdim-A.RDS"))
p2 <- plot_tpdm(A, x_labels = FALSE, y_labels = FALSE) %>% update(aspect = 0.6)

ggarrange(p1, p2, ncol = 2)
```

The exploratory plots in @fig-sim-pca-example-data provides some insight into the data structure and the mechanics of the two procedures. In each plot, the support of the angular measure (i.e. the points $\mathcal{C}\bm{a}_1,\ldots,\mathcal{C}\bm{a}_{50}$) is represented by the green diamonds. By construction, these points exhibit a one-dimensional structure and lie on the compositional straight line $\{\tau\odot(0.12,0.58,0.3):\tau\in\R\}$. The black points are the extremal angles $\bm{\theta}_{(1)},\ldots,\bm{\theta}_{(k)}$ taken from one set of realisations from the TL max-linear random vector (left) and the MS max-linear random vector (middle and left). The TL data are noisier in the sense that the extremal angles do not tend to lie exactly on the green points, whereas the MS angles converge to the limiting distribution very quickly. The left and middle plots show the results of CoDA-PCA. The red and blue dashed lines represents the first and second (sample) principal axes, respectively. The method correctly identifies the true low-dimensional structure, though there is some estimation error visible in the TL case (left). The red crosses represent the rank-one reconstructions obtained by projecting the black points onto the red line. The rank-two reconstructions always have zero error since $\mathbb{S}_{+(1)}^{2}$ is a two-dimensional space. The right-hand plot depicts the results of DS-PCA for the same MS dataset. Now we add blue crosses showing the rank-two reconstructions. Clearly, there are several issues to discuss. The one-dimensional projections lie along a straight line (the process of visualising the line on a ternary plot distorts it slightly) and do a poor job of describing the data. Some points lie outside of the triangle; this means the projected vector had a negative component. The rank-two reconstructions do a much better job, but are still imperfect because DS-PCA treats the data as points in $\R^3$.  

```{r make-fig-sim-pca-example-data}
#| label: fig-sim-pca-example-data
#| fig-cap: "Examples of CoDA-PCA (left and middle) and DS-PCA (right) applied to max-linear data from the TL construction (left) and MS construction (middle and right). The green diamonds represent the true angular measure. The black points are the angular components associated with the $k=50$ largest observations among a sample of size $n=5000$. The red and blue dashed lines represent the first and second principal axes, respectively. The red and blue crosses represent the projections onto the first and second principal subspaces, respectively."
#| fig-scap: "Example of CoDA-PCA and DS-PCA on max-linear data."
#| fig-height: 6
#| warning: false

A <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-az-A-matrix.RDS"))

set.seed(12)
n <- 5000
k_frac <- 0.01

par(mfrow = c(1, 3), mar = c(0, 0, 0, 0), omi = c(0, 0, 0, 0))
# RV + coda pca
tmp <- extreme_pca(X_train = rmaxlinearfactor(n = n, A = A, type = "cooley"),
                   X_test = rmaxlinearfactor(n = n, A = A, type = "cooley"), 
                   k = floor(n * k_frac),
                   method = "coda", n_pcs = 1, plot = TRUE)
TernaryPoints(t(A), col = "green", cex = 1, pch = 5)
# generate MS data
X_tmp <- rmaxlinearfactor(n = n, A = A, type = "maxlin")
# coda pca
tmp <- extreme_pca(X_train = X_tmp, X_test = X_tmp, 
                   k = floor(n * k_frac), method = "coda", n_pcs = 1, plot = TRUE)
TernaryPoints(t(A), col = "green", cex = 1, pch = 5)

# ds pca
tmp <- extreme_pca(X_train = X_tmp, X_test = X_tmp,
                   k = floor(n * k_frac), method = "ds", n_pcs = 1:2, plot = TRUE)
TernaryPoints(t(A), col = "green", cex = 1, pch = 5)
```

The full results are shown in @fig-sim-pca-max-linear-loss. The four sub-plots show the distributions of the four performance measures: the Aitchison MSRE $\mathcal{L}_a$ (top left), the Euclidean MSRE (top right), estimates $\hat{p}_{\min}$ (bottom left), and estimates $\hat{p}_{\max}$ (bottom right). Within each sub-plot, the two sub-panels are based on the two data generating processes. The horizontal axis indicates which PCA method is being used and the number of retained principal components, $p$. As expected from @fig-sim-pca-example-data, CoDA-PCA produces excellent reconstructions when $p=1$ under *both* the Aitchison and Euclidean loss. The picture for DS-PCA is more mixed. According to $\mathcal{L}_e$, adding a second component improves the reconstructions considerably. The Aitchison loss $\mathcal{L}_a$ disagrees, suggesting that the two-dimensional projections are poor approximations of points near the simplex boundary. This can be observed in @fig-sim-pca-example-data (right) near the upper and right-hand vertices. In terms of reconstruction error, there is not any noticeable difference between the MS and TL results. Now consider the tail probability estimates in the bottom row. The boxplots are the estimates $\hat{p}_{\bullet}(\hat{\mathcal{S}}_p)$ and the blue dashed line marks the true probability $p_{\bullet}$. CoDA-PCA with $p=1$ yields very good estimates in the sense of (i) being close to the true value and (ii) being as good as the $p=2$ estimate resulting from the uncompressed empirical angular measure. The estimates are slightly biased for the TL data, suggesting that the extremal angles converge to their limiting distribution in such a way that biases the principal eigenvector of the empirical CLR-covariance matrix. For DS-PCA, $\hat{p}_{\min}(\hat{\mathcal{S}}_1)$ and $\hat{p}_{\max}(\hat{\mathcal{S}}_1)$ show a large positive and negative bias, respectively. Adding a second component reduces the bias considerably but the uncertainty is quite large. Euclidean loss treats all errors equally irrespective of where they occur in the simplex, so the PCA solution finds a compromise between good fit near the centre and near the boundary. In contrast, the Aitchison loss prioritises good reconstruction of points near the boundary, resulting in more stable PCA solutions. Analogous plots based on samples of size $n=50,000$ are shown in Appendix XX. The results are virtually identical, confirming that errors are due to methodological deficiencies, not a lack of data. Even with infinite samples DS-PCA will fit a straight line to curved data!

```{r make-fig-sim-pca-max-linear-loss}
#| label: fig-sim-pca-max-linear-loss
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 7
#| warning: false

res <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-az.RDS")) %>%
  filter(n_train == 5000, k_frac == 0.01)

p1 <- res %>%
  mutate(maxlin_model = case_when(maxlin_model == "cooley" ~ "Transformed-linear",
                                  .default = "Max-stable")) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(n_train = as.factor(n_train), k_frac = as.factor(label_percent()(k_frac))) %>%
  rename('n' = n_train, 'k/n' = k_frac) %>%
  pivot_longer(cols = c(test_loss_aitchison, test_loss_euclidean), names_to = "loss_type", values_to = "test_loss") %>%
  filter(loss_type == "test_loss_aitchison") %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = test_loss)) +
  geom_boxplot() +
  facet_grid(. ~ maxlin_model) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.05)), breaks = breaks_extended(n = 4)) +
  scale_x_discrete(guide = "axis_nested") +
  labs(fill = "PCA method",
       x = "",
       y = "Aitchison reconstruction error") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))

p2 <- res %>%
  mutate(maxlin_model = case_when(maxlin_model == "cooley" ~ "Transformed-linear",
                                  .default = "Max-stable")) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(n_train = as.factor(n_train), k_frac = as.factor(label_percent()(k_frac))) %>%
  rename('n' = n_train, 'k/n' = k_frac) %>%
  pivot_longer(cols = c(test_loss_aitchison, test_loss_euclidean), names_to = "loss_type", values_to = "test_loss") %>%
  filter(loss_type == "test_loss_euclidean") %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = test_loss)) +
  geom_boxplot() +
  facet_grid(. ~ maxlin_model) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.05)), breaks = breaks_extended(n = 4)) +
  scale_x_discrete(guide = "axis_nested") +
  labs(fill = "PCA method",
       x = "",
       y = "Euclidean reconstruction error") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))

p3 <- res %>%
  mutate(maxlin_model = case_when(maxlin_model == "cooley" ~ "Transformed-linear",
                                  .default = "Max-stable")) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(n_train = as.factor(n_train), k_frac = as.factor(label_percent()(k_frac))) %>%
  rename('n' = n_train, 'k/n' = k_frac) %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = p_min_hat / 3)) +
  geom_boxplot() +
  facet_grid(. ~ maxlin_model) +
  geom_hline(aes(yintercept = p_min / 3), colour = "blue", linetype = "dashed") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = breaks_extended(n = 4)) +
  scale_x_discrete(guide = "axis_nested") +
  labs(fill = "PCA method",
       x = "",
       y = expression(hat(p)[min])) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))

p4 <- res %>%
  mutate(maxlin_model = case_when(maxlin_model == "cooley" ~ "Transformed-linear",
                                  .default = "Max-stable")) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(n_train = as.factor(n_train), k_frac = as.factor(label_percent()(k_frac))) %>%
  rename('n' = n_train, 'k/n' = k_frac) %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = p_max_hat / 3)) +
  geom_boxplot() +
  facet_grid(. ~ maxlin_model) +
  geom_hline(aes(yintercept = p_max / 3), colour = "blue", linetype = "dashed") +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = breaks_extended(n = 4)) +
  scale_x_discrete(guide = "axis_nested") +
  labs(fill = "PCA method",
       x = "",
       y = expression(hat(p)[max])) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))

ggarrange(p1, p2, p3, p4, nrow = 2, ncol = 2)
```

The three-dimensional example is instructive but limited in terms of comparing dimension reducing capabilities. Our next experiment is based on the same construction with $d=10$, $q=50$ and $s=2$. The vectors $\bm{v}_1,\bm{v}_2\in\mathbb{S}_{+(1)}^{9}$ and scalars $\{\beta_{jl}:j=1,\ldots,50,\,l=1,2\}$ are generated randomly producing the matrix in @fig-sim-pca-example-data (right). The results in @fig-sim-pca-az-highdim-loss are based on 25 repeated simulations from the MS construction with $n=10^4$ and $k/n=0.02$. The left-hand plot shows the average cumulative proportion of the total variance as a function of the number of retained components. These are computed as $\sum_{j\leq p}\hat{\lambda}_j/\sum_{j=1}^d \hat{\lambda}_j$, where $\{\hat{\lambda}_j:j=1,\ldots,10\}$ are the sample eigenvalues of the TPDM or CLR-covariance matrix. Note that for CoDA-PCA it is the total variance $\mathrm{totVar}_a(\Pi_{\hat{S}_p}\bm{\Theta})$ being measured. The dashed line represents the 95\% threshold. In PCA a common rule-of-thumb is to retain as many components as necessary to explain 95\% of the variance. Based on this criterion, CoDA-PCA correctly identifies that two components are sufficient, whereas DS-PCA requires one additional dimension. However, this only means that three components are adequate for capturing *Euclidean* variability in the angles. Hopefully the reader is by now persuaded that this is not sufficient for a good description of extremal dependence. The right-hand plot confirms our hypothesis. Here we see estimates $\hat{p}_{\max}(\hat{\mathcal{S}}_p)$ against $p$ for each PCA algorithm. CoDA-PCA estimates $p_{\max}$ very accurately with $p=2$. DS-PCA requires at least six or seven components. This stark contrast emphasises our point that diagnosing PCA performance using the Euclidean notions of reconstruction error or proportion of variance can be rather misleading. 

```{r make-fig-sim-pca-az-highdim-loss}
#| label: fig-sim-pca-az-highdim-loss
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 4
#| warning: false

res <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-az-highdim.RDS"))

p1 <- res %>%
  filter(maxlin_model == "maxlin") %>%
  filter(n_pcs <= 6) %>%
  group_by(n_pcs, pca_method) %>%
  summarise(prop_var = median(prop_var)) %>%
  ungroup() %>%
  ggplot(aes(x = as.factor(n_pcs), y = prop_var, colour = pca_method, group = pca_method)) +
  geom_point() +
  geom_line() +
  geom_hline(aes(yintercept = 0.95), colour = "black", linetype = "dashed") +
  scale_colour_manual(labels = c("CoDA", "DS"), values = c("red", "blue")) +
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.005)), labels = label_percent()) +
  labs(x = "Number of PCs",
       colour = "",
       y = "Cumulative prop. of (total) variance") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(legend.position = c(0.8, 0.2),
        legend.background = element_rect(fill = "transparent"),
        legend.key = element_rect(fill = "transparent"))

p2 <- res %>%
  filter(maxlin_model == "maxlin") %>%
  mutate(maxlin_model = case_when(maxlin_model == "cooley" ~ "Regular variation",
                                  .default = "Max-stable")) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  ggplot(aes(x = as.factor(n_pcs), y = p_max_hat / 10)) +
  geom_boxplot() +
  geom_hline(aes(yintercept = p_max / 10), colour = "blue", linetype = "dashed") +
  facet_grid(pca_method ~ .) +
  labs(fill = "",
       x = "Number of PCs",
       y = expression(hat(p)[max])) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light()

ggarrange(p1, p2, ncol = 2)
```


```{r}
Gamma_list <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-hr-threedim-Gamma.RDS"))
Lambda_threedim_latex <- lapply(seq_along(Gamma_list), function(ind) {
  paste0("\\Lambda_",
        ind,
        "=\\begin{pmatrix}",
        paste(apply(round(Gamma_list[[ind]]/4, 3), 1, paste, collapse = " & "), collapse = " \\\\ "),
        "\\end{pmatrix}")
}) %>%
  paste(collapse = ",\\quad ")
```

#### Hüsler-Reiss

Now we consider more realistic examples where the data are generated from four different Hüsler-Reiss models. For $i=1,\ldots,4$, suppose $\bm{X}_i\in\mathcal{RV}_+^d(1)$ follows a Hüsler-Reiss distribution parametrised by $\Lambda_i\in\R_+^{d\times d}$. For the first three examples we set $d=3$ and
$$ `r Lambda_threedim_latex` $$
The fourth model is 10-dimensional example with $\Lambda_4\in\R_+^{10\times 10}$ constructed in such a way that there are three clusters of asymptotically dependent variables -- see the corresponding pairwise tail dependence coefficients $\chi_{ij}$ in @fig-sim-pca-hr-highdim-param (left). The entries of $\Lambda_1,\ldots,\Lambda_4$ were randomly generated following the steps in Appendix B1 in @fomichovSphericalClusteringDetection2023. 

The exploratory plots in @fig-sim-pca-hr-threedim-example-data show that $\Lambda_1,\Lambda_2,\Lambda_3$ induce qualitatively different dependence structures. The plots' interpretation is the same as in @fig-sim-pca-example-data. Each ternary plot displays $k=250$ extremal angles extracted from $n=10^4$ samples of $\bm{X}$ under the three models. For $\Lambda_1$ (left) and $\Lambda_3$ (right) the data variability is approximately one-dimensional with differing degrees of curvature. Under $\Lambda_2$ (middle) the data cloud is two-dimensional. The dependence structure corresponding pairwise tail dependence coefficients $\chi_{ij}$ is shown in @fig-sim-pca-hr-highdim-param (left). There are three clusters of asymptotically dependent variables. The pairwise dependencies are strong in clusters 1 and 3 and more variable in cluster 2. This structure can also be seen via the CLR-covariance matrix eigenvectors (@fig-sim-pca-hr-highdim-param, right). The leading eigenvectors $\bm{u}_1$ and $\bm{u}_2$ separate out the three clusters. The localised behaviour of cluster 2 is captured by eigenvectors $\bm{u}_3$, $\bm{u}_4$ and $\bm{u}_5$. The remaining four (non-zero) eigenvectors relate to fine-scale behaviour in clusters 1 and 3. 

```{r make-fig-sim-pca-hr-highdim-param}
#| label: fig-sim-pca-hr-highdim-param
#| fig-cap: "Matrix of tail dependence coefficients $\\chi_{ij}=2\\bar{\\Phi}(\\Lambda)$."
#| fig-scap: "Tail dependence coefficients and CoDA-PCA eigenvectors for 10-dimensional Hüsler-Reiss model."
#| fig-height: 3.5

d <- 10
Gamma <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-hr-highdim-Gamma.RDS"))
Chi <- 2 * (1 - pnorm(sqrt(Gamma) / 2)) 
rownames(Chi) <- paste0("X[", seq_len(ncol(Chi)), "]") %>% parse(text = .)
colnames(Chi) <- paste0("X[", seq_len(ncol(Chi)), "]") %>% parse(text = .)
p1 <- plot_tpdm(Chi)

# compute matrix of CoDA eigenvectors
pc <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-hr-highdim-loadings.RDS"))
U_coda <- pc$loadings[seq_len(d), seq_len(d-1)]
colnames(U_coda) <- paste0("bold(u)[", seq_len(ncol(U_coda)), "]")
p2 <- plot_tpdm_eigen(U_coda, n_col = 13, y_labels = FALSE)

ggarrange(p1, p2, ncol = 2)
```

```{r make-fig-sim-pca-hr-threedim-example-data}
#| label: fig-sim-pca-hr-threedim-example-data
#| fig-cap: "Example data from the three trivariate Hüsler-Reiss models. Based on $n=10^4$ and $k=250$."
#| fig-scap: "Example data from the three trivariate Hüsler-Reiss models."
#| fig-height: 5

Gamma_list <- readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-hr-threedim-Gamma.RDS"))
par(mfrow = c(1, 3), mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))
for (i in 1:3) {
  mev::rmev(n = 10000, d = 3, sigma = Gamma_list[[i]], model = "hr") %>%
    set_colnames(paste0("X", seq_len(ncol(.)))) %>%
    extreme_pca(X_train = .,
                X_test = ., 
                k = 250, 
                method = "coda", 
                n_pcs = 1, 
                plot = TRUE)
}
```


Similar to before, we repeatedly generate $n$ samples of each random vector $\bm{X}_i$, $i=1,\ldots,4$ and perform PCA based on the $k$ largest observations in norm. We choose $n=5,000$ and $k=50$ when $d=3$ and $n=10^4$ and $k=200$ when $d=10$. The true probabilities $p_{\min}$ and $p_{\max}$ are computed empirically from $n=10^6$ samples based on $u=100$. To three decimal places, the true values of $p_{\min}$ (resp. $p_{\max}$) under the four sub-models are found to be 0.089, 0.228, 0.081 and 0.000 (resp. 0.603, 0.456, 0.586 and 0.385). 

@fig-sim-pca-hr-aitchison-loss illustrates the Aitchison loss $\mathcal{L}_a$ for the first three models. As expected, models 1 and 3 permit an accurate one-dimensional representation, whereas model 2 requires both CoDA components. Under model 2, extremal dependence is strong and the angular measure is concentrated near the barycentre of the simplex, where the Euclidean and Aitchison geometries are most similar. Therefore both methods produce comparable outcomes. On the other hand, the extremal angles of $\bm{X}_1$ and $\bm{X}_3$ lie near the simplex boundary, so we expect the methods to diverge. It is obvious from @fig-sim-pca-hr-threedim-example-data (left) that DS-PCA with $p=1$ will fail for model 1; this is borne out in the results. Interestingly, it also performs poorly on model 3, despite the apparently linear structure of the data. This is because lines that are visually straight in a ternary diagram do not generally correspond to Euclidean straight lines. (Line segments that are visually straight and near the simplex centre may be well-approximated by a Euclidean line segment.)

```{r make-fig-sim-pca-hr-aitchison-loss}
#| label: fig-sim-pca-hr-aitchison-loss
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 3.5

res <- bind_rows(readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-hr-threedim.RDS")),
                 readRDS(file = file.path("scripts", "compositional", "results", "sim-pca-hr-highdim.RDS"))) %>%
  mutate(vario_seed_index = case_when(is.na(vario_seed_index) ~ 4, .default = vario_seed_index))

res %>%
  filter(vario_seed_index <= 3) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(vario_seed_index = paste0("Lambda[", vario_seed_index, "]")) %>%
  pivot_longer(cols = c(test_loss_aitchison, test_loss_euclidean), names_to = "loss_type", values_to = "test_loss") %>%
  filter(loss_type == "test_loss_aitchison") %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = test_loss)) +
  geom_boxplot(outliers = FALSE) +
  facet_wrap(. ~ as.factor(vario_seed_index), scales = "free", labeller = label_parsed) +
  scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, 0.05)), breaks = breaks_extended(n = 6)) +
  scale_x_discrete(guide = "axis_nested") +
  labs(x = "",
       y = "Aitchison reconstruction error") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))
```

Finally, we analyse the quality of the dimension reduction in terms of the tail event probabilities. The boxplots in @fig-sim-pca-hr-p-estimates represents the empirical distributions of $\hat{p}_{\min}$ (top two rows) and $\hat{p}_{\max}$ (top two rows) across the four test cases. The true probabilities are indicated by the blue dashed lines. For models 1-3, the results follow the same pattern as for the Aitchison loss: for models 1 and 3 one component is sufficient for CoDA but not for DS; for model 2 the PCA methods perform similarly. Recall that $p_{\min}\approx 0$ under model 4, owing to the fact that there are three clusters which act (almost) independently. DS-PCA is founded on the empirical TPDM, which tends to overestimate weak dependence -- recall the bias issue described in Section XX -- inducing a positive bias in the estimates of $p_{\min}$. The CoDA-based estimates do not suffer the same issue, suggesting the CLR-covariance matrix is more robust in such settings. Angles corresponding to events occurring in different clusters are mapped to very distant points in CLR-space. This creates large separation between clusters, enabling accurate detection of weakly dependent variables. Both methods perform similar in terms of $p_{\max}$, requiring three or four principal components to attain good estimates. However, our CoDA estimates have lower variance and are therefore superior in terms of mean squared error.

```{r make-fig-sim-pca-hr-p-estimates}
#| label: fig-sim-pca-hr-p-estimates
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 9

p1 <- res %>%
  mutate(d = case_when(vario_seed_index <= 3 ~ 3, .default = 10)) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(vario_seed_index = paste0("Lambda[", vario_seed_index, "]")) %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = p_min_hat / d)) +
  geom_boxplot() +
  geom_hline(aes(yintercept = p_min), colour = "blue", linetype = "dashed") +
  facet_wrap(. ~ as.factor(vario_seed_index), scales = "free", labeller = label_parsed) +
  scale_x_discrete(guide = "axis_nested") +
  labs(x = "",
       y = expression(hat(p)[min])) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))

p2 <- res %>%
  mutate(d = case_when(vario_seed_index <= 3 ~ 3, .default = 10)) %>%
  mutate(pca_method = case_when(pca_method == "coda" ~ "CoDA",
                                .default = "DS")) %>%
  mutate(vario_seed_index = paste0("Lambda[", vario_seed_index, "]")) %>%
  ggplot(aes(x = interaction(as.factor(n_pcs), pca_method), y = p_max_hat / d)) +
  geom_boxplot() +
  geom_hline(aes(yintercept = p_max), colour = "blue", linetype = "dashed") +
  facet_wrap(. ~ as.factor(vario_seed_index), scales = "free", labeller = label_parsed) +
  scale_x_discrete(guide = "axis_nested") +
  labs(x = "",
       y = expression(hat(p)[max])) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) +
  theme_light() +
  theme(ggh4x.axis.nestline = element_line(linetype = 2))

ggarrange(p1, p2, nrow = 2, ncol = 1)
```

## Compositional classification for extremes

### Motivation and framework

To lay out the framework for this section, we formulate the problem of binary classification in extreme regions as set out in @jalalzaiBinaryClassificationExtreme2018. Let $(\bm{X},Y)$ be a random pair with unknown joint distribution $F_{(\bm{X},Y)}$, where $Y\in\{-1,+1\}$ is a binary class label and $\bm{X}=(X_1,\ldots,X_d)$ is an $\R_+^d$-valued random vector containing covariate information that is presumed to be useful for predicting $Y$. For $\sigma\in\{-,+\}$, assume $\bm{X}\mid Y=\sigma 1$ is multivariate regularly varying with tail index $\alpha=1$ and angular measure $H_\sigma$ with respect to $\|\cdot\|_1$. Suppose $(\bm{X}_1,Y_1),\ldots,(\bm{X}_n,Y_n)$ is a labelled training set comprising independent copies of $(\bm{X},Y)$. In general classification settings, the goal is to learn a classifier $g:\R^d\to\{-1,+1\}$ that minimises the expected classification error rate 
\begin{equation*}
    \mathcal{L}(g) := \mathbb{P}(Y\neq g(\bm{X})).
\end{equation*}
By the law of total probability, this decomposes as
\begin{equation*}
\mathcal{L}(g) = \mathbb{P}(Y\neq g(\bm{X}) \mid \|\bm{X}\| \leq t) \mathbb{P}(\|\bm{X}\| \leq t) + \mathbb{P}(Y\neq g(\bm{X}) \mid \|\bm{X}\| > t) \mathbb{P}(\|\bm{X}\| > t)
\end{equation*}
for some large $t>0$. @jalalzaiBinaryClassificationExtreme2018 point out that the optimal classifier need not perform well in extreme regions of the predictor space $\{\|\bm{X}\| >t\}$, since such regions exert a negligible influence over the global prediction error. They propose building a classifier that minimises the asymptotic risk in the extremes, defined as
\begin{equation*}
    \mathcal{L}_{\infty}(g) := \lim_{t\to\infty}\mathbb{P}(Y\neq g(\bm{X}) \mid \|\bm{X}\| > t).
\end{equation*}
They prove that, under certain assumptions and a certain class $\mathcal{G}$, the optimal tail classifier $g^\star := \argmin_{g\in\mathcal{G}}\mathcal{L}_{\infty}(g)$ is of the form $g^\star(\bm{x})=g^\star(\bm{x}/\|\bm{x}\|)$ [@jalalzaiBinaryClassificationExtreme2018, Theorem 1]. That is, the optimal classifier depends on $\bm{x}$ only through its angular component. In practice, they suggest estimating $g^\star$ by minimising the empirical risk 
\begin{equation}\label{eq-extreme-classification-optimisation}
\hat{\mathcal{L}}_t(g) = \frac{1}{k} \sum_{i=1}^k \ind\{g(\bm{\Theta}_{(i)}) \neq Y_{(i)}\},
\end{equation}
where $Y_{(i)}$ is the class label associated with $\bm{X}_{(i)}$. The remainder of their paper is devoted to providing theoretical guarantees for this learning principle. They leave aside the practical issue of designing algorithms for solving \eqref{eq-extreme-classification-optimisation}, instead resorting to general-purpose classifiers such as $k$-NN and random forests. 

Being familiar with CoDA, we may recognise \eqref{eq-extreme-classification-optimisation} as simply a compositional binary classification problem. The CoDA community has developed bespoke algorithms for classifying compositional data in a way that accounts for their unique geometry [@jooBinaryClassificationCompositional2021; @martin-fernandezCriticalApproachNonParametric1998; @tsagrisImprovedClassificationCompositional2016].  The contribution of this section is to showcase the use of off-the-shelf CoDA methods for extremal classification and demonstrate their superiority over traditional classifiers intended for unconstrained data.  

### Compositional classifiers and the $\alpha$-metric

Classifiers are intrinsically linked to the geometry of the sample space. Suppose we observe training samples $(\bm{x}_i,y_i),\ldots,(\bm{x}_n,y_n)$ of $(\bm{X},Y)$ and wish to classify a new, unseen data point $\bm{x}^\star$. Consider three popular classifiers: $k$-nearest neighbours ($k$-NN), support vector machines (SVM) and random forest (RF). We assume basic familiarity with these algorithms and refer the reader to @hastieElementsStatisticalLearning2009 for more details. The $k$-NN algorithm allocates $\bm{x}^\star$ to the majority class among its $k$ nearest neighbours. The notion of neighbours implicitly assumes an underlying distance metric. An SVM finds the optimal hyperplane that maximally separates data into different classes. The definition of a hyperplane is inherently dependent on the ambient space and speaking of maximal separation assumes a distance metric. Random forests are less dependent on the geometry, but it still influences the model in terms of how points are distributed over the feature space. If $\bm{X}$ is a compositional random vector, then the CoDA philosophy dictates that each algorithm should be tailored to the geometry of the simplex. The Aitchison Hilbert space is an obvious choice. Under this approach, the data undergo a CLR-transformation before being classified in the usual way. However, @greenacreChiPowerTransformationValid2024 argues that for supervised problems where an objective performance criterion (e.g. out-of-sample classification error rate) is available, we should not be wedded to the Aitchison geometry and might consider alternatives such as the $\alpha$-metric proposed by @tsagrisImprovedClassificationCompositional2016. The $\alpha$-metric is similar to the Aitchison matrix, except the CLR-transformation is substituted with a Box-Cox type transformation.

:::{#def-alpha-transformation}
For $\alpha \geq 0$, the $\alpha$-transformation is defined by
\begin{equation*}
        \bm{z}_\alpha : \mathbb{S}_{+(1)}^{d-1} \to \R^d, \qquad \bm{x} \mapsto H \cdot \left(\frac{d (\alpha\odot \bm{x}) - \bm{1}_d}{\alpha}\right),
\end{equation*}
where $H$ is any $(d-1)\times d$ real matrix with orthonormal rows and the $\alpha=0$ case is interpreted as $\bm{z}_0(\bm{x}):=\lim_{\alpha\downarrow 0} \bm{z}_\alpha(\bm{x})$. 
:::

Typically $H$ is chosen as the Helmert matrix with its first row removed, but the classification algorithms we consider are invariant to this choice. Note that $\alpha$ is completely unrelated to the tail index of $\bm{X}$, also denoted by $\alpha$, which we have fixed equal to 1 throughout this section. 

:::{#def-alpha-metric}
Let $\bm{x},\bm{y}\in\mathbb{S}_{+(1)}^{d-1}$ be closed compositions. For $\alpha\in\R$, the $\alpha$-metric is defined as
\begin{equation*}
  d_\alpha(\bm{x},\bm{y}) := \|\bm{z}_\alpha(\bm{x}) - \bm{z}_\alpha(\bm{y})\|_e
\end{equation*}
:::

The special cases $\alpha=0$ and $\alpha=1$ are equivalent to the Aitchison and Euclidean distances, respectively. For any Euclidean classifier $g_e:\R^d\to\{-1,1\}$ and $\alpha\geq 0$, the $\alpha$-transformed compositional classifier is simply
\begin{equation*}
g_\alpha:\mathbb{S}_{+(1)}^{d-1}\to\{-1,1\}, \qquad \bm{\theta} \mapsto g_e(\bm{z}_\alpha(\bm{\theta})).
\end{equation*}
Adapting $k$-NN, SVM and RF in this way, @tsagrisImprovedClassificationCompositional2016 define three families of compositional classification algorithms, which we denote by $k$-NN($\alpha$), SVM($\alpha$) and RF($\alpha$). Each family encompasses a Euclidean- and Aitchison-based classifier corresponding to $g_0$ and $g_1$, respectively. The hyperparameter $\alpha$ may be selected by standard tuning procedures such as cross-validation and provides a way of comparing prediction error across different geometries. Crucially, if the classification error rate is lower for $\alpha=0$ than for $\alpha=1$, then it confirms that switching to a simplicial geometry is beneficial for performance.

### Simulation experiments

For our simulation experiments, we generate realisations of $\bm{X}\mid Y=y$ in $d=3$ dimensions on 1-Fréchet margins from one of three MEV models: symmetric logistic (SL), negative symmetric logistic (NL), and bilogistic (BL). The classes are balanced globally and asymptotically, meaning
\begin{equation*}
p=\mathbb{P}(Y=+1)=0.5, \qquad p_{\infty}:=\lim_{t\to\infty}\mathbb{P}(Y=+1\mid\|\bm{X}\|_1>t)=0.5.
\end{equation*}
The negative ($y=-1$) and positive ($y=+1$) class instances are generated using different (scalar) dependence parameters, denoted $\vartheta_{-1}$ and $\vartheta_1$, respectively. For the SL and NL models, $\vartheta_{\sigma}$ corresponds to the parameter $1/\gamma$ and $1/\gamma$ in @def-symmetric-logistic and @def-negative-symmetric-logistic. The BL model sets all of its parameters equal to $\vartheta_{\sigma}\in[0,1]$ -- see the documentation of the \texttt{mev} package for more details. 

From each model, we simulate $n=5\times 10^3$ training samples $(\bm{x}_1,y_1),\ldots,(\bm{x}_n,y_n)$ and use $\{(\bm{\theta}_{(i)},y_{(i)}):i=1,\ldots,k\}$ with $k=500$ to train each tail classifier. The set of classifiers we consider is $k$-NN($\alpha$) and RF($\alpha$) for $\alpha\in\{0,0.1,\ldots,0.9,1\}$ and SVM($\alpha$) for $\alpha\in\{0,0.2,\ldots,0.8,1\}$. Implementations of these algorithms are readily available in the \texttt{Compositional} and \texttt{CompositionalML} R packages. The hyperparameters (e.g. the number of neighbours in $k$-NN($\alpha$)) are chosen by minimising the estimated out-of-sample classification error through 10-fold cross-validation. The use of cross-validation for estimating asymptotic classification risk is studied in more detail in @aghbalouCrossvalidationExtremeRegions2024. The cross-validated empirical risk is used as a measure of the training error of the optimal classifier. To assess its out-of-sample, asymptotic performance, we compute an approximation of the asymptotic risk $\mathcal{L}_{\infty}$ by measuring the error rate across $10^5$ realisations from the true limit model, i.e. angles sampled from $H_{-}$ and $H_{+}$ generated via the $\texttt{rmevspec}$ function from the $\texttt{mev}$ package. All reported results are based on 100 repeated simulations. 

@fig-sim-classification-ternary illustrates one realisation of the extremal angles in the training set for each model and different combinations of $\vartheta_{-1},\vartheta_1$. In the top row, $\vartheta_1/\vartheta_{-1}=1.5$ and the classes' dependence structures are very similar, making prediction very challenging. For the bottom row, the ratio becomes $\vartheta_1/\vartheta_{-1}=3$ and the class separation becomes much greater. In general classification settings the classes may concentrate in different sub-regions of the feature space. Our setting is complicated by the fact the angular measures $H_{+}$ and $H_{-}$ are subject to the constraints \eqref{eq-H-mean-constraints}. It is impossible to encounter a scenario where, say, the red points are all near the bottom edge and the blue points are all near the opposing vertex. Instead, any class separation that occurs tends to relate to whether the points are close to the centre or near the boundary, as shown in the bottom middle and bottom right plots. For the other sub-models, the plots suggest that predictive performance will vary across the features space. For example, under the BL model with $\vartheta_{1}/\vartheta_{-1}=1.5$ (top left), points near the right-most edge can be classified as $Y=1$ with a reasonable degree of confidence, but points near the centre are near-impossible to classify.

```{r make-fig-sim-classification-ternary}
#| label: fig-sim-classification-ternary
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 5
#| message: false

library(ggtern)
readRDS(file = file.path("scripts", "compositional", "results", "classification-sim-ternary-data.RDS")) %>%
  mutate(nice_model = case_when(nice_model == "Symmetric" ~ "SL",
                                nice_model == "Bi" ~ "BL",
                                nice_model == "Negative" ~ "NL",)) %>%
  mutate(nice_model = paste0(nice_model, ": " , "vartheta[-1] == ", param0)) %>%
  mutate(param1_frac = paste0("vartheta[1]/vartheta[-1] == ", param1_frac)) %>%
  ggtern(aes(X1, X2, X3, colour = class)) +
  geom_mask() +
  geom_point(shape = 4) +
  facet_grid(param1_frac ~ nice_model, labeller = label_parsed) +
  scale_colour_manual(values = c("red", "blue"), labels = c(expression(vartheta[-1]), expression(vartheta[1]))) +
  xlab("") +
  ylab("") +
  labs(colour = "Class dependence parameter") +
  Llab(expression(X[1])) +
  Tlab(expression(X[2])) +
  Rlab(expression(X[3])) +
  theme_hidegrid() +
  theme_hidelabels() +
  theme(legend.position = "top")
detach(package:ggtern, unload = TRUE)
```


@fig-sim-classification-asymptotic-risk plots the asymptotic risk as a function of $\alpha$ for each algorithm and generative process. The coloured lines are the median risk across the 100 simulations while the shaded regions demarcate the interquartile range. As expected, the ratio between the dependence parameters dictates the difficulty level of the learning task: the error rates are between 30-40% when $\vartheta_1/\vartheta_0=1.5$ and between 2-16% when $\vartheta_1/\vartheta_0=3$. Universally, the classifiers perform worst when $\alpha=1$, i.e. in the Euclidean geometry. This supports our main argument that traditional multivariate techniques are ill-suited to tasks of this nature. For the BL and SL data, the asymptotic risk is minimised by taking $\alpha=0$. For the NL data, the optimum occurs at some intermediate value, say $0.2 \leq \alpha \leq 0.4$. Thus the optimal classifiers fall somewhere under the CoDA umbrella and are significantly different from the naive Euclidean classifiers. The choice of classifier is obviously a key determinant of performance, with $k$-NN($\alpha$) typically being the worst-performing and $\alpha$-SVM the best. A full comparison of the relative merits of each classifier is beyond the scope of our investigation. However, it should be pointed out that for each $\vartheta_1/\vartheta_0=3$ sub-case (bottom row), the worst-performing Aitchison classifier is as good as the best-performing Euclidean classifier. Switching to a more appropriate geometry reaps similar performance benefits to utilising more sophisticated classifiers.

```{r make-fig-sim-classification-asymptotic-risk}
#| label: fig-sim-classification-asymptotic-risk
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 6

readRDS(file = file.path("scripts", "compositional", "results", "classification-sim-results-knn-svm-rf.RDS")) %>%
  mutate(nice_model = case_when(nice_model == "Symmetric" ~ "SL",
                                nice_model == "Bi" ~ "BL",
                                nice_model == "Negative" ~ "NL",)) %>%
  mutate(classifier = case_when(classifier == "knn" ~ 'k * "-NN" * (alpha)',
                                classifier == "svm" ~ '"SVM" * (alpha)',
                                classifier == "rf" ~ '"RF" * (alpha)')) %>%
  mutate(param0 = paste0("vartheta[0] == ", param0),
         param1_frac = paste0("vartheta[1]/vartheta[-1] == ", param1_frac)) %>%
  ggplot(aes(x = alpha, y = test_error, colour = classifier, fill = classifier)) +
  stat_summary(geom = "line", fun = median, linewidth = 0.7) +
  stat_summary(geom = "ribbon", fun.min = function(x) quantile(x, 0.25), fun.max = function(x) quantile(x, 0.75), alpha = 0.1, colour = NA) +
  facet_nested_wrap(param1_frac ~ nice_model, scales = "free", labeller = label_parsed, nrow = 2,
                    nest_line = element_line(colour = "white")) +
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 6)) +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.05)), breaks = breaks_extended(n = 4), labels = label_percent()) +
  scale_colour_manual(values = c("red", "blue", "darkgreen"), labels = parse_format()) +
  scale_fill_manual(values = c("red", "blue", "darkgreen"), labels = parse_format()) +
  xlab(expression(alpha)) +
  ylab("Asymptotic risk") +
  labs(fill = "Classifier", colour = "Classifier") +
  theme_light() +
  theme(legend.position = "top",
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank())
```

In practice, we cannot choose $\alpha$ by examining the asymptotic risk. Instead, $\alpha$ becomes an additional hyperparameter to be selected by minimising the cross-validation training error along with the other tuning parameters. Let $\hat{\alpha}$ denote the selected value of $\alpha$ (i.e. the $\alpha$ that attains the lowest training error) and $\alpha^\star$ the true optimum with respect to the asymptotic risk. @fig-sim-classification-cv-alpha depicts kernel density estimates of the distribution of $\hat{\alpha}$ across the 100 data sets. In accordance with @fig-sim-classification-asymptotic-risk, the mode of $\hat{\alpha}$ is close to $\alpha^\star = 0$ for SL and BL and $\alpha^\star \approx 0.3$ for the NL model. However, there is significant variation and even $\hat{\alpha}=1$ is selected in some instances. The profile of $\hat{\alpha}$ indicates the degree to which the difficulty of the classification task varies across geometries. For example, consider the BL model with $\vartheta_1/\vartheta_0=3$ (top left). Here, the large variation in $\hat{\alpha}$ can be attributed to the minimal separation between the classes (see @fig-sim-classification-ternary, top left). Adjusting the geometry does little to improve this predicament, so the error rate will be quite insensitive to the choice of $\alpha$. Noisy estimates of $\alpha^\star$ are the result. Another example can be seen in the SL data with $\vartheta_1/\vartheta_0=3$. In this case, classification is relatively easy (@fig-sim-classification-ternary, bottom right). However, the classes are so well separated that Euclidean nearest-neighbours are the same as Aitchison nearest-neighbours, so the performance of $k$-NN($\alpha$) does not change dramatically between $\alpha=0$ and $\alpha=1$. For SVM($\alpha$) and RF($\alpha$), adjusting the geometry affects the way they partition the feature space, so classification performance is sensitive to this choice and $hat{\alpha}$ concentrates near $\alpha^\star$.

```{r make-fig-sim-classification-cv-alpha}
#| label: fig-sim-classification-cv-alpha
#| fig-cap: "Blah."
#| fig-scap: "Blah."
#| fig-height: 6

readRDS(file = file.path("scripts", "compositional", "results", "classification-sim-results-knn-svm-rf.RDS")) %>%
  mutate(nice_model = case_when(nice_model == "Symmetric" ~ "SL",
                                nice_model == "Bi" ~ "BL",
                                nice_model == "Negative" ~ "NL",)) %>%
  mutate(classifier = case_when(classifier == "knn" ~ 'k * "-NN" * (alpha)',
                                classifier == "svm" ~ '"SVM" * (alpha)',
                                classifier == "rf" ~ '"RF" * (alpha)')) %>%
  mutate(param0 = paste0("vartheta[0] == ", param0),
         param1_frac = paste0("vartheta[1]/vartheta[0] == ", param1_frac)) %>%
  group_by(nice_model, param0, param1_frac, classifier, rep) %>%
  slice_min(train_error, n = 1) %>%
  ggplot(aes(x = alpha, fill = classifier)) +
  geom_density(alpha = 0.7) +
  facet_nested_wrap(param1_frac ~ nice_model, scales = "free", labeller = label_parsed, nrow = 2,
                    nest_line = element_line(colour = "white")) +
  scale_x_continuous(limits = c(0, 1), expand = c(0,0), breaks = breaks_extended(n = 6)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05)), breaks = breaks_extended(n = 4)) +
  scale_fill_manual(values = c("red", "blue", "darkgreen"), labels = parse_format()) +
  xlab(expression(alpha)) +
  ylab("Density") +
  labs(fill = "Classifier") +
  theme_light() +
  theme(legend.position = "top", 
        panel.grid.minor = element_blank(),
        panel.grid.major = element_blank()) 
```

### Discussion and outlook

*Discuss conclusions of CoDA PCA classification stuff here.*


TO DO:
- Write captions and discussion section for Compositional.
- Write future work section for ChangingExtDep. 
- Check I have addressed all feedback points for Compositional and ChangingExtDep.
- Quick proofread through Chapters 2-4 and address anything that comes up. 


