# Literature review

```{r background-load-packages}
#| include: false
library(tidyverse)
library(tidyr)
library(magrittr)
library(scales)
library(ggh4x)
library(ggpubr)
library(colorspace)
library(kableExtra)
library(reshape2)
library(mev)
library(GGally)
library(ggforce)

options(dplyr.summarise.inform = FALSE)
options(knitr.kable.NA = "")
```

```{r background-source-functions}
#| include: false
sapply(list.files(path = "R/general", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
sapply(list.files(path = "R/background", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

## Univariate extreme value theory

### Block maxima and the generalised extreme value (GEV) distribution

Let $X_1,X_2,\ldots$ be a sequence of independent, identically distributed, continuous random variables with distribution function $F$. For $n\geq 1$, define the random variable
\begin{equation}
    M_n := \max (X_1,\ldots, X_n) = \bigvee_{i=1}^n X_i.
\end{equation}
The exact distribution of $M_n$ is given by
\begin{equation*}
    \mathbb{P}(M_n \leq x) = \mathbb{P}(X_1\leq x,\ldots X_n \leq x) = \prod_{i=1}^n \mathbb{P}(X_i \leq x) = F^n(x), \qquad (x\in\R).
\end{equation*}
This result is not particularly useful in practice, where $F$ is typically unknown. Instead, we study the limiting behaviour of $F^n$ as $n\to\infty$. Clearly the asymptotic distribution of $M_n$ is degenerate, since $M_n \overset{p}{\to} x_F:=\sup\{x:F(x)<1\}$, the (possibly infinite) upper end-point of $F$. However, the Extremal Types Theorem states that, after suitable rescaling, there are three classes of non-degenerate asymptotic distribution (CITE).

:::{#thm-extremal-types}
Suppose there exist real sequences $\{a_n > 0\}$ and $\{b_n\in\R\}$ and a non-degenerate distribution function $G$ such that
\begin{equation}\label{eq-extremal-types}
    \mathbb{P}\left(\frac{M_n - b_n}{a_n} \leq x \right) \overset{d}{\to} G(x), \qquad (n\to\infty).
\end{equation}
Then $G$ belongs to one of three parametric families: Gumbel, Fréchet or negative Weibull.
:::

When \eqref{eq-extremal-types} holds, we say that $F$ lies in the maximum domain of attraction (MDA) of $G$. The three families are unified by the Generalised Extreme Value (GEV) distribution. Its distribution function is
\begin{equation}\label{eq-gev}
    G(x) = \exp\left\lbrace - \left[ 1 + \xi \left(\frac{x-\mu}{\sigma}\right)\right]_+^{-1/\xi} \right\rbrace,
\end{equation}
where $[x]_+:=\max(0,x)$ denote the positive part of $x$. The parameters $\mu\in\R$, $\sigma >0$ and $\xi\in\R$ are called the location, scale, and shape, respectively. The sign of the shape parameter determines the sub-class that $G$ belongs to: $\xi>0$ corresponds to the heavy-tailed Fréchet class, $\xi=0$ (with \eqref{eq-extremal-types} interpreted as $\xi\to 0$) corresponds the exponential-tailed Gumbel class, and $\xi<0$ the negative Weibull class, which has a finite upper limit.

The GEV distribution is used to model the upper tail of $X$ via the block maxima approach (CITE). Let $x_1,\ldots,x_n$ denote independent observations of $X_1,\ldots,X_n$. The data are partitioned into finite blocks of size $m$. Provided $m$ is sufficiently large, the maximum observation in each block is approximately GEV distributed by @thm-extremal-types. Once the block-wise maxima have been extracted, estimates of the GEV parameters may be obtained, e.g. by maximum likelihood inference. The performance of the fitted model is sensitive to the choice of block size. Selection of the tuning parameter $m$ requires managing a bias-variance trade-off. If the blocks are too small, then the underlying asymptotic approximation may not be valid and the maxima may not be representative as extreme events, biasing the estimates. Taking larger blocks reduces the amount of data available for inference, resulting in noisier estimation of the GEV parameter estimates.

### Threshold exceedances and the generalised Pareto distribution (GPD)

The block maxima procedure is considered inefficient, because it fails to exploit all the available information. Each block is summarised by a (single) maximum value, even if it contains other 'extreme' events that might be informative for the tail. The intimately related peaks-over-threshold method makes better use of the available data. If $X$ is in the maximum domain of attraction of a $\mathrm{GEV}(\mu,\sigma,\xi)$ distribution, then
\begin{equation}\label{eq-gpd}
    \lim_{u\to\infty} \mathbb{P}(X-u>x \mid X>u) = \left[1+\frac{\xi x}{\tilde{\sigma}}\right]_+^{-1/\xi}, \qquad (x>0),
\end{equation}
where $\tilde{\sigma}=\sigma + \xi(u-\mu)$ (CITE). The limiting conditional distribution is called the generalised Pareto distribution (GPD). The GPD describes the distribution of excesses over a high threshold. Given observations $x_1,\ldots,x_n$, the peaks-over-threshold method assumes that exceedances of some pre-specified high threshold $u>0$ are approximately GPD distributed. Maximum likelihood or Bayesian inference procedures may be used to estimate the GPD parameters $\bar{\sigma},\xi$. Threshold selection is subject to similar considerations as for the block size. Picking a low threshold risks model misspecification, causing bias in the fitted model. Choosing a high threshold directly reduces the number of threshold exceedances, increasing the uncertainty in the parameter estimates. Various diagnostics and procedures have been proposed to aid with this choice. Many approaches rely on inspecting diagnostic plots, such as mean residual life (MRL) plots (CITE) and parameter stability plots (CITE). Automated selection procedures aim to remove subjectivity by optimising with respect to some criterion. These include change-point methods (CITE Wadsworth 2016), cross-validation in a Bayesian framework (CITE Northrop et al. 2017), and minimising expected quantile discrepancies (CITE Murphy and Tawn 2024). 

### Non-stationary extremes

The block-maxima and peaks-over-threshold methods as presented above assume that the data are stationary over the observation period. In environmental applications, climate change threatens the validity of this assumption, with changes in the frequency and intensity of extreme weather events (CITE). Non-stationary models accommodate temporal dependence by allowing parameters to vary over time or in relation to covariates. For example, CITE Vanem 2015 incorporate trends into the GEV location and scale parameters by specifying
\begin{equation*}
\mu(t)=\mu_0 + \mu_1 t,\qquad \sigma(t) = \exp(\sigma_0 + \sigma_1 t).
\end{equation*}
If the parameters $\mu_1$ and $\sigma_1$ are significantly different from zero, it suggests the data exhibit non-stationarity. In principle the shape parameter may be extended analogously. Often the shape parameter is assumed constant because is notoriously difficult to estimate accurately and results (quantiles, return periods, etc.) are very sensitive to changes in its sign. *CITE further papers or a review?* 

## Multivariate extreme value theory

Multivariate extreme value theory (MEVT) generalises the study of extreme events from univariate to multivariate settings. Understanding the joint tail behaviour of several variables is critical in various fields. In environmental science, practitioners are tasked with assessing the risk of compound extreme events involving several variables. For example, the impact of drought -- defined by the IPCC (CITE) as a prolonged period of low precipitation -- is exacerbated by high temperatures. Similarly, extreme rainfall occurring simultaneously across multiple locations may lead to a widespread flood event. In finance, investors seek to diversify their portfolio to mitigate against the risk of simultaneous extreme losses across multiple assets. Each of these examples calls for a statistical analysis of the joint tail distribution of some random vector.

### Componentwise maxima

Consider a $d$-dimensional random vector $\bm{X}=(X_1,\ldots,X_d)$ with unknown joint distribution function $F$, meaning
\begin{equation*}
    F(\bm{x}) := \mathbb{P}(X_1\leq x_1, \ldots, X_d \leq x_d),
\end{equation*}
for any $\bm{x}=(x_1,\ldots,x_d)\in\R^d$. Let $\bm{X}_1,\bm{X}_2,\ldots$ be a sequence of independent copies of $\bm{X}$. The notion of `extremes' or a 'maximum' becomes subjective in the multivariate setting, because $\R^d$ is not an ordered set. One possibility is to define the maximum component-wise as
\begin{equation*}
    \bm{M}_n := \left( \bigvee_{i=1}^n X_{i1}, \ldots, \bigvee_{i=1}^n X_{id} \right). 
\end{equation*}
We say that $F$ lies in the multivariate MDA of a non-degenerate distribution $G$ if there exist $\R^d$-valued sequences $\{\bm{a}_n > \bm{0}\}$ and $\{\bm{b}_n \in \R^d\}$ such that
\begin{equation}\label{eq-multivariate-mda}
    \mathbb{P}\left(\frac{\bm{M}_n - \bm{b}_n}{\bm{a}_n} \leq \bm{x} \right) \overset{d}{\to} G(\bm{x}), \qquad (n\to\infty).
\end{equation}
Applying @thm-extremal-types to the marginal components reveals that the margins of $G$ follow a univariate GEV distribution. The crucial difference to the univariate setting is that now the limit (joint) distribution $G$ does *not* admit a parametric representation. The inherently challenging nature of MEVT largely stem from this fact. The problem of estimating/modelling $G$ is usually split into two (sequential) steps. First, one models the margins to describe the extreme behaviour of each variable individually (using univariate EVT). Then, one standardises to common margins and models the extremal dependence structure, i.e. the inter-relationships between extremes across multiple variables. Copula theory provides a rigorous justification for this two-step process.

### Copulae and marginal standardisation

In multivariate statistics, Sklar's theorem allows for the separation of the marginal distributions of variables from their joint dependence structure through the use of a copula. It states that any multivariate distribution can be expressed as a combination of individual marginal distributions and a copula that captures the dependence between them.

:::{#thm-sklar}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ has joint distribution function $F$ and continuous marginal distributions $X_i\sim F_i$ for $i=1,\ldots,d$. Then there exists a unique copula $C$ such that
\begin{equation}
    F(x_1,\ldots,x_d) = C\left(F_1(x_1),\ldots,F_d(x_d)\right).
\end{equation}
:::

The copula $C$ characterises the dependence structure of the variables, and represents the distribution function of $\bm{X}$ after transforming to standard uniform margins. Uniform margins are a standard choice in multivariate statistics, but copulae may be defined with alternative marginal distributions. In extreme value theory, it is common to use Fréchet, exponential or Gumbel margins. The different choices accentuate particular features of the extreme values. For example, heavy-tailed Fréchet margins serve to highlight the most extreme values, while Gumbel or exponential margins are often favoured for conditional extremes modelling (CITE Heffernan and Tawn). Although the marginal distribution is an important modelling choice, ultimately all choices are valid/equivalent in the sense that monotonic transformations of the univariate marginals do not change the nature of tail dependence [@resnickHeavytailPhenomenaProbabilistic2007].

There are broadly two ways of performing the preliminary marginal standardisation. Suppose $\bm{X}=(X_1,\ldots,X_d)$ has marginal distributions $X_i\sim F_i$ for $i=1,\ldots,d$. If the functions $F_i$ are known, then the marginal distributions can be transformed to some common target distribution $F_\star$ via the probability integral transform:
\begin{equation}\label{eq-marginal-transformation}
    X_i \mapsto F_\star^{-1}(F_i(X_i)) \sim F_\star, \qquad (i=1,\ldots,d).
\end{equation}
If the marginal distributions are unknown, as is usually the case, then $F_i$ is replaced with some estimate $\hat{F}_i$ in \eqref{eq-marginal-transformation}. A standard choice for $\hat{F}_i$ is the empirical CDF (non-parametric), perhaps with GPD tails above a high threshold (semi-parametric). Examples of these two approaches can be found in @russellAnalyzingDependenceMatrices2018 and @rohrbeckSimulatingFloodEvent2023, respectively. Throughout this thesis, uncertainty arising from estimation of the marginal distributions shall be neglected. Relaxing this assumption, as in @clemenconConcentrationBoundsEmpirical2023, represents an avenue for future work. 

### The exponent measure and angular measure

Suppose $\bm{X}$ is on unit Fréchet margins, that is
\begin{equation}\label{eq-unit-frechet}
    \mathbb{P}(X_i<x) = \exp(-1/x), \qquad (x>0),
\end{equation}
for $i=1,\ldots,d$. This corresponds to a GEV distribution with $\mu=\sigma=\xi=1$. The joint distribution $G$ in \eqref{eq-multivariate-mda} may be rewritten in the form 
\begin{equation}\label{eq-mevd}
    G(\bm{x}) = \exp(-V(\bm{x})),
\end{equation}
where $\bm{x}=(x_1,\ldots,x_d)$ and $x_i>0$ for $i=1,\ldots,d$. The exponent measure $V$ is a function of the form
\begin{equation}\label{eq-exponent-measure}
    V(\bm{x}) = d \int_{\mathbb{S}_{+(1)}^{d-1}} \bigvee_{i=1}^d \left(\frac{\theta_i}{x_i}\right)\,\dee H(\bm{\theta}).
\end{equation}
Here
\begin{equation}\label{eq-Lp-simplex}
    \mathbb{S}_{+(p)}^{d-1} := \{\bm{x}\in\R_+^d : \|\bm{x}\|_p = 1\}
\end{equation}
denotes the $L_p$-simplex in the non-negative orthant of $\R^d$ and the angular measure $H$ is a probability measure on $\mathbb{S}_{+(1)}^{d-1}$ satisfying the moment constraints
\begin{equation}\label{eq-H-mean-constraints}
    \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \,\dee H(\bm{\theta}) = 1/d, \qquad (i=1,\ldots,d).
\end{equation}
Our notation for the simplex is borrowed from @fixSimultaneousAutoregressiveModels2021. The exponent $d-1$ highlights the fact that the simplex is a $(d-1)$-dimensional set embedded in the $d$-dimensional space $\R^d$. The $+$ and $(p)$ in the subscript convey that the set is restricted to the non-negative orthant and is with respect to the $L_p$-norm, respectively. The constraints on $H$ arise due to tail equivalence of the margins. Functions $G$ satisfying \eqref{eq-mevd} are called multivariate extreme value distributions. If $V$ is differentiable, then the density $h$ of $H$ exists in the interior and on the low-dimensional boundaries of the simplex. The relation between $V$ and $h$ is given by
\begin{equation}
    h\left(\frac{\bm{x}}{\|\bm{x}\|_1}\right) = -\frac{\|\bm{x}\|_1^{d+1}}{d}\frac{\partial^d}{\partial x_1\cdots \partial x_d} V(\bm{x}).
\end{equation}
The benefit of introducing the exponent and angular measures is that models for $G$ may be specified in terms of $V$ or $H$. The extremal dependence structure of $\bm{X}$ is completely characterised by $H$: the angular measure determines $V$ via \eqref{eq-exponent-measure} and subsequently $G$ via \eqref{eq-mevd}. Modelling the angular measure now becomes our primary focus.

### Parametric multivariate extreme value models

The class of valid dependence structures is in direct correspondence to the infinite-dimensional class of valid measures $H$. This greatly hinders efforts to perform statistical inference: efficient estimation via likelihood inference, hypothesis testing, and inclusion of covariates immediately become unavailable. We may return to the parametric paradigm by postulating a suitable parametric sub-family. Ideally the chosen sub-family generates a wide class of valid dependence structures. A detailed review of popular models can be found in @gudendorfExtremeValueCopulas2010. 

There are several drawbacks to the parametric approach. Working with a parametric model instead of the general class runs the risk of model misspecification. Generating valid models is a challenging endeavour due to the moment constraints, resulting in models that are either overly simplistic or have unwieldy distribution functions and parameter constraints. Striking a balance between flexibility and parsimony becomes especially in high dimensions (i.e. when $d$ is large). For these reasons, parametric models are not a primary focus of this thesis. Nevertheless, we now review a small selection of models. These primarily feature as data-generating processes for our numerical experiments. Functionality for generating independent observations $\bm{x}_1,\ldots,\bm{x}_n$ of $\bm{X}$ or $\bm{\theta}_1,\ldots,\bm{\theta}_n\sim H$ based on the sampling algorithms formulated in @dombryExactSimulationMaxstable2016 is provided in the R package \texttt{mev}. 

#### Logistic-type models

One of the oldest and simplest multivariate extreme value models is the symmetric logistic distribution [@gumbelBivariateExponentialDistributions1960].

:::{#def-symmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the symmetric logistic distribution is
\begin{equation}\label{eq-symmetric-logistic-V}
    V(\bm{x}) = \left(\sum_{i=1}^d x_i^{-1/\gamma}\right)^{\gamma}, \qquad \gamma \in (0,1].
\end{equation}
:::

The single dependence parameter $\gamma\in(0,1]$ characterises the strength of the association between all variables. Independence occurs when $\gamma=1$ and the variables approach complete dependence as $\gamma\to 0$. All variables are exchangeable, since the distribution function is invariant under coordinate permutation. A flexible extension is the asymmetric logistic model of @tawnModellingMultivariateExtreme1990. Greater control over the dependence structure is achieved by increasing the number of parameters.

:::{#def-asymmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the asymmetric logistic distribution is of the form 
\begin{equation}\label{eq-asymmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}\left[\sum_{i\in\beta}\left(\frac{\theta_{i,\beta}}{x_i}\right)^{1/\gamma_\beta}\right]^{\gamma_\beta}, \qquad 
    \begin{cases}
      \gamma_\beta\in(0,1], \\
      \theta_{i,\beta}\in[0,1], & \text{if }i\in\beta, \\
      \theta_{i,\beta}=0, & \text{if }i\notin\beta, \\
      \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset} \theta_{i,\beta}=1,
    \end{cases}
\end{equation}
where $\mathcal{P}(\{1,\ldots,d\})\setminus\emptyset$ denotes the set of non-empty subsets of $\{1,\ldots,d\}$.
:::

The set of parameters $\{\gamma_\beta:\beta\in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset\}$ control the dependence strength among the corresponding variables $\{X_i : i\in\beta\}$ in a similar way to the symmetric logistic model. The model's complexity arises from the set of asymmetry parameters $\bm{\theta}_\beta=(\theta_{i,\beta}:i\in\beta)$, which dictate the direction/composition of extreme events involving the variables $\{X_i : i\in\beta\}$. Further models can be generated by `inverting' the logistic and asymmetric models. **The purpose of inverting is...**. When applied to the models described above, inversion yields the negative symmetric logistic model [@galambosOrderStatisticsSamples1975] and the negative asymmetric logistic model [@joeFamiliesMinstableMultivariate1990], respectively.

:::{#def-negative-symmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the negative symmetric logistic distribution is
\begin{equation}\label{eq-negative-symmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}(-1)^{|\beta|+1}\left(\sum_{i\in\beta} x_i^{\gamma}\right)^{-1/\gamma}, \qquad \gamma>0.
\end{equation}
:::

:::{#def-negative-asymmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the negative asymmetric logistic distribution is
\begin{equation}\label{eq-negative-asymmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}(-1)^{|\beta|+1}\left(\sum_{i\in\beta} x_i^{\gamma}\right)^{-1/\gamma}, \qquad \gamma>0.
\end{equation}
:::

Other logistic-type models include the bilogistic @smithStatisticsMultivariateExtremes1990] and negative bilogistic [@colesStatisticalMethodsMultivariate1994].

#### The Brown-Resnick process and Hüsler-Reiss distribution

The Brown-Resnick process of @brownExtremeValuesIndependent1977 is a class of stochastic processes commonly used to model the extremal dependence structure of spatial phenomena, including rainfall [@davisonStatisticalModelingSpatial2012], snow depths [@schellanderModelingSnowDepth2018] and wind gusts [@oestingStatisticalPostprocessingForecasts2017]. It is naturally defined through a transformation of a Gaussian process -- a formal construction can be found in CITE Kabluchko et al. (2009). Let $\Omega\in\R^2$ be a spatial domain. Consider a Brown-Resnick process $\{X(\bm{s}):\bm{s}\in\Omega\}$ with semi-variogram
\begin{equation}\label{eq-fractal-variogram}
    \gamma(\bm{s}, \bm{s}') = (\|\bm{s}-\bm{s}'\|_2/\rho)^\kappa, \qquad \rho>0, \kappa\in(0,2].
\end{equation} 
Semi-variograms of this form are called fractal semi-variograms and the associated process $\{X(\bm{s})\bm{s}\in\Omega\}$ is stationary and isotropic [@engelkeEstimationHuslerReissDistributions2015]. Stationarity and isotropy mean that the statistical properties of the spatial process are invariant under translation and rotation. Specifically, the dependence between two sites only depends on the distance between them, not the direction or their position within the spatial domain. The parameters $\rho$ and $\kappa$ in \eqref{eq-fractal-variogram} control the range and smoothness, respectively. The range parameter determines how quickly the dependence strength decreases over distance. The smoothness parameter governs the regularity of the process and affects its local behaviour.

Let $\bm{s}_i,\bm{s}_j\in\Omega$ be a pair of spatial locations and define random variables $X_i=X(\bm{s}_i)$ and $X_j=X(\bm{s}_j)$. The exponent measure of the bivariate random vectors $(X_i,X_j)$ is [@huserCompositeLikelihoodEstimation2013]
\begin{equation}\label{eq-brown-resnick-V}
    V(x_i,x_j) = \frac{1}{x_i} \Phi\left(\frac{a_{ij}}{2} + \frac{1}{a_{ij}}\log\frac{x_j}{x_i}\right) + \frac{1}{x_j} \Phi\left(\frac{a_{ij}}{2} + \frac{1}{a_{ij}}\log\frac{x_i}{x_j}\right),
\end{equation}
where $a_{ij} = \sqrt{\gamma(\bm{s}_i,\bm{s}_j)}$. The stationary/isotropic nature of the underlying process is apparent because $V$ depends on $\bm{s}_i$ and $\bm{s}_j$ only through $\|\bm{s}_i-\bm{s}_j\|_2$. 

*Other things I could mention: Davison et al. (2012) apply BR to rainfall data, finding $1/2<\kappa<1$. Although the Brown–Resnick processes are max-stable, the processes observed at a finite number of locations are also multivariate regularly varying.*

The Brown-Resnick process is intimately related to the Hüsler-Reiss distribution of @huslerMaximaNormalRandom1989. The Hüsler-Reiss distribution is of fundamental importance in multivariate extremes: it has been labelled the Gaussian distribution for extremes [@engelkeGraphicalModelsExtremes2019]. In $d\geq 2$ dimensions the distribution is parametrised by a matrix $\Lambda=(\lambda_{ij}^2)_{1\leq i,j\leq d}$ belonging to the class of symmetric, strictly conditionally negative definite matrices
\begin{equation*}
    \mathcal{D} := \left\lbrace M \in \R_+^{d\times d} \, : \, M=M^T,\, \mathrm{diag}(M)=\bm{0},\, \bm{x}^TM\bm{x} < 0 \,\forall \bm{x}\in\R^d\setminus\{\bm{0}\} \text{ such that } \sum_{j=1}^d x_j=0\right\rbrace.
\end{equation*}
The class of Hüsler-Reiss distributions is closed in the sense that if $\bm{X}=(X_1,\ldots,X_d)$ follows a Hüsler-Reiss distribution with parameter matrix $\Lambda$, then any random sub-vector $(X_i,X_j)$ is also Hüsler-Reiss distributed with parameter $\lambda_{ij}^2$. This permits very flexible control over the pairwise dependence structure. The dependence between any pair of variables $X_i$ and $X_j$ can be adjusted by modifying the corresponding parameter $\lambda_{ij}$, subject to the constraint $\Lambda\in\mathcal{D}$. The finite-dimensional distribution of a Brown-Resnick process at locations $\bm{s}_1,\ldots,\bm{s}_d$ is precisely the Hüsler-Reiss distribution with $\Lambda = (\gamma(\bm{s}_i,\bm{s}_j)/4)_{1\leq i,j \leq d}$ [@engelkeEstimationHuslerReissDistributions2015]. Due to this link, the Hüsler-Reiss distribution may be parametrised in terms of its variogram matrix $\Gamma:=4\Lambda\in\mathcal{D}$ [@engelkeSparseStructuresMultivariate2021; @fomichovSphericalClusteringDetection2023] and the exponent measure of $(X_i,X_j)$ is given by \eqref{eq-brown-resnick-V} with $a_{ij}$ replaced by $2\lambda_{ij}$.

#### The max-linear model

The final parametric model we consider is the max-linear (factor) model [@einmahlMestimatorTailDependence2012; @fougeresDenseClassesMultivariate2013; @yuenCRPSMestimationMaxstable2014]. *Its exact origin is unclear, but it seems to stem from around these papers.* Max-linear models are a simple but flexible class possessing important theoretical properties. Any discrete angular measure concentrating on finitely many points corresponds to a max-linear model [@yuenCRPSMestimationMaxstable2014]. Due to its flexibility and theoretical properties, the max-linear model has enjoyed widespread use across several areas of extremes, including clustering [@janssenKmeansClusteringExtremes2020; @medinaSpectralLearningMultivariate2021], graphical modelling for causal inference [@gissiblIdentifiabilityEstimationRecursive2019; @gissiblMaxlinearModelsDirected2018; @tranCausalDiscoveryRiver2021] and tail event probability estimation [@kirilioukEstimatingProbabilitiesMultivariate2022]. In future sections/chapters, the max-linear model will be applied in more general settings where the marginal distributions are Fréchet with shape parameter $\alpha\geq 1$ and the angular measure is defined with respect to the $L_\alpha$-norm on $\R^d$. In anticipation of this, the max-linear model is introduced in this more general setting. To revert to the setting established in the previous sections, the reader may simply take $\alpha=1$.

:::{#def-max-linear}
Let $A=(\bm{a}_1,\ldots,\bm{a}_q)\in\R_+^{d\times q}$ for some $q\geq 1$. Assume that $\bm{a}_j\neq\bm{0}$ for all $j=1,\ldots,q$ and each row has unit $L_\alpha$-norm, i.e. $\sum_{j=1}^q a_{ij}^\alpha =1$ for $i=1,\ldots,d$. A random vector $\bm{X}=(X_1,\ldots,X_d)$ with discrete probability angular measure
\begin{equation}\label{eq-max-linear-H}
    H(\cdot) = \frac{1}{\sum_{j=1}^q \|\bm{a}_j\|_{\alpha}^{\alpha}}\sum_{j=1}^q \|\bm{a}_j\|_{\alpha}^{\alpha} \delta_{\bm{a}_j/\|\bm{a}_j\|_{\alpha}}(\cdot)
\end{equation}
is said to follow the max-linear model with parameter matrix $A$.
:::

The row-wise unit-norm constraint on $A$ results ensures the marginal components are Fréchet distributed with unit scale and shape $\alpha$. Setting $\alpha=1$, we see that \eqref{eq-max-linear-H} is a valid angular measure: for any $i=1,\ldots,d$,
\begin{equation*}
    \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \,\dee H(\bm{\theta}) 
    = \frac{1}{\sum_{j=1}^q \|\bm{a}_j\|_1} \sum_{j=1}^q \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \|\bm{a}_j\|_1 \delta_{\bm{a}_j/\|\bm{a}_j\|_1}(\bm{\theta})\,\dee \bm{\theta}
    = \frac{\sum_{j=1}^q a_{ij}}{\sum_{i=1}^d \sum_{j=1}^q  a_{ij}} 
    = \frac{1}{d}.
\end{equation*}
The number of free parameters is $d\times(q-1)$ and the order of the columns of $A$ is inconsequential. The factors $\bm{a}_1,\ldots,\bm{a}_q$ correspond to the possible directions that extremal observations may take. The column  norms $\|\bm{a}_1\|_{\alpha},\ldots,\|\bm{a}_q\|_{\alpha}$ determine the respective weights assigned to these directions. There is a direct correspondence between the class of discrete angular measure placing mass on $q<\infty$ points and the class of max-linear random vectors with $q$ factors [@yuenCRPSMestimationMaxstable2014]. Moreover, the class of angular measures \eqref{eq-max-linear-H} is dense in the class of valid angular measures [@fougeresDenseClassesMultivariate2013]. In other words, any extremal dependence structure can be arbitrarily well-approximated by that of a max-linear model with sufficiently many factors. This makes max-linear modelling a versatile and powerful framework, despite its simplicity.

There are several ways to construct a random vector $\bm{X}=(X_1,\ldots,X_d)$ with angular measure \eqref{eq-max-linear-H}. This thesis uses two constructions. Let $Z_1,\ldots,Z_q$ be independent Fréchet random variables with unit scale and shape parameter $\alpha$, and set $\bm{Z}=(Z_1,\ldots,Z_q)$. The two constructions are
\begin{equation}\label{eq-max-linear-X}
    \bm{X} = A \times_{\max} \bm{Z} := \left( \bigvee_{j=1}^q a_{1j}Z_j, \ldots, \bigvee_{j=1}^q a_{dj}Z_j \right)
\end{equation}
and
\begin{equation}\label{eq-max-linear-X-cooley}
    \bm{X} = A \otimes \bm{Z} := \bigoplus_{j=1}^q (\bm{a}_j \odot Z_j).
\end{equation}
Adopting the terminology of @cooleyDecompositionsDependenceHighdimensional2019, we refer to these as the max-stable and transformed-linear constructions, respectively. Under the max-stable construction, each component $X_i$ is the maximum of linear combinations of the heavy-tailed latent variables $Z_1,\ldots,Z_q$. The second construction, employed in @cooleyDecompositionsDependenceHighdimensional2019, is defined in terms of vector space operations $\oplus$ and $\odot$ defined therein. These operations will be defined explicitly and discussed later in Section XX. The difference between the two constructions manifests in their realisations, as illustrated in Figure 7 in the Supplementary Material of @cooleyDecompositionsDependenceHighdimensional2019. The directions of large realisations of the max-stable construction tend to correspond almost exactly to the points $\bm{a}_1/\|\bm{a}_1\|_{\alpha},\ldots,\bm{a}_q/\|\bm{a}_q\|_{{\alpha}}$. Under the transformed-linear construction, the directions of extreme events tend to lie in a neighbourhood of, but not exactly on, these discrete locations. 

Computing joint tail event probabilities is straightforward under the max-linear model. Suppose $\bm{X}$ is max-linear with parameter matrix $A$. Consider the extreme failure region
\begin{equation*}
\mathcal{R}_f(x) := \{\bm{y}\in\R_+^d : f(\bm{y})>x\}
\end{equation*}
for some function $f:\R_+^d\to\R$. Provided the failure region is sufficiently extreme (distant from the origin), then
\begin{equation}\label{eq-max-linear-failure-probability}
\mathbb{P}(\bm{X}\in\mathcal{R}_f(x)) \approx \sum_{j=1}^q \frac{\|\bm{a}_j\|_{\alpha}^{\alpha}}{r_\star(\bm{a}_j/\|\bm{a}_j\|_{\alpha})^\alpha},
\end{equation}
where $r_\star=r_\star(\bm{\theta})$ is such that $f(r_\star \bm{\theta})=x$ [@cooleyDecompositionsDependenceHighdimensional2019; @kirilioukEstimatingProbabilitiesMultivariate2022]. The formulae corresponding to some popular failure regions are listed below:
\begin{align*}
f(\bm{y}) = \max\bm{y}, \qquad & \mathbb{P}(\max\bm{X}>x) \approx \sum_{j=1}^q \max_{i=1,\ldots,d}\frac{a_{ij}}{x} \\
f(\bm{y}) = \min\bm{y}, \qquad & \mathbb{P}(\min\bm{X}>x) \approx \sum_{j=1}^q \min_{i=1,\ldots,d}\frac{a_{ij}}{x} \\
f(\bm{y}) = \bm{v}^T\bm{y}, \qquad & \mathbb{P}(\bm{v}^T\bm{X}>x) \approx \sum_{j=1}^q \frac{\bm{v}^T\bm{a}_j}{x}.
\end{align*}
The first and second regions concern extreme events affecting at least one variable or all variables simultaneously, respectively. For the third region, the weight vector $\bm{v}$ satisfies $v_i\geq 0$ and $v_1+\ldots+v_d=1$. Such regions are of interest for climate event attribution [@kirilioukClimateExtremeEvent2020] or quantifying the Value-at-Risk of an asset portfolio [@yuenUpperBoundsValuerisk2014]. Each of these failure probabilities may be perceived as a measure of risk. Risk mitigation is the practice of taking action -- bolstering flood defences or diversifying a portfolio -- to ensure these probabilities are acceptably small.  

### Multivariate regular variation

Multivariate regular variation (MRV) provides an alternative framework for characterising the probabilistic structure of the joint tail of random vectors. By imposing a regularity structure on the joint tail, MRV facilitates the development of theoretically justified procedures for extrapolating the probability law from moderately large values to more extreme tail regions. We introduce the concept of regular variation in the univariate setting before extending to the multivariate case.

:::{#def-regular-variation-function}
A function $f:\R_+\to\R_+$ is regularly varying with index $\alpha\in\R$ if, for all $x>0$,
    \begin{equation}
        \lim_{t\to\infty}\frac{f(tx)}{f(t)} = x^\alpha.
    \end{equation}
:::

If $\alpha=0$, then $f$ is called slowly-varying. Intuitively, a regularly varying function is one that behaves like a power function as the argument approaches infinity. This notion is generalised to random variables by taking the distributional tail as the function of interest. 

:::{#def-regular-variation-random-variable}
A non-negative random variable $X$ is regularly varying with tail index $\alpha \geq 0$ if the right-tail of its distribution function is regularly varying with index $-\alpha$, i.e. for all $x>1$,
    \begin{equation*}
        \lim_{t\to\infty} \mathbb{P}(X > tx \mid X > t) = x^{-\alpha}.
    \end{equation*}
:::

If $X$ is regularly varying with index $\alpha$, then its survivor function is of the form
\begin{equation}\label{eq-regularly-varying-survivor-function}
    \mathbb{P}(X>x) = x^{-\alpha} L(x)
\end{equation}
for some slowly-varying function $L$ [@jessenRegularlyVaryingFunctions2006]. Regularly varying random variables are those with power law tails. In fact, a random variable $X$ is regularly varying if and only if it belongs to the Fréchet MDA (CITE). Crucially, \eqref{eq-regularly-varying-survivor-function} reveals that regularly varying distributions possess asymptotic scale invariance, in the sense that for all $\lambda>0$,
\begin{equation*}
    \mathbb{P}(X > \lambda x) = (\lambda x)^{-\alpha} L(\lambda x) \sim \lambda^{-\alpha} \mathbb{P}(X > x).
\end{equation*}
The ubiquity of regular variation in extreme value statistics is due to this homogeneity property. Under regular variation, the probability law of $X$ at some level $\lambda x$ is identical to the probability law at level $\lambda$, up to some constant factor. An analogous interpretation holds when regular variation is generalised to multivariate random vectors, where the joint tail distribution is represented by a homogeneous limit measure.

Although MRV can be formulated more generally -- see Section 6.5.5 in @resnickHeavytailPhenomenaProbabilistic2007 -- we exclusively focus on random vectors $\bm{X}$ taking values on the positive orthant $\R_+^d:=[0,\infty)^d$. This common assumption is not as restrictive as it might initially seem. In most applications, the risk being assessed is directional. For example, a climatologist might model the lows or the highs of precipitation records depending on they are analysing drought risk or flood risk. Without loss of generality and by means of a transformation if necessary, this direction of interest can be defined as ‘positive’.

:::{#def-mrv}
A random vector $\bm{X}=(X_1,\ldots,X_d)$ is multivariate regularly varying with tail index $\alpha>0$, denoted $\bm{X}\in\mathcal{RV}_+^d(\alpha)$, if it satisfies the following (equivalent) statements [@resnickHeavytailPhenomenaProbabilistic2007]: 

1. There exists a sequence $b_n\to\infty$ and a non-negative Radon measure $\nu$ on $\mathbb{E}_0:=[0,\infty]^d\setminus\{\bm{0}\}$ such that
\begin{equation}\label{eq-mrv}
n\mathbb{P}(b_n^{-1}\bm{X} \in \cdot) \stackrel{\mathrm{v}}{\rightarrow} \nu(\cdot),\qquad (n\to\infty),
\end{equation}
where $\stackrel{\mathrm{v}}{\rightarrow}$ denotes vague convergence in the space of non-negative Radon measures on $\mathbb{E}_0$. The exponent measure $\nu$ is homogeneous of order $-\alpha$, that is, for any $s>0$,
\begin{equation}
    \nu(s\,\cdot)=s^{-\alpha}\nu(\cdot).
\end{equation}
2. Let $\|\cdot\|$ be an arbitrary norm on $\R^d$. Denote the radial and angular components of $\bm{X}$ by $R:=\|\bm{X}\|$ and $\bm{\Theta}:=\bm{X}/\|\bm{X}\|$. Then there exists a sequence $b_n\to\infty$ and a finite measure $H$ on the simplex
    \begin{equation}\label{eq-general-simplex}
        \mathbb{S}_{+}^{d-1}:=\{\bm{x}\in\R_+^d:\|\bm{x}\|=1\}
    \end{equation}
    such that
\begin{equation}\label{eq-polar-mrv}
n\mathbb{P}((b_n^{-1}R,\bm{\Theta}) \in \cdot) \stackrel{\mathrm{v}}{\rightarrow} \nu_{\alpha}\times H(\cdot),\qquad (n\to\infty),
\end{equation}
in the space of non-negative Radon measures on $(0,\infty]\times\mathbb{S}_{+}^{d-1}$, where $\nu_{\alpha}((x,\infty))=x^{-\alpha}$ for any $x>0$.
:::

The limit measures $\nu$ and $H$ in \eqref{eq-mrv} and \eqref{eq-polar-mrv} are related via 
\begin{equation}\label{eq-nu-H-relation}
    \nu(\{\bm{x}\in\mathbb{E}_0:\|\bm{x}\| > s,\bm{x}/\|\bm{x}\|\in\cdot\}) 
    = s^{-\alpha}H(\cdot),\qquad  \nu(\dee r\times \dee \bm{\theta})
    =\alpha r^{-\alpha-1}\dee r\,\dee H(\bm{\theta}).
\end{equation}
The attractive feature of MRV is best represented by its pseudo-polar formulation \eqref{eq-polar-mrv}. This states that the extremal behaviour of $\bm{X}$ is fully characterised by two quantities: the tail index and the angular measure. The tail index $\alpha$ represents the index of regular variation of the (univariate) radial component. It governs the heavy-tailedness of the size (norm) of $\bm{X}$. The angular measure $H$ fully characterises the dependence structure. Crucially, the right-hand side of \eqref{eq-polar-mrv} is a product measure, signifying that the radial and angular components are independent in the limit. 

The MRV property implicitly requires that the marginal components $X_1,\ldots,X_d$ are heavy-tailed with a shared tail index. Standard practice is to standardise the margins prior to modelling the dependence structure (Section XX), so this is not restrictive. In this thesis, we will always choose Fréchet margins with unit scale and shape parameter $\alpha>0$, that is
\begin{equation}\label{eq-alpha-frechet}
    \mathbb{P}(X_i<x) = \exp(-x^{-\alpha}), \qquad (x>0).
\end{equation}
An MRV random vector on $\alpha$-Fréchet margins \eqref{eq-alpha-frechet} has tail index $\alpha$. Thus, as before, fixing the margins deals with the tail index and the angular measure becomes the object of interest.

The angular measure is unique only with respect to a pre-specified norm $\|\cdot\|$ and lies on the corresponding unit simplex \eqref{eq-general-simplex}. As mentioned previously, we exclusively choose the $L_p$-norm 
\begin{equation}
    \|\cdot\|_p : \R^d \to \R, \qquad \|\bm{x}\|_p = \left(\sum_{i=1}^d x_i^{p}\right)^{1/p}
\end{equation}
with \eqref{eq-Lp-simplex} the corresponding simplex. The mass of the angular measure is $m:=H(\mathbb{S}_{+}^{d-1})\in(0,\infty)$. The sequence $\{b_n\}$ and the quantity $m$ are jointly determined by \eqref{eq-polar-mrv}. Replacing $\{b_n\}$ by $\{sb_n\}$ for some $s>0$ yields a new angular measure $H'=s^{-\alpha}H$ whose mass is $m'=s^{-\alpha}m$. We are free to choose whether the scaling information is contained in $\{b_n\}$ or $m$. Possible reasons for preferring one over the other are discussed in @fougeresDenseClassesMultivariate2013, but ultimately it is an arbitrary modelling choice. In previous sections, $H$ was normalised to be a probability measure with $m=1$. Henceforth, we will tend to specify $\{b_n\}$ and push the scaling information on to $H$. With $\bm{X}$ standardised to $\alpha$-Fréchet margins, the centre of mass of $H$ must lie in the simplex interior:
\begin{equation}\label{eq-H-mean-constraints-general}
    \int_{\mathbb{S}_{+}^{d-1}} \theta_i \,\dee H(\bm{\theta}) = \mu > 0, \qquad (i=1,\ldots,d).
\end{equation}
Were this not the case it would imply that at least one variable can never be extreme, contradicting the assumption that all variables have equally heavy tails. The value of $\mu$ depends on the choice of norm and the mass of $H$. If $\|\cdot\|=\|\cdot\|_1$, then $\mu=m/d$ in accordance with \eqref{eq-H-mean-constraints}. If $\|\cdot\|=\|\cdot\|_2$, then $m/d\leq \mu \leq m/\sqrt{d}$ according to Lemma 2.1 in @fomichovSphericalClusteringDetection2023. The lower and upper bounds are attained when $H$ places all its mass at the vertices of the simplex or at its centre, respectively. These can be understood as the limiting cases of extremal dependence, which is formalised in the next section.

### Extremal dependence measures


The extremal dependence structure of a random vector $\bm{X}$ can be quantified and classified using a plethora of summary measures [@colesDependenceMeasuresExtreme1999]. We focus on the tail dependence coefficient and the extremal dependence measure. 

#### The tail dependence coefficient

Extremal dependence is analogous to, but separate from, the notion of statistical dependence in non-extreme statistics. In particular, two random processes might appear independent in the bulk of the distribution but exhibit dependence in their extremes, or vice versa. The extremal dependence structure may be very complex; angular measures form an infinite-dimensional class subject only to a set of moment constraints. For example, suppose $X_i$ and $X_j$ represent the recorded values of a meteorological variable measured at two spatial locations. The extremal dependence between $X_i$ and $X_j$ may depend on the spatial proximity of the sites, the topography of the spatial domain, the physics of the climatological process, and a multitude of other factors. The complexity grows as more variables are introduced, as higher-order dependencies come into play. Extremal dependence measures aim to provide summary information about particular aspects of the dependence structure. One such measure is the tail dependence coefficient (CITE).

:::{#def-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ with $X_i\sim F_i$ for $i=1,\ldots,d$. Let $\beta\subseteq\{1,\ldots,d\}$ with $|\beta|\geq 2$ and define $\bm{X}_\beta := \{X_i : i\in\beta\}$. The tail dependence coefficient associated with $\beta$ is (CITE e.g. Simpson et al 2020)
\begin{equation}
    \chi_\beta = \lim_{u\to 1} \chi_\beta (u) = \lim_{u\to 1} \frac{\mathbb{P}(F_i(X_i) > u : i \in \beta)}{1-u}.
\end{equation}
When $\beta=\{i,j\}$ for $i\neq j$, we write $\chi_\beta=:\chi_{ij}$.
:::

We say that $X_i$ and $X_j$ are asymptotically independent (AI) if and only if $\chi_{ij}=0$. Asymptotic independence means that both variables cannot take extreme values simultaneously. If $\chi_{ij}\in(0,1]$, then the variables are asymptotically dependent (AD) and may be simultaneously extreme. The interpretation of $\chi_\beta$ for $|\beta|>2$ is more subtle. If $\chi_\beta\in(0,1]$, then all components of $\bm{X}_\beta$ may be simultaneously large. If $\chi_\beta=0$, then the corresponding variables may not be concomitantly extreme, but this does not preclude the possibility that $\chi_{\beta'}>0$ for some $\beta'\subset\beta$ with $|\beta'|\geq 2$.

The nullity of otherwise of the tail dependence coefficients is determined by which subspaces of the simplex are charged with $H$-mass. Specifically, $\chi_\beta >0$ if and only if there exists $\beta'\supseteq\beta$ such that
\begin{equation}\label{eq-asy-dep-chi-H}
    H(\{\bm{\theta}\in\mathbb{S}_+^{d-1} : \theta_i > 0 \iff i\in\beta'\}) >0.
\end{equation}
For example, consider the angular measures
\begin{equation}\label{eq-limiting-discrete-H}
    H^{(1)} = \frac{m}{d}\sum_{i=1}^d \delta_{\bm{e}_i}, \qquad H^{(2)} = m\delta_{\bm{1}/\|\bm{1}\|},
\end{equation}
where $\bm{e}_1,\ldots,\bm{e}_d$ denote the canonical basis vectors of $\R^d$. The measure $H^{(1)}$ places all its mass on the vertices of the simplex. This corresponds to full asymptotic independence, since then $\chi_\beta = 0$ for all $\beta\subseteq\{1,\ldots,d\}$ with cardinality at least equal to two. The angular measure $H^{(2)}$ concentrates at a single point at the centre of the simplex. This implies that $\chi_{\{1,\ldots,d\}}>0$ and consequently $\chi_{\beta}>0$ for all subsets $\beta$.

If the bivariate exponent measure $V_{ij}$ of $(X_i,X_j)$ is known, then the tail dependence coefficient $\chi_{ij}$ may be computed using the relation $\chi_{ij}=2-V_{ij}(1,1)$ [@colesDependenceMeasuresExtreme1999]. The following examples illustrate this for selected parametric models. 

:::{#exm-symmetric-logistic-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ be symmetric logistic distributed with dependence parameter $\gamma\in(0,1]$. For any $i\neq j$, let $V_{ij}$ denote the bivariate exponent measure of $(X_i,X_j)$. Then
\begin{equation*}
    \chi_{ij} = 2- V_{ij}(1,1) = 2 - \left[ \left(x_i^{-1/\gamma} + x_j^{-1/\gamma} \right)^\gamma \right] = 2-2^\gamma.
\end{equation*}
Therefore $X_i$ and $X_j$ are asymptotically independent when $\gamma=1$ and approach complete asymptotic dependence as $\gamma\to 0$.
:::

:::{#exm-husler-reiss-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ be Hüsler-Reiss distributed with parameter matrix $\Lambda=(\lambda_{ij}^2)$. For any $i\neq j$, let $V_{ij}$ denote the bivariate exponent measure of $(X_i,X_j)$. Then
\begin{equation*}
    \chi_{ij} = 2 - V_{ij}(1,1) = 2 - 2 \Phi\left(\lambda_{ij} + \frac{1}{2\lambda_{ij}}\log 1 \right) = 2 - 2\Phi(\lambda_{ij}),
\end{equation*}
where $\Phi$ is the standard normal distribution function. Variables $X_i$ and $X_j$ are asymptotically dependent for all $\lambda_{ij}>0$, with asymptotic independence in the limit as $\lambda_{ij}\to\infty$. *Refer back to this equation when discussing Hazra and Bose changepoint method -- it gives one-to-one relationship between HR parameter and dependence strength, so testing for change in $\lambda$ or $\chi$ are equivalent.*
:::

:::{#exm-max-linear-chi}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ is max-linear with parameter matrix $A\in\R_+^{d\times q}$. Substituting \eqref{eq-max-linear-H} into \eqref{eq-exponent-measure} yields
\begin{equation}
    \chi_{ij} = 2 - V_{12}(1,1) = 2 - 2\int_{\mathbb{S}_{+(1)}^1} (\theta_1 \vee \theta_2) \,\dee H(\bm{\theta}) = 2 - \sum_{l=1}^q (a_{il} \vee a_{jl}).
\end{equation}
Consider two max-linear random vectors with discrete angular measures $H^{(1)}$ and $H^{(2)}$ as in \eqref{eq-limiting-discrete-H}. The parameter matrices are given by
\begin{equation*}
A^{(1)} =  I_d \in \R_+^{d\times d}, \qquad A^{(2)} = \bm{1}_d \in \R_+^{d\times 1}.
\end{equation*}
The tail dependence coefficients under these models are
\begin{equation*}
\chi_{ij}^{(1)} = 2 - \sum_{j=1}^2 \max(0,1) = 0, \qquad \chi_{ij}^{(2)} = 2 - \sum_{j=1}^1 \max(1,1) = 1,
\end{equation*}
corresponding to complete dependence and asymptotic dependence, as expected.
:::

Estimates of $\chi_{ij}$ are obtained by estimating $\hat{\chi}_{ij}(u)$ at a sequence of high quantiles $u$ approaching one. The \texttt{taildep} function in the R package \texttt{extRemes} achieves this using the estimator given in Equation 2.62 in @reissStatisticalAnalysisExtreme2007 and produces a diagnostic plot as shown in @fig-chi-estimation. For this example the data were generated from a symmetric logistic model with $\gamma=0.5$. The horizontal dashed line indicates the true value $\chi_{ij}=2-\sqrt{2}\approx 0.59$, while the blue points represent the estimates $\hat{\chi}_{ij}(u)$ over the range $0.8 \leq u \leq 0.995$. The shaded region depicts the 95\% Wald confidence interval. We encounter a bias-variance trade-off in relation to quantile/threshold, similar in nature to that described in Section XX with respect to the selecting the block size/threshold. 

```{r background-make-fig-chi-estimation}
#| label: fig-chi-estimation
#| fig-cap: "Empirical estimates $\\hat{\\chi}_{12}(u)$ of the tail dependence coefficient for bivariate symmetric logistic data with $\\gamma=0.5$ and $n=5,000$ observations. The true coefficient $\\chi_{12}=2-2^\\gamma\\approx 0.59$ is marked by the dashed line. The shaded region represents the 95\\% Wald confidence interval."
#| fig-scap: "Empirical estimates $\\hat{\\chi}_{12}(u)$ for bivariate symmetric logistic data."
#| fig-height: 2.8
#| fig-width: 5

set.seed(1)
gamma <- 0.5
X <- rmev(n = 5000, d = 2, par = 1 / gamma, model = "log")

taildep(X, meas = "chi", qlim = c(0.8, 0.995), plot = FALSE)$chi %>%
  as.data.frame() %>%
  mutate(q = seq(from = 0.8, to = 0.995, length.out = 40)) %>%
  ggplot(aes(x = q, y = coef)) +
  geom_ribbon(aes(ymax = upperci, ymin = lowerci), fill = "lightblue", colour = NA, alpha = 0.4) +
  geom_line(colour = "blue") +
  geom_point(colour = "blue") +
  geom_hline(yintercept = sl_chi(gamma), linetype = "dashed", colour = "black") +
  scale_x_continuous(limits = c(0.8, 1), expand = expansion(mult = c(0.01, 0)), breaks = breaks_extended(n = 8)) +
  theme_light() +
  labs(x = expression(u),
       y = expression(hat(chi)[12](u)))
```

Estimation of $\chi_\beta$ for $|\beta| > 2$ is more complicated and is related to the task of determining the support of the angular measure [@goixSparseRepresentationMultivariate2017; @meyerMultivariateSparseClustering2023; @simpsonDeterminingDependenceStructure2020]. This thesis primarily concerns dependence at the pairwise level, so we direct the reader to the aforementioned papers and the review @engelkeSparseStructuresMultivariate2021 for further details.

Let $\chi=(\chi_{ij})$ denote the Tail Dependence Matrix (TDM) of bivariate tail dependence coefficients with diagonal entries $\chi_{ii}:=1$. The TDM provides a high level summary of the extremal dependence structure. It has been applied for exploratory analysis [@huangNewExploratoryTools2019] and considered as a tool for clustering [@fomichovSphericalClusteringDetection2023]. Other works focus on its theoretical properties. @shyamalkumarTailDependenceMatrices2020 conjecture that the 'realisation problem' -- determining whether a given matrix is a valid TDM -- is NP-complete; this was recently proved by @janssenTaildependenceExceedanceSets2023. By establishing a correspondence between the class of TDMs and a metric space, @janssenTaildependenceExceedanceSets2023 also show that, in certain cases, higher order tail-dependence is determined by the bivariate TDM. Section XX introduces a similar (and similarly named) matrix, the Tail *Pairwise* Dependence Matrix (TPDM), which is the eponym of this thesis. Rather than the tail dependence coefficient $\chi_{ij}$, the TPDM is founded on an alternative bivariate summary measure called the Extremal Dependence Measure (EDM).

#### Extremal dependence measure

The extremal dependence measure (EDM) is a pairwise summary measure similar to $\chi_{ij}$. It was originally proposed @resnickExtremalDependenceMeasure2004 and later generalised by @larssonExtremalDependenceMeasure2012.

:::{#def-edm}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with angular measure $H$. The EDM between $X_i$ and $X_j$ is
\begin{equation}
    \mathrm{EDM}_{ij} := \int_{\mathbb{S}_{+}^{d-1}} \theta_i\theta_j \,\dee H(\bm{\theta}).
\end{equation}
:::

The EDM depends on the choice of norm via the angular measure, but @larssonExtremalDependenceMeasure2012 show that EDMs under different norms are equivalent in a certain sense. The EDM was originally defined by @resnickExtremalDependenceMeasure2004 for bivariate random vectors $\bm{X}=(X_1,X_2)$. In their definition, the integrand is
\begin{equation}
\left(\frac{4}{\pi}\right)^2 \arctan\left(\frac{\theta_2}{\theta_1}\right)\left[\frac{\pi}{2} - \arctan\left(\frac{\theta_2}{\theta_1}\right) \right].
\end{equation}
rather than $\theta_1\theta_2$. The original and refined versions are also equivalent. 

Being explicitly defined in terms of the angular measure, the EDM's interpretation in terms of AD/AI is straightforward. Recall from \eqref{eq-asy-dep-chi-H} that variables $X_i$ and $X_j$ are asymptotically independent if and only if $H(\{\bm{\theta}:\theta_i,\theta_j > 0\})=0$. Then
\begin{equation*}
\chi_{ij} = 0 \iff \int_{\{\bm{\theta}\in\mathbb{S}_{+}^{d-1} \, : \, \theta_i,\theta_j > 0\}} \theta_i \theta_j \,\dee H(\bm{\theta}) = 0 \iff \mathrm{EDM}_{ij}=0.
\end{equation*}
The EDM is maximal when $X_i$ and $X_j$ are perfectly asymptotically dependent. The maximal value depends on the choice of norm and the mass of the angular measure. When $d=2$ and $\|\cdot\|=\|\cdot\|_p$ we have $\mathrm{EDM}_{ij} \leq 2^{-2/p}m$ with equality if and only if $H$ places all its mass at the simplex barycentre, that is $H(\{(2^{-1/p}, 2^{-1/p})\}) = m$.

We return to the EDM in Section XX when introducing the tail pairwise dependence matrix.

## Inference

We now shift our attention to the topic of (non-parametric) inference in multivariate extremes. The general approach entails using the angular components of large observations to learn a model for $H$. This strategy is justified by the MRV assumption: \eqref{eq-polar-mrv} implies that
\begin{equation}\label{eq-extremal-angles-converge-H}
    \bm{\Theta}\mid (R>t) \stackrel{d}{\to} H(\cdot), \qquad (t\to\infty).
\end{equation}
The angular measure is the limiting distribution of the angles of exceedances of some radial threshold. By analogy to the peaks-over-threshold approach (Section XX), it suggests itself to base inference on the subset of data points whose norm exceeds some high fixed threshold. Increasing the threshold reduces the number of observations that enter into the estimators, and vice versa. It is generally more convenient to specify the desired number of threshold exceedances, denoted $k$, and set the threshold accordingly. This approach is most conveniently described using order statistics. 

### Framework and notation

Consider a $d$-dimensional MRV random vector $\bm{X}\in\mathcal{RV}_+^d(\alpha)$. Let $\bm{X}_1,\bm{X}_2,\ldots$ denote a sequence of independent copies of $\bm{X}$ and fix a norm $\|\cdot\|$ on $\R^d$. For $i\geq 1$, denote by
\begin{equation}
    R_i := \|\bm{X}_i\|, \qquad \bm{\Theta}_i := (\Theta_{i1},\ldots,\Theta_{id})=\frac{\bm{X}_i}{\|\bm{X}_i\|},
\end{equation}
the radial and angular components of $\bm{X}_i$ with respect to the chosen norm. Assume that the distribution of $\|\bm{X}\|$ is continuous. Then for any $n\geq 1$, there exists a permutation $\pi:\{1,\ldots,n\}\to\{1,\ldots,n\}$ such that 
\begin{equation*}
    \|\bm{X}_{(1),n}\| > \|\bm{X}_{(2),n}\| > \ldots > \|\bm{X}_{(n),n}\|,
\end{equation*}
where $\bm{X}_{(i),n}:=\bm{X}_{\pi(i)}$ for $i=1,\ldots,n$. The random variable $\|\bm{X}_{(j),n}\|$ is called the $j$th (upper) order statistic of $\{\|\bm{X}_i\|:i=1,\ldots,n\}$. Henceforth, we suppress the dependence on $n$ in our order statistic notation. Let the radial and angular components of $\bm{X}_{(i)}$ be denoted by
\begin{equation}
    R_{(i)} = \|\bm{X}_{(i)}\|, \qquad \bm{\Theta}_{(i)} = (\Theta_{(i),1},\ldots,\Theta_{(i),d})=\frac{\bm{X}_{(i)}}{\|\bm{X}_{(i)}\|}.
\end{equation}
Performing inference based on the $k=k(n)$ largest observations is equivalent to performing inference based on the set of observations whose norm exceeds the threshold $t=R_{(k+1)}$.

### Selecting the radial threshold or the number of exceedances

All estimators will require on choosing the number of extreme observations $k$ that enter into them. In theoretical analyses, it is customary to choose the sequence $\{k(n):n\geq 1\}$ such that
\begin{equation}\label{eq-k-rate-conditions}
    \lim_{n\to\infty}k(n)=\infty, \qquad \lim_{n\to\infty}\frac{k(n)}{n}=0.
\end{equation}
These arise as sufficient conditions for proving various asymptotic properties (e.g. consistency, asymptotic normality) of estimators. The condition $k\to\infty$ ensures that the number of extremes -- the effective sample size -- grows arbitrarily large. The second condition $k/n\to 0$ requires that the proportion of threshold exceedances becomes vanishingly small, ensuring that inference is targeting the tail. 
In practice, $n$ is fixed and selecting $k$ requires striking a balance between these two aspects. Choosing $k$ too small reduces the amount of available information and leads to unnecessarily high uncertainty. If $k$ is too large, we risk using data that does not reflect the extremal dependence structure leading to bias. An appropriate choice depends on both the sample size and the underlying distribution of $\bm{X}$. If the convergence in \eqref{eq-extremal-angles-converge-H} is rapid, then a low threshold may be adequate. Several threshold selection procedures have been proposed in univariate extremes (Section XX), but the literature on radial threshold selection is comparatively scant. By combining two sub-tests regarding (i) independence of the radial and angular components and (ii) regular variation of the radial component, @einmahlTestingMultivariateRegular2020 devise a formal procedure testing the validity of the MRV assumption. They suggest choosing the threshold by examining a plot of the sequence of p-values against $k$. The support-detection algorithm of @meyerMultivariateSparseClustering2023 chooses $k$ automatically via minimisation of a penalised log-likelihood. This procedure is specific to their setting and relies on additional technical assumptions. Most applied studies use a rule-of-thumb approach and/or produce a threshold stability plot checking the (in)sensitivity of their results to the choice of $k$ -- see @jiangPrincipalComponentAnalysis2020 and @russellAnalyzingDependenceMatrices2018 for examples. 

### The empirical angular measure

Once the tuning parameter $k$ has been chosen, attention turns towards the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$. In view of \eqref{eq-extremal-angles-converge-H}, the empirical distribution of $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ is the natural non-parametric estimator for the angular measure.

:::{#def-empirical-angular-measure}
The empirical angular measure based on $\bm{X}_1,\ldots,\bm{X}_n$ is the random measure on $\mathbb{S}_{+}^{d-1}$ defined as
\begin{equation}
    \hat{H}(\cdot) := \frac{m}{k}\sum_{i=1}^n \delta_{\bm{\Theta}_i}(\cdot) \ind\{R_i > R_{(k+1)}\} = \frac{m}{k} \sum_{i=1}^k \delta_{\bm{\Theta}_{(i)}}(\cdot).
\end{equation}
:::

Note that $\hat{H}$ does not enforce the moment constraints \eqref{eq-H-mean-constraints}, so is not necessarily a valid angular measure. @einmahlMaximumEmpiricalLikelihood2009 construct an alternative non-parametric estimator that does enforce these restrictions, but it is limited to the bivariate setting. Proposition 3.3 in @janssenKmeansClusteringExtremes2020 establishes consistency $\hat{H}\overset{p}{\to}H$ of the empirical angular measure provided the level $k$ satisfies the rate conditions \eqref{eq-k-rate-conditions}. Their result holds for general norms in arbitrary dimensions. @clemenconConcentrationBoundsEmpirical2023 conduct a non-asymptotic (i.e. finite sample) analysis of $\hat{H}$, establishing high-probability bounds on the worst-case estimation error $\sup_{A\in\mathcal{A}}|H(A) - \hat{H}(A)|$ over classes $\mathcal{A}$ of Borel subsets on $\mathbb{S}_{+}^{d-1}$. Their results hold with $\|\cdot\|=\|\cdot\|_p$ for $p\in[1,\infty]$. Since $\hat{H}$ is a discrete measure concentrating at $k$ points, there exists a max-linear random vector $\bm{X}$ with parameter matrix 
\begin{equation}\label{eq-empirical-A}
    \hat{A} := \left(\frac{m}{k}\right)^{1/\alpha}\left(\bm{\Theta}_{(1)}, \ldots, \bm{\Theta}_{(k)}\right) \in\R_+^{d\times k}.
\end{equation}
whose angular measure is $\hat{H}$. Estimates of tail event probabilities under the empirical model $\hat{H}$ may then be computed using the formula \eqref{eq-max-linear-failure-probability}.

### Non-parametric estimators

@larssonExtremalDependenceMeasure2012 remark that analysing extremal dependence often involves quantities of the form
\begin{equation}\label{eq-expectation-f}
    \mathbb{E}_{H}[f(\bm{\Theta})] := \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee H(\bm{\theta}) = \mathbb{E}_{m^{-1}H}[mf(\bm{\Theta})],
\end{equation}
where $f:\mathbb{S}_{+}^{d-1}\to\R$. We have already seen an example of this in @def-edm: the EDM between $X_i$ and $X_j$ is defined as \eqref{eq-expectation-f} with $f(\bm{\theta})=\theta_i\theta_j$. We reiterate that in our notation, the expectation is with respect to a measure $H$ that is not necessarily normalised. When manipulating expectations/variances, the following relations may be useful to bear in mind:
\begin{align*}
\mathbb{E}_{H} [f(\bm{\Theta})] &= \mathbb{E}_{m^{-1}H}[m f(\bm{\Theta})] = m \mathbb{E}_{m^{-1}H}[f(\bm{\Theta})] \\ 
\mathrm{Var}_{H}[f(\bm{\Theta})] &= \mathbb{E}_{m^{-1}H} [m^2 f(\bm{\Theta})^2] - \mathbb{E}_{m^{-1}H} [mf(\bm{\Theta})]^2 = m^2 \mathrm{Var}_{m^{-1}H}[f(\bm{\Theta})].
\end{align*}
@kluppelbergEstimatingExtremeBayesian2021 opt to normalise $H$ and absorb $m$ into $f$. For example, the EDM would correspond to $f(\bm{\theta})=m\theta_i\theta_j$ in their notation. Suppressing the normalising constant arguably results in less cumbersome notation, but in any case the choice is purely stylistic.

To construct non-parametric estimators of quantities \eqref{eq-expectation-f}, we simply replace $H$ with the empirical angular measure $\hat{H}$, yielding [@kluppelbergEstimatingExtremeBayesian2021]
\begin{equation}
    \hat{\mathbb{E}}_{H} [f(\bm{\Theta})] := \mathbb{E}_{\hat{H}}[f(\bm{\Theta})] = \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee \hat{H}(\bm{\theta}) = \frac{m}{k}\sum_{i=1}^k f(\bm{\Theta}_{(i)}).
\end{equation}

@kluppelbergEstimatingExtremeBayesian2021 prove asymptotic normality of these estimators by generalising a result in @larssonExtremalDependenceMeasure2012. 

:::{#thm-clt-extremes}
Let $f:\mathbb{S}_+^{d-1}\to\R$ be continuous and assume $k$ satisfies the rate conditions $\eqref{eq-k-rate-conditions}$. Moreover, suppose that
\begin{equation}\label{eq-clt-rate-condition}
    \lim_{n\to\infty} \sqrt{k}\left[\frac{n}{k}\mathbb{E}[f(\bm{\Theta}_1)\ind\{R_1 \geq b_{\lfloor n/k \rfloor} t^{-1/\alpha}\}] - \mathbb{E}_{H}[f(\bm{\Theta})]\frac{n}{k}\bar{F}_R(b_{\lfloor n/k \rfloor} t^{-1/\alpha})\right]=0
\end{equation}
holds locally uniformly for $t\in[0,\infty)$, where $\bar{F}_R(\cdot)=\mathbb{P}(R>\cdot)$ denotes the survivor function of $R$. Finally, assume that
\begin{equation}\label{eq-clt-variance-condition}
    \nu^2 := \mathrm{Var}_{H}(f(\bm{\Theta})) > 0.
\end{equation}
Then
\begin{equation}
    \sqrt{k}\left[\hat{\mathbb{E}}_{H} [f(\bm{\Theta})] - \mathbb{E}_{H} [f(\bm{\Theta})]\right]\to N(0, \nu^2), \qquad (n\to\infty).
\end{equation}
:::

The rate condition \eqref{eq-clt-rate-condition} requires that the dependence between the radius and angle decays sufficiently quickly. This condition is non-observable and must be assumed.

:::{#exm-edm-estimator}
Let $\bm{X}_1,\ldots,\bm{X}_n$ be independent copies of $\bm{X}\in\mathcal{RV}_+^d(\alpha)$. The estimator for the EDM between $X_i$ and $X_j$ is
\begin{equation*}
\widehat{\mathrm{EDM}}_{ij} := \hat{\mathbb{E}}_{H} [\Theta_i\Theta_j] \frac{m}{k}\sum_{l=1}^k \Theta_{(l),i}\Theta_{(l),j}.
\end{equation*}
Under the conditions of @thm-clt-extremes,
\begin{equation*}
\sqrt{k}[\widehat{\mathrm{EDM}}_{ij} - \mathrm{EDM}_{ij}] \to N(0, \nu_{ij}^2), \qquad \nu_{ij}^2 = \mathrm{Var}_H(\Theta_i\Theta_j).
\end{equation*}
:::


## Tail pairwise dependence matrix (TPDM)

This section introduces the key protagonist of this thesis: the tail pairwise dependence matrix (TPDM).

### Definition and examples

*Preamble.*

:::{#def-tpdm}
Let $\bm{X}\in\mathcal{RV}_+^d(2)$ with normalising sequence $b_n=n^{1/2}$. Let $H$ denote the angular measure with respect to $\|\cdot\|_2$. The TPDM of $\bm{X}$ is the $d\times d$ matrix
\begin{equation}\label{eq-tpdm}
    \Sigma = (\sigma_{ij}), \qquad \sigma_{ij} = \int_{\mathbb{S}_{+(2)}^{d-1}} \theta_i \theta_j \,\dee H(\bm{\theta}) = \mathbb{E}_{H} [\Theta_i\Theta_j].
\end{equation}
:::

The TPDM is essentially a matrix of EDMs subject to additional restrictions on the tail index, normalising sequence, and norm. Each off-diagonal entry $\sigma_{ij}$ may be interpreted as summarising the dependence between $X_i$ and $X_j$, with $\sigma_{ij}=0$ if and only if the corresponding variables are asymptotically independent. The original definition was generalised by @kirilioukEstimatingProbabilitiesMultivariate2022 to permit general $\alpha$.

:::{#def-tpdm-alpha}
For $\alpha\geq 1$, let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with normalising sequence $b_n=n^{1/\alpha}$. Let $H$ denote the angular measure with respect to $\|\cdot\|_\alpha$. The TPDM of $\bm{X}$ is the $d\times d$ matrix
\begin{equation}\label{eq-tpdm-alpha}
    \Sigma = (\sigma_{ij}), \qquad \sigma_{ij} = \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee H(\bm{\theta}) = \mathbb{E}_{H} [\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}].
\end{equation}
:::

The tail index of $\bm{X}$ is now arbitrary, but the normalisation sequence and norm are still required to conform with this index. It is obvious that these definitions coincide when $\alpha=2$, but @kirilioukEstimatingProbabilitiesMultivariate2022 provide no direct rationale for why \eqref{eq-tpdm-alpha} is the natural generalisation of \eqref{eq-tpdm}. Appendix XX provides a series of results shedding light on this matter. After generalising a result in @fixSimultaneousAutoregressiveModels2021 (@lem-angular-density-transformation), we prove that the TPDM is invariant to the choice of $\alpha$ (@prp-tpdm-h1-formula). This culminates in an expression for the TPDM (for any $\alpha$) in terms of the $L_1$ angular density that does not depend on $\alpha$. We now use of this formula and the angular densities in @semadeniInferenceAngularDistribution2020 to compute the TPDM under the symmetric logistic and Hüsler-Reiss models. These model TPDMs will be especially useful in Chapter XX for evaluating the performance of TPDM estimators.

:::{#exm-symmetric-logistic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ follows the symmetric logistic distribution with dependence parameter $\gamma\in(0,1)$. For any $i\neq j$,
\begin{equation}\label{eq-symmetric-logistic-tpdm}
   \sigma_{ij} = \frac{1-\gamma}{\gamma} \int_0^1 [u(1-u)]^{\frac{1}{\gamma}-\frac{3}{2}}[(1-u)^{1/\gamma} + u^{1/\gamma}]^{\gamma-2}\,\dee u.
\end{equation}
:::

:::{#exm-husler-reiss-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ follows the Hüsler-Reiss distribution with parameter matrix $\Lambda=(\lambda_{ij}^2)$. For any $i\neq j$, 
\begin{equation}\label{eq-husler-reiss-tpdm}
    \sigma_{ij} = \int_0^1 \frac{\exp(-\lambda_{ij}/4)}{2\lambda_{ij} u(1-u)} \phi\left(\frac{1}{2\lambda_{ij}}\log\left(\frac{u}{1-u}\right)\right) \,\dee u.
\end{equation}
:::

The blue lines in @fig-parametric-chi-tpdm plot \eqref{eq-symmetric-logistic-tpdm} and \eqref{eq-husler-reiss-tpdm} against the model parameter. For comparison, we also include the tail dependence coefficients (red lines) computed using @exm-symmetric-logistic-chi and @exm-husler-reiss-chi. For both models, the strength of association is a decreasing function of the model parameter, with complete dependence  (resp. asymptotic independence) as the parameter approaches zero (resp. its upper limit). For the Hüsler-Reiss distribution, dependence is very weak beyond $\lambda\approx 3$. We can check that this is correct by comparing with Figure 1 in the Supplementary Material of @cooleyDecompositionsDependenceHighdimensional2019. The figure reveals that for a Brown-Resnick process with semi-variogram \eqref{eq-fractal-variogram} with range $\rho=2.4$ and smoothness $\kappa=1.8$, dependence vanishes beyond a distance of approximately 12 units. Recall from Section XX that the dependence between two sites $h$ units apart under the Brown-Resnick model is equivalent to the dependence between two Hüsler-Reiss variables with dependence parameter $\lambda_{ij}=\sqrt{2(h/\rho)^\kappa}/2$. Setting $h=12$ gives $\lambda_{ij}=\sqrt{2(12/2.4)^{1.8}}/2\approx 3.01$, corroborating the results of @fig-parametric-chi-tpdm. Further verification of our expressions are provided by the shaded regions in @fig-parametric-chi-tpdm. These represent the minimum/maximum values of 10 estimates of $\chi_{ij}$ and $\sigma_{ij}$ for a sequence of values of $\gamma$ and $\lambda$. The estimates are obtained from large samples ($n=5\times 10^5$) so it is reasonable to neglect the influence of estimation error. The empirical estimates agree with our calculations.


```{r background-make-fig-parametric-chi-tpdm}
#| label: fig-parametric-chi-tpdm
#| fig-cap: "True dependence strengths for the symmetric logistic (left) and Hüsler-Reiss (right) models, measured using the tail dependence coefficient (red line) and TPDM (blue line). The shaded regions represent the minimum/maximum values of empirical estimates over 10 repeated simulations using bivariate samples of size $n=5\\times 10^5$."
#| fig-scap: "Dependence $\\chi$ and $\\sigma$ for symmetric logistic and Hüsler-Reiss models."
#| fig-height: 3.5

data <- readRDS(file.path("scripts", "background", "results", "parametric_chi_tpdm_empirical.RDS")) 

p1 <- ggplot() + 
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) +
  geom_function(aes(colour = "chi"), fun = sl_chi, xlim = c(0, 1)) + 
  geom_function(aes(colour = "tpdm"), fun = sl_tpdm, xlim = c(0, 1)) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 5)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(gamma), 
       y = "Dependence strength", 
       title = "Symmetric logistic") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10))  

p2 <- ggplot() + 
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) + 
  geom_function(aes(colour = "chi"), fun = hr_chi, xlim = c(0, 3.5)) + 
  geom_function(aes(colour = "tpdm"), fun = hr_tpdm, xlim = c(0, 3.5)) + 
  scale_x_continuous(limits = c(0, 3.5), expand = c(0, 0), breaks = breaks_extended(n = 4)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(lambda), 
       y = "Dependence strength", 
       title = "Hüsler-Reiss") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10)) 

ggarrange(p1, p2, ncol = 2, common.legend = TRUE) 
``` 

The angular measure of a max-linear random vector is discrete, so the angular density does not exist. Nevertheless, it is straightforward to compute the model TPDM directly from the definition [@cooleyDecompositionsDependenceHighdimensional2019; @kirilioukEstimatingProbabilitiesMultivariate2022].

:::{#exm-max-linear-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is max-linear with parameter matrix $A$. Then for any $i\neq j$,
\begin{align*}
\sigma_{ij} 
&= \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^{\alpha/2}\theta_j^{\alpha/2} \,\dee H(\bm{\theta}) \\
&= \sum_{l=1}^q \|\bm{a}_{l}\|_\alpha^\alpha \left(\frac{a_{li}}{\|\bm{a}_{l}\|_\alpha}\right)^{\alpha/2}\left(\frac{a_{lj}}{\|\bm{a}_{l}\|_\alpha}\right)^{\alpha/2} \\
&= \sum_{l=1}^q a_{il}^{\alpha/2}a_{jl}^{\alpha/2}.
\end{align*}
Therefore $\Sigma=A^{\alpha/2}(A^{\alpha/2})^T$. Taking $A$ to be $A^{(1)}$ and $A^{(2)}$ as defined in @exm-max-linear-chi, the corresponding TPDMs are
\begin{equation*}
\Sigma^{(1)} = I_d I_d^T  = I_d, \qquad
\Sigma^{(2)} = \bm{1}\bm{1}^T = J_d,
\end{equation*}
where $J_d$ is the $d\times d$ all-ones matrix. By construction, these represent the TPDMs under asymptotic dependence and complete dependence, respectively.
:::

The connection between $A$ and $\Sigma$ will play a prominent role in this thesis. *Say more about this?*

### Interpretation of the TPDM entries

The definition of the TPDM
\begin{equation}\label{eq-tpdm-covariance-form}
    \Sigma = \mathbb{E}_{H}\left[\bm{\Theta}^{\alpha/2}(\bm{\Theta}^{\alpha/2})^T\right],
\end{equation}
bears a striking resemblance to the definition of a covariance matrix in the non-extreme setting. The covariance matrix represents the second-order (central) moment of a random vector. Its diagonal entries convey the scale (variance) of the components, while the off-diagonal entries summarise the strength of association (unnormalised correlation) between all pairs of variables. The TPDM entries offer analogous interpretations, except the notions of scale and association are adapted to refer to properties of the joint distributional tail.

:::{#def-scale}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with normalisation sequence $b_n$. For $i=1,\ldots,d$, the scale of $X_i$ is defined as [@kluppelbergEstimatingExtremeBayesian2021]
\begin{equation*}
    \mathrm{scale}(X_i) = \left[\int_{\mathbb{S}_+^{d-1}}\theta_i^\alpha\,\dee H(\bm{\theta})\right]^{1/\alpha}.
\end{equation*}
:::

As discussed earlier, a well-defined notion of scale must fix either the sequence $b_n$ or the mass of the angular measure in advance. In the above definition, the normalisation sequence is fixed and scaling information is contained in $H$. The scale is so-called because it yields information about the scale of the marginal distributions. Using \eqref{eq-nu-H-relation}, one can show that
\begin{align*}
    \lim_{n\to\infty} n\mathbb{P}(b_n^{-1}X_i > x)
    &= \int_{\mathbb{S}_{+(\alpha)}^{d-1}}\int_{x/\theta_i}^\infty \alpha r^{-\alpha-1}\,\dee r\,\dee H(\bm{\theta}) \\
    &= \int_{\mathbb{S}_{+(\alpha)}^{d-1}} [r^{-\alpha}]_{\infty}^{x/\theta_i} \,\dee H(\bm{\theta}) \\
    &= x^{-\alpha} [\mathrm{scale}(X_i)]^\alpha,
\end{align*}
Moreover, it behaves like a measure of scale: for any $c>0$,
\begin{align*}
    \mathrm{scale}(cX_i) &= 
    \left[\frac{\lim_{n\to\infty} n\mathbb{P}(b_n^{-1}cX_i > x)}{x^{-\alpha}}\right]^{1/\alpha} \\
    &= \left[c^\alpha\frac{\lim_{n\to\infty} n\mathbb{P}(b_n^{-1}X_i > x/c)}{(x/c)^{-\alpha}}\right]^{1/\alpha}\\
    &=c \, \cdot \, \mathrm{scale}(X_i).
\end{align*}
Comparing @def-scale against @def-tpdm-alpha, the diagonal entries of the TPDM are related to the marginal scales via $\mathrm{scale}(X_i)=\sigma_{ii}^{1/\alpha}$. Consequently, if the marginal distributions are standardised to have unit scales, then all diagonal entries of the TPDM are equal to one. Moreover, when $b_n=n^{1/\alpha}$ and $\|\cdot\|=\|\cdot\|_{\alpha}$, the mass of the angular measure relates to the marginal scales via
\begin{equation*}
  \sum_{i=1}^d \sigma_{ii}
    = \sum_{i=1}^d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^\alpha \,\dee H(\bm{\theta}) 
    =  \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \sum_{i=1}^d \theta_i^\alpha \,\dee H(\bm{\theta}) 
    = \int_{\mathbb{S}_{+(\alpha)}^{d-1}}\dee H(\bm{\theta})
    = m.
\end{equation*}
In this thesis, all random vectors will be pre-processed to be on $\alpha$-Fréchet margins and we take $b_n=n^{1/\alpha}$, so that
\begin{align*}
  \sigma_{ii} 
    &= \mathrm{scale}(X_i)^\alpha \\
    &= \frac{\lim_{n\to\infty} n\mathbb{P}(X_i > n^{1/\alpha}x)}{x^{-\alpha}} \\
    &= \frac{\lim_{n\to\infty} n \left\lbrace 1 - \exp\left[-(n^{1/\alpha}x)^{-\alpha}\right] \right\rbrace}{x^{-\alpha}} \\
    &= 1,
\end{align*}
and 
\begin{equation*}
m = \sum_{i=1}^d \sigma_{ii} = d.
\end{equation*}
Standardising the margins is akin to working with re-scaled variables with unit variance in the non-extremes setting. The appropriate analogue to the TPDM then becomes the correlation rather than covariance matrix.

As mentioned earlier, the TPDM's off-diagonal entries are simply pairwise EDMs. Thus the interpretation of $\sigma_{ij}$ is inherited from the EDM: $X_i$ and $X_j$ are asymptotically independent if and only $\sigma_{ij}=0$, and the magnitude of $\sigma_{ij}>0$ reveals the strength of tail dependence between $X_i$ and $X_j$. Like a correlation matrix, $\sigma_{ij}$ attains its maximal value (one) when $X_i$ and $X_j$ are completely dependent (@exm-max-linear-tpdm).


### Decompositions of the TPDM

The TPDM is useful as a summary statistic for quantifying pairwise dependencies, but what sets it apart from other pairwise dependence matrices (e.g. the TDM)? The TPDM admits two types of decomposition: eigendecomposition and the completely positive decomposition \parencite{cooley_decompositions_2019}. These underpin most statistical applications of the TPDM. The following results and proofs are reproduced from @kirilioukEstimatingProbabilitiesMultivariate2022.

:::{#prp-tpdm-symmetric-positive-definite}
The TPDM is symmetric and positive semi-definite. 
:::

::: {.proof}
For any $i,j=1,\ldots,d$,
\begin{equation*}
    \sigma_{ij} = \int_{\mathbb{S}_+^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee H(\bm{\theta}) = \int_{\mathbb{S}_+^{d-1}} \theta_j^{\alpha/2} \theta_i^{\alpha/2} \,\dee H(\bm{\theta}) =\sigma_{ji}.
\end{equation*}
Hence $\Sigma=\Sigma^T$. For any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation*}
  \bm{y}^T \Sigma \bm{y} 
  = \bm{y}^T \mathbb{E}_{H}[\bm{\Theta}^{\alpha/2}(\bm{\Theta}^{\alpha/2})^T] \bm{y} 
  = \mathbb{E}_{H}\left[\left(\bm{y}^T \bm{\Theta}^{\alpha/2} \right)^2 \right] \geq 0.
\end{equation*}
:::

By standard linear algebra results, the TPDM can be decomposed as $\Sigma=UDU^T$, where $D\in\R^{d\times d}$ is a diagonal matrix of eigenvalues $\lambda_1 \geq \ldots \geq \lambda_d \geq 0$ and $U\in\R^{d\times d}$ is an orthogonal matrix whose columns are the corresponding eigenvectors $\bm{u}_1,\ldots,\bm{u}_d\in\R^d$. *Foreshadow here.*

:::{#def-cp}
A matrix $M\in\R^{d\times d}$ is completely positive (CP) if there exists a matrix $B\in\R_+^{d\times q}$ such that $M=BB^T$.
:::

:::{#prp-tpdm-completely-positive}
The TPDM is completely positive.
:::

::: {.proof}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with angular measure $H$ and TPDM $\Sigma$. By Proposition 5 in @fougeresDenseClassesMultivariate2013, there exists a sequence of matrices $\{A_q\in\R_+^{d\times q}:q\geq 1\}$ such that $H_q\overset{v}{\to} H$, where $H_q$ is the angular measure of the max-linear random vector $\bm{X}_q\in\mathcal{RV}_+^d(\alpha)$ parametrised by $A_q$. The TPDM of $\bm{X}_q$ is $\Sigma_q=A_q^{\alpha/2}(A_q^{\alpha/2})^T$ by @exm-max-linear-tpdm. Thus, $\{\Sigma_q : q\geq 1\}$ is a sequence of completely positive matrices. The limit $\lim_{q\to\infty}\Sigma_q=\Sigma$ must also be completely positive (CITE Theorem 2.2 in Berman \& Shaked-Monderer (2003)).
:::

In principle this provides a way to check whether a given matrix is a TPDM, but the membership problem for the completely positive cone is NP-hard [@dickinsonComputationalComplexityMembership2014]. *Foreshadow here.*

### The empirical TPDM

:::{#def-empirical-tpdm}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ on Fréchet margins \eqref{eq-alpha-frechet} and let $H$ be the angular measure with respect to $\|\cdot\|_\alpha$ and normalising sequence $b_n=n^{1/\alpha}$. Let $\bm{X}_1,\ldots,\bm{X}_n$ be an iid sample of $\bm{X}$. The empirical TPDM estimator is the $d\times d$ matrix
\begin{equation}\label{eq-empirical-tpdm}
  \hat{\Sigma} = (\hat{\sigma}_{ij}), \qquad 
  \hat{\sigma}_{ij} := \hat{E}_H[\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}]=\frac{d}{k}\sum_{l=1}^k \Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2}.
\end{equation}
:::

Note that the empirical TPDM implicitly depends on the customary tuning parameter $k$ -- or equivalently a radial threshold $t>0$ -- via the empirical angular measure. 

:::{#prp-empirical-tpdm-completely-positive}
The empirical TPDM is completely positive.
:::

::: {.proof}
Let $A=\hat{A}$, the $d\times k$ matrix with non-negative entries defined in \eqref{eq-empirical-A}. Then
\begin{equation*}
    \hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T = \frac{d}{k} \sum_{i=1}^k \bm{\Theta}_{(i)}^{\alpha/2} \left(\bm{\Theta}_{(i)}^{\alpha/2}\right)^T = \hat{\Sigma}.
\end{equation*}
:::

:::{#prp-empirical-tpdm-symmetric-positive-definite}
The empirical TPDM is symmetric and positive semi-definite.
:::

::: {.proof}
By complete positivity, $\hat{\Sigma}=AA^T$ for some matrix $A$. For any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation}
    \bm{y}^T\hat{\Sigma}\bm{y} = \bm{y}^T AA^T \bm{y} = \|A^T\bm{y}\|_2^2 \geq 0.
\end{equation}
Since $\mathrm{rank}(\hat{\Sigma})=\mathrm{rank}(AA^T)=\mathrm{rank}(A)$, the empirical TPDM is positive definite if and only if the columns of $A$ are linearly independent.
:::

:::{#prp-empirical-tpdm-normality-entries}
Under the conditions of @thm-clt-extremes, the entries of $\hat{\Sigma}$ are consistent and asymptotically normal, that is, for any $i,j=1,\ldots,d$,
\begin{equation}
\sqrt{k}(\hat{\sigma}_{ij} - \sigma_{ij}) \to\mathrm{N}(0,\nu_{ij}^2), \qquad 
\nu_{ij}^2 := \mathrm{Var}_{H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}).
\end{equation}
:::  

::: {.proof}
See @exm-edm-estimator. 
:::

If $X_i$ and $X_j$ are asymptotically independent ($\sigma_{ij}=0$), then $\nu_{ij}^2=0$ and the limit distribution is degenerate. In this case, the above result only proves consistency, i.e. $\hat{\sigma}_{ij}\to 0$. Thus, it is not possible to formally test for asymptotic independence ($\sigma_{ij}=0$ against $\sigma_{ij}>0$) using this result. Alternative strategies are explored in @lehtomaaAsymptoticIndependenceSupport2020].

Using asymptotic normality one may construct asymptotic confidence intervals
\begin{equation*}
    \lim_{n\to\infty}\mathbb{P}\left[ |\sigma_{ij}-\hat{\sigma}_{ij}| < z_{\beta/2}\sqrt{\nu_{ij}^2/k}\right] = 1-\beta,
\end{equation*}
where $z_{\beta/2}=\Phi^{-1}(1-\beta/2)$. If the angular measure is known the asymptotic variance $\nu_{ij}^2$ may be computed using the formula derived in Appendix XX.


:::{#exm-symmetric-logistic-asymptotic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is symmetric logistic with $\gamma=0.6$. Using  @exm-symmetric-logistic-tpdm and results in Appendix XX, $\sigma_{ij}\approx 0.760$ and $\nu_{ij}^2 \approx 0.065$ for all $i\neq j$. For sufficiently large $n$, 
\begin{equation*}
    \mathbb{P}\left[\hat{\sigma}_{ij} \in \left(0.760 \pm 1.96 \sqrt{\frac{0.065}{k}}\right)\right] \approx 0.95.
\end{equation*}
For example, setting $n=10^4$ and $k=\sqrt{n}$ yields $\mathbb{P}(0.710 < \hat{\sigma}_{ij} < 0.810 )\approx 0.95.$
:::

In practice, the asymptotic variance may be replaced with the plug-in estimator [@leePartialTailCorrelation2023]
\begin{equation*}
\hat{\nu}_{ij}^2 := \frac{1}{k-1} \sum_{l=1}^k \left(d \Theta_{(l),i}\Theta_{(l),j} - \hat{\sigma}_{ij} \right)^2.
\end{equation*}

The following result, proved by @kraliCausalityEstimationMultivariate2018 for $\alpha=2$, generalises asymptotic normality of the empirical TPDM to the entire matrix, rather than just individual entries. This is most simply expressed in terms of upper-half vectorisations of $\Sigma$ and $\hat{\Sigma}$, that is
\begin{align*}
    \bm{\sigma}
    &:= \mathrm{vecu}(\Sigma) 
    := (\sigma_{12},\sigma_{13},\ldots,\sigma_{1d},\sigma_{23},\ldots,\sigma_{2d},\ldots,\sigma_{d-1,d}), \\
    \hat{\bm{\sigma}} 
    &:= \mathrm{vecu}(\hat{\Sigma}) 
    := (\hat{\sigma}_{12},\hat{\sigma}_{13},\ldots,\hat{\sigma}_{1d},\hat{\sigma}_{23},\ldots,\hat{\sigma}_{2d},\ldots,\hat{\sigma}_{d-1,d}).
\end{align*}
Each vector contains ${d \choose 2}=d(d-1)/2$ entries taken from the strictly upper-triangular part of each matrix. This is justified because the matrices are symmetric and we are not concerned with their diagonal entries. For simplicity, components are indexed according to the sub-indices of the corresponding matrix entry, e.g. the first entry of $\bm{\sigma}$ is $\sigma_{12}$ rather than $\sigma_{1}$.

:::{#prp-empirical-tpdm-normality}
Under the conditions of @thm-clt-extremes, the estimator $\hat{\bm{\sigma}}$ is consistent and asymptotically normal, i.e. 
\begin{equation*}
    \sqrt{k}(\hat{\bm{\sigma}}-\bm{\sigma}) \to N(\bm{0},V),
\end{equation*}
The diagonal and off-diagonal entries of the ${d \choose 2}\times{d \choose 2}$ asymptotic covariance matrix $V$ are given by
\begin{equation*}
   v_{ij,lm} 
   := \lim_{k\to\infty}k\mathrm{Cov}(\hat{\sigma}_{ij},\hat{\sigma}_{lm}) 
   = \begin{cases}
      \nu_{ij}^2, & (i,j) = (l,m),\\
      \rho_{ij,lm} & \text{otherwise},
    \end{cases}
\end{equation*}
where $\nu_{ij}^2$ is as defined in @prp-empirical-tpdm-normality-entries and
\begin{equation*}
    \rho_{ij,lm} := \frac{1}{2}\left[\mathrm{Var}_{H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) - \nu_{ij}^2 - \nu_{lm}^2\right].
 \end{equation*}
:::

The proof can be found in Appendix XX. It extends the proof of Theorem 5.23 in @kraliCausalityEstimationMultivariate2018 to permit general $\alpha$. The following example illustrates an application of @prp-empirical-tpdm-normality to the max-linear model.

:::{#exm-max-linear-asymptotic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_4)\in\mathcal{RV}_+^4(1)$ is max-linear with (randomly generated) parameter matrix $A\in\R_+^{4\times 12}$ as shown in @fig-max-linear-example-A-Sigma-V (top). The TPDM $\Sigma=A^{1/2}(A^{1/2})^T$ is visualised in the bottom-left plot, with each cell's colour intensity representing the magnitude of the corresponding entry of $\Sigma$. All pairs of components exhibit strong dependence. The matrix in the bottom-right is the asymptotic covariance matrix $V$ of $\hat{\bm{\sigma}}$, derived in Appendix XX. It has ${4\choose 2}=6$ rows and columns. *Any comments about the matrix itself?* We now run simulations verifying/illustrating @prp-empirical-tpdm-normality for this example. We generate $n=10^4$ independent observations $\bm{x}_1,\ldots,\bm{x}_n$ of $\bm{X}=A\times_{\max}\bm{Z}$ (see eq-max-linear-X) and compute the empirical TPDM using $k=\sqrt{n}=100$ extremes. Repeating this process, we obtain 1,000 independent realisations of $\hat{\Sigma}$. After row-wise vectorisation, these estimates should be approximately $N(\bm{\sigma}, k^{-1}V)$ distributed. @fig-max-linear-example-ggpairs examines whether this is the case. First consider the diagonal panels. These show that the density function of an $N(\sigma_{ij},\nu_{ij}^2/k)$ random variable (blue curve) provides a good fit for the empirical distribution of $\hat{\sigma}_{ij}$ (red histogram). Now consider the scatter plots in the lower triangular portion of the plot. The grey points represent 1,000 realisations of $(\hat{\sigma}_{ij},\hat{\sigma}_{lm})$. The blue ellipses are the true asymptotic 95\% data ellipses centred at $(\sigma_{ij},\sigma_{lm})$ (blue crosses). Their orientation relates to the association $\rho_{ij,lm}$ between $\hat{\sigma}_{ij}$ and $\hat{\sigma}_{lm}$, while the lengths of the major and minor axes are dictated by the asymptotic variances $\nu_{ij}^2,\nu_{lm}^2$. The red ellipses and crosses are defined analogously but estimated from the data. They are generally in close agreement. The upper-triangular panels list the true values of $\rho_{ij,lm}$ (blue) alongside empirical estimates (red) based on the sample covariance between $\hat{\sigma}_{ij}$ and $\hat{\sigma}_{lm}$. 
:::

```{r make-fig-max-linear-example-A-Sigma-V}
#| label: fig-max-linear-example-A-Sigma-V
#| fig-cap: "Visual representation of the matrices discussed in @exm-max-linear-asymptotic-tpdm. Top: a randomly generated max-linear parameter matrix $A$ with $d=4$ and $q=12$. Bottom left: the TPDM $\\Sigma$ of $\\bm{X}=A\\times_{\\max}\\bm{Z}$. Bottom right: the asymptotic covariance matrix $V$ of $\\hat{\\bm{\\sigma}}$."
#| fig-scap: "Max-linear parameter matrix $A$ and the associated $\\Sigma$ and $V$."
#| fig-height: 7

data <- readRDS("scripts/background/results/max-linear-asymptotic-tpdm.RDS")

p1 <- plot_tpdm(data$A, y_labels = FALSE) %>% update(aspect = 0.5)
p2 <- plot_tpdm(data$Sigma)
p3 <- plot_tpdm_eigen(data$V)

ggarrange(p1, ggarrange(p2, p3, ncol = 2), nrow = 2)
```


```{r make-fig-max-linear-example-ggpairs}
#| label: fig-max-linear-example-ggpairs
#| fig-cap: "Pairs plot illustrating asymptotic normality of the empirical TPDM -- see @exm-max-linear-asymptotic-tpdm for details. All panels: red represents the empirical quantity based on the 1,000 repeated simulations; blue represents the theoretical quantity based on asymptotic normality. Diagonal panels: the distribution (histogram or density function) of $\\hat{\\sigma}_{ij}$. Lower triangular panels: pairwise scatter plots of $(\\hat{\\sigma}_{ij},\\hat{\\sigma}_{lm})$ (grey points) along with the mean (crosses) and the 95\\% data ellipse. Upper triangular panels: the entries $v_{ij,lm}$ of $V$."
#| fig-scap: "Empirical verification of asymptotic normality of $\\hat{\\bm{\\sigma}}$."
#| fig-height: 6
#| warning: false
#| message: false

tpdm_max_linear_ggpairs(data = data$emp_V, Sigma = data$Sigma, V = data$V, k = data$k)
```


## Existing applications and extensions of the TPDM

The general goal of this thesis is to develop novel statistical applications of the TPDM for analysing extremal dependence. Before outlining our contributions, it seems logical to first familiarise the reader with existing TPDM-based methods in the literature. Our methods will either build upon these (e.g. compositional PCA in Chapter XXX) or address gaps in the field.

Our survey divides TPDM-related tools into four categories: principal components analysis (PCA), clustering, model fitting, and miscellaneous. The PCA methods leverage the TPDM eigendecomposition to perform dimension reduction. The extremal dependence structure is thereby represented by a low-dimensional object, facilitating exploratory analysis [@jiangPrincipalComponentAnalysis2020; @russellAnalyzingDependenceMatrices2018; @szemkusSpatialPatternsIndices2024]


and generation of synthetic extreme events [@rohrbeckSimulatingFloodEvent2023]. The clustering techniques use the TPDM to partition a collection of random variables into groups according to asymptotic (in)dependence [@fomichovSphericalClusteringDetection2023; @richardsModernExtremeValue2024]. When applied as a preliminary step, this splits a high-dimensional problem into several `independent', low-dimensional, sub-problems. The model fitting section explains how the TPDM can be used to aid inference for the max-linear model [@fixSimultaneousAutoregressiveModels2021; @kirilioukEstimatingProbabilitiesMultivariate2022]. Fitting a parametric model permits straightforward estimation of tail event probabilities. We conclude with a summary of other applications and extensions of the TPDM, such as in time series [@mhatreTransformedLinearModelsTime2021] and graphical models [@gongPartialTailCorrelationCoefficient2024; @leePartialTailCorrelation2023].

### Principal component analysis (PCA) for extremes

:::{#def-sparsity-i}
The support of the angular measure has dimension $p^\star \ll d$.
:::

This means the angular measure can be represented by a low-dimensional object, prompting the application of dimension reduction methods. 

#### PCA in general finite-dimensional Hilbert spaces

In classical multivariate analysis, principal component analysis (PCA) is the flagship method for reducing the dimension of a random vector. PCA identifies linear subspaces that minimise the distance between the data and its low-dimensional projections.

PCA revolves around an underlying algebraic-geometric structure. Specifically, PCA assumes one is working in a Hilbert space $\mathcal{H}$. Without this theoretical foundation, it is meaningless to speak of principal components as orthogonal basis vectors or consider low-rank reconstructions as unique projections onto a subspace. A Hilbert space comprises a $d$-dimensional vector space with operations $\oplus$ and $\odot$ endowed with an inner product $\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}$. The induced norm and metric are $\|\cdot\|_{\mathcal{H}}=\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}^{1/2}$ and $d_{\mathcal{H}}(\bm{x},\bm{y})=\|\bm{x}\ominus\bm{y}\|_{\mathcal{H}}$, respectively. In most applications $\mathcal{H}=\R^d$ with the usual Euclidean geometry. This thesis will additionally consider PCA in alternative spaces, including $\R_+^d$ and $\mathbb{S}_{+(1)}^{d-1}$. However, in each case, the Hilbert space in question will be isometric to the usual Euclidean space $(\R^d, \left\langle\cdot,\cdot\right\rangle)$. That is, there exists an isomorphism $h:\mathcal{H}\to\R^d$ such that for any $\bm{x},\bm{y}\in\mathcal{H}$,
\begin{equation*}
    \left\langle\bm{x},\bm{y}\right\rangle_{\mathcal{H}} = \left\langle h(\bm{x}),h(\bm{y})\right\rangle, \qquad \|\bm{x}\ominus \bm{y}\|_{\mathcal{H}} = \|h(\bm{x})-h(\bm{y})\|_2.
\end{equation*}
We present PCA for random vectors in $\R^d$, with the understanding that the data may have undergone an isometric transformation in pre-processing and outputs may need to be back-transformed to lie in the original space. This transform/back-transform approach is equivalent to conducting the analysis in the original space with appropriately generalised notions of mean, variance, etc. [@pawlowsky-glahnGeometricApproachStatistical2001].

\begin{table}[]
\small
\begin{tabular}{@{}llll@{}}
\toprule
$\mathcal{H}$ & $\R^d$ & $\R_+^d$ @cooleyDecompositionsDependenceHighdimensional2019 & $\mathbb{S}_{+(1)}^{d-1}$ @aitchisonPrincipalComponentAnalysis1983 \\ \midrule
$h:\mathcal{H}\to\R^d$ & $h(\bm{x})=\bm{x}$ & $h(\bm{x}) = \tau^{-1}(\bm{x}) = \log[\exp(\bm{x})-1]$ & $h(\bm{x})=\mathrm{clr}(\bm{x})=\log[\bm{x}/\bar{g}(\bm{x})]$ \\
$h^{-1}:\R^d\to\mathcal{H}$ & $h^{-1}(\bm{y})=\bm{y}$ & $h^{-1}(\bm{y}) = \tau(\bm{y}) = \log[1+\exp(\bm{y})]$ & $h^{-1}(\bm{y}) = \mathrm{clr}^{-1}(\bm{y})=\mathcal{C}\exp(\bm{y})$ \\
$\bm{x}\oplus\bm{y}$ & $\bm{x} + \bm{y}$ & $\tau[\tau^{-1}(\bm{x})+\tau^{-1}(\bm{y})]$ & $\mathcal{C}(x_1y_1,\ldots,x_dy_d)$ \\
$\alpha\odot\bm{x}$ & $\alpha \bm{x}$ & $\tau[\alpha \tau^{-1}(\bm{x})]$ & $\mathcal{C}(x_1^\alpha,\ldots,x_d^\alpha)$ \\
$\left\langle \bm{x},\bm{y}\right\rangle_{\mathcal{H}}$ & $\sum_{i=1}^d x_iy_i$ & $\sum_{i=1}^d \tau^{-1}(x_i)\tau^{-1}(y_i)$ & $\sum_{i=1}^d \log[x_i/\bar{g}(\bm{x})]\log[y_i/\bar{g}(\bm{x})]$ \\ \bottomrule
\end{tabular}
\end{table}

Suppose $\bm{Y}=(Y_1,\ldots,Y_d)$ is a random vector in $\R^d$ satisfying $\mathbb{E}[\|\bm{Y}\|_2^2]<\infty$. Let $\bm{Y}_1,\ldots,\bm{Y}_n$ be independent copies of $\bm{Y}$. The reconstruction error of a subspace $\mathcal{S}\subseteq\R^d$ is measured as
\begin{equation}\label{eq-pca-true-risk}
    R(\mathcal{S}) := \mathbb{E}[\|\bm{Y}-\Pi_{\mathcal{S}}\bm{Y}\|_2^2] 
\end{equation}
Fundamental to PCA are the eigenvectors $\bm{u}_1,\ldots,\bm{u}_d\in\R^d$ and respective eigenvalues $\lambda_1\geq \ldots \geq \lambda_d \geq 0$ of the positive semi-definite matrix
\begin{equation*}
    \Sigma=\mathbb{E}[\bm{Y}\bm{Y}^T].
\end{equation*}
The entries of $\Sigma$, herein referred to as the non-centred covariance matrix, are the second-order moments of $\bm{Y}$.  By a change of basis, the random vector $\bm{Y}$ may be equivalently decomposed as 
\begin{equation*}
    \bm{Y} = \sum_{j=1}^d \left\langle \bm{Y},\bm{u}_j\right\rangle \bm{u}_j.
\end{equation*}
The scores $V_j:=\left\langle \bm{Y},\bm{u}_j \right\rangle$ represent the stochastic basis coefficients when $\bm{Y}$ is decomposed into the basis $\{\bm{u}_1,\ldots,\bm{u}_d\}$. They satisfy $\mathbb{E}[V_iV_j]=\lambda_i\ind\{i=j\}$. For $1\leq p < d$, the truncated expansion 
\begin{equation*}
    \hat{\bm{Y}}^{[p]} := \sum_{j=1}^p V_j \bm{u}_j = \Pi_{\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_p\}}\bm{Y}.
\end{equation*}
produces the optimal $p$-dimensional projection of $\bm{Y}$. In other words, the subspace $\mathcal{S}_p=\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_p\}$ minimises the criterion \eqref{eq-pca-true-risk} over $\mathcal{V}_p$, the set of all linear subspaces of dimension $p$ of $\R^d$. It is the unique minimiser provided the multiplicity of $\lambda_p$ is one. The corresponding risk is determined by the eigenvalues of the discarded components via $R(\mathcal{S}_p)=\sum_{j>p}\lambda_j$. 

In practice, the covariance matrix is unknown so \eqref{eq-pca-true-risk} cannot be minimised directly. Instead we resort to an empirical risk minimisation (ERM) approach, whereby the risk is replaced by
\begin{equation}\label{eq-pca-empirical-risk}
    \hat{R}(\mathcal{S}) := \frac{1}{n} \sum_{i=1}^n \|\bm{Y}_i-\Pi_{\mathcal{S}}\bm{Y}_i\|_2^2
\end{equation} 
Minimisation of the empirical risk follows analogously based on the empirical non-centred covariance matrix 
\begin{equation*}
    \hat{\Sigma}=\frac{1}{n}\sum_{i=1}^n \bm{Y}_i\bm{Y}_i^T
\end{equation*}
and its ordered eigenpairs $(\hat{\lambda}_j,\hat{\bm{u}}_j)$ for $j=1,\ldots,d$. For $p=1,\ldots,d$ and $i=1,\ldots,n$, the rank-$p$ reconstruction of $\bm{Y}_i$ is given by
\begin{equation*}
    \hat{\bm{Y}}_i^{[p]} := \sum_{j=1}^p \hat{V}_{ij} \bm{u}_j = \Pi_{\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\}}\bm{Y},
\end{equation*}
where $\hat{V}_{ij}:=\left\langle \bm{Y}_i,\bm{u}_j\right\rangle$.
The subspace $\hat{\mathcal{S}}_p=\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\}$ minimises \eqref{eq-pca-empirical-risk} in $\mathcal{V}_p$; the objective at the minimum is $\hat{R}(\hat{\mathcal{S}}_p)=\sum_{j>p}\hat{\lambda}_j$.

Usually the dimension of the target subspace (if it exists) is unknown, so the number of retained components $p$ must be selected according to some criterion. At the heart of this choice is a trade-off between dimension reduction and approximation error. Selecting $p=\max\{j:\hat{\lambda}_j>0\}$ results in perfect reconstructions but the reduction in dimension will be minimal if any. Excessive compression incurs information loss and destroys key features of the data. Several criteria for selecting the number of retained components based on the eigenvalues have been proposed. These include stopping when the reconstruction error $\sum_{j>p}\hat{\lambda}_j$ is acceptably small, cutting off components with $\lambda_j<1$, or retaining components based on where the `scree plot' forms an elbow.  

If $\bm{Y}$ is mean-zero (or the $n\times d$ data matrix is column-centred in pre-processing), then $\Sigma$ is the covariance matrix of $\bm{Y}$ and the procedure is termed centred PCA. In this case, PCA can be equivalently reformulated in terms of finding low-dimensional projections that maximally preserve variance. In the non-centred case this interpretation is not valid, the projections merely maximise variability around the origin. A detailed comparison between centred PCA and non-centred PCA is conducted in @cadimaRelationshipsUncentredColumnCentred2009. They obtain relationships between and bounds on the eigenvectors/eigenvalues of the non-centred and standard covariance matrices. Based on their theoretical analysis and a series of example, they conclude that both types of PCA generally produce similar results. In particular, the leading eigenvector (up to sign and scaling) of the non-centred covariance matrix is very often close to the vector of the column means of the data matrix. Thus the first non-centred principal component essentially relates to the centre of the data.

We now return to the context of multivariate extremes. Suppose $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ has sparse angular measure $H$ and $\bm{X}_1,\ldots,\bm{X}_n$ is a sample of $\bm{X}$. There are several reasons why the  the low-dimensional structure of the angular measure cannot be identified by naively applying standard PCA to $\bm{X}_1,\ldots,\bm{X}_n$. At a practical level, the components $X_1,\ldots,X_d$ are heavy-tailed, so the requirement that second-order moments exist may be violated. The variance of an $\alpha$-regularly varying random variable is infinite if $\alpha<2$. More pertinently, standard PCA reveals relationships between variables in the centre rather than the tail of the joint distribution, because it arises from the covariance matrix. Moreover, the non-centred/centred covariance matrix captures dependence in both directions around the origin/mean, whereas we focus on extremes in a particular direction of interest (`positive'). Finally, standard PCA fails to capitalise on the probabilistic structure inherent to MRV random vectors. The one-dimensional radial component is (asymptotically) independent of the angular component. This points towards targetting dimension reduction at the angular component $\bm{\Theta}$ rather than the original vector $\bm{X}$. Indeed, the two key PCA methods of @dreesPrincipalComponentAnalysis2021 and @cooleyDecompositionsDependenceHighdimensional2019 follow this approach. Despite emerging almost simultaneously, both are essentially based on eigendecomposition of the TPDM. 

#### @dreesPrincipalComponentAnalysis2021

Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathrm{RV}_+^{d-1}(2)$ with angular measure $H$ with respect to the Euclidean norm $\|\cdot\|_2$. The aim is to identify a low-dimensional linear subspace of $\R^d$ supporting $H$. For any subspace $\mathcal{S}\subset\R^d$, define the risk
\begin{equation*}
    R(\mathcal{S}) = \mathbb{E}_{\bm{\Theta}\sim H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_2^2].
\end{equation*}
This represents the expected reconstruction error under the limit model. By assumption, there exists a linear subspace $\mathcal{S}^\star\in\mathcal{V}_{p^\star}$ of dimension $p^\star \ll d$ such that $R(\mathcal{S}^\star)=0$, and $R(\mathcal{S})>0$ for all $\mathcal{S}\in\mathcal{V}_p$ with $p<p^\star$. The angular measure is unknown, so they adopt an ERM approach following the intuition that above a sufficiently high threshold the extremal angles will lie in a neighbourhood of $\mathcal{S}^\star$. The empirical risk is defined by replacing $H$ with the empirical angular measure $\hat{H}$ based on the $k$ largest observations in norm among a sample $\bm{X}_1,\ldots,\bm{X}_n$. That is
\begin{equation*}
    \hat{R}(\mathcal{S}) 
    := \hat{\mathbb{E}}_{\bm{\Theta}\sim H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_2^2] 
    = \frac{m}{k}\sum_{i=1}^k \|\bm{\Theta}_{(i)}-\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}\|_2^2.
\end{equation*}
This setup is almost identical to classical PCA on the random vector $\bm{\Theta}$. Note that boundedness of the simplex guarantees $\mathbb{E}[\|\bm{\Theta}\|_2^2]<\infty$. Let $\Sigma=\mathbb{E}_{\bm{\Theta}\sim H}[\bm{\Theta}\bm{\Theta}^T]$ be the TPDM of $\bm{X}$ and $(\bm{u}_j,\lambda_j)$ its (ordered) eigenpairs for $j=1,\ldots,d$. Then $\mathcal{S}_p=\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_d\}$ minimises $R$ in $\mathcal{V}_p$ and $R(\mathcal{S}_p)=\sum_{j>p}\lambda_p$. Choosing $p\geq p^\star$ yields $R(\mathcal{S}_p)=0$. Analogously, the minimiser $\hat{\mathcal{S}}_p\in\mathcal{V}_p$ of $\hat{R}$ is the subspace spanned by the leading $p$ eigenvectors of the empirical TPDM $\hat{\Sigma}$. 

 @dreesPrincipalComponentAnalysis2021 derive theoretical statistical guarantees for their approach. Most importantly, they prove that the learnt subspace converges to the optimal one as the sample size increases to infinity. Provided $k(n)$ satisfies the rate conditions \eqref{eq-k-rate-conditions}, then $\hat{\mathcal{S}}_{p}\to \mathcal{S}_{p}$ in the sense that
\begin{equation*}
    \lim_{n\to\infty} \sup_{\bm{\theta}\in\mathbb{S}_{+(2)}^{d-1}} \|\Pi_{\hat{\mathcal{S}}_{p}}\bm{\theta} - \Pi_{\mathcal{S}_{p}}\bm{\theta}\|_2  = 0.
\end{equation*}
If the target dimension is chosen correctly as $p=p^\star$, then $\hat{\mathcal{S}}_{p^\star}\to \mathcal{S}^\star$. They also provide high probability bounds on $|\hat{R}(\mathcal{S})-R(\mathcal{S})|$ for fixed $n$.

Basing their approach on the angles viewed as points in $\R^d$ eases the derivation of theoretical guarantees, but creates interpretability issues. Consider $\hat{\bm{\Theta}}_i^{[p]}=\Pi_{\mathcal{S}_p}\bm{\Theta}_i$, the rank-$p$ reconstruction of an extremal angle $\bm{\Theta}_i$. Its components need not satisfy the unit-norm constraint and may even be negative. This may be remedied by shifting/normalising $\hat{\bm{\Theta}}_i^{[p]}$ appropriately, but its optimality properties will be destroyed in the process. One can also question whether Euclidean distances are an appropriate measure of angular reconstruction error; angular distances such as cosine distance may be better suited. Similarly, the hypothesis that the angular measure's low-dimensional structure manifests in a linear fashion may be unrealistic, since data in the simplex are prone to exhibit curvature @aitchisonPrincipalComponentAnalysis1983.

#### @cooleyDecompositionsDependenceHighdimensional2019

The PCA technique developed by @cooleyDecompositionsDependenceHighdimensional2019 focusses on reconstruction and exploration of extreme events in terms of the original vector $\bm{X}$. As such, their PCA is grounded on an inner product space on $\mathcal{H}=\R_+^d$, the natural sample space of the data. The vector space is based on the softplus transformation
\begin{equation*}
    \tau:\R\to \R_+, \qquad \tau(x) = \log[1+\exp(x)].
\end{equation*}
This transformation is bijective with inverse function $\tau^{-1}(y)=\log[\exp(y)-1]$. The reason for choosing this particular mapping is that it is tail-preserving, i.e. $\lim_{x\to 1}\tau(x)/x = 1$. This provides an avenue for moving between the spaces $\R^d$ and $\R_+^d$ with negligible effect on the tails.

The linear-transformed inner product space is constructed as follows. For any $\bm{x},\bm{y}\in\R_+^d$ and $\alpha\in\R$, define
\begin{align*}
    \bm{x} \oplus \bm{y} &= \tau[\tau^{-1}(\bm{x})+\tau^{-1}(\bm{y})] \\
    \alpha \odot \bm{x} &= \tau[a\tau^{-1}(\bm{x})].
\end{align*}
Then the vector space $(\R_+^d,\oplus,\odot)$ is endowed with an inner product and norm
\begin{align*}
    \left\langle \bm{x},\bm{y}\right\rangle_\tau &= \sum_{i=1}^d \tau^{-1}(x_i)\tau^{-1}(y_i) = \left\langle \tau^{-1}(\bm{x}),\tau^{-1}(\bm{y})\right\rangle \\
    \|\bm{x}\|_\tau &= \left\langle \bm{x},\bm{x}\right\rangle_\tau^{1/2} = \|\tau^{-1}(\bm{x})\|_2.
\end{align*}
The transform $\tau^{-1}$ is an isometry linking their inner product space on the positive orthant to the standard Euclidean space $\R^d$. Thus the PCA of @cooleyDecompositionsDependenceHighdimensional2019 can be equivalently formulated in $\R_+^d$ with regards to the original data in the space or in $\R^d$ using the transform/back-transform approach articulated earlier.

Suppose $\bm{X}\in\mathrm{RV}_+^d(\alpha)$ has TPDM $\Sigma$. Denote the ordered eigenpairs of $\Sigma$ in $\R^d$ by $(\bm{u}_j,\lambda_j)$ for $j=1,\ldots,d$. Then $\{\bm{\omega}_1,\ldots,\bm{\omega}_d\}=\{\tau({\bm{u}}_1),\ldots,\tau({\bm{u}}_d)\}$ forms an orthonormal basis of $\R_+^d$. In this new basis, the random vector $\bm{X}$ may be decomposed as
\begin{equation*}
    \bm{X} = \bigoplus_{j=1}^d (V_j \odot \bm{\omega}_j) = \tau\left(\sum_{j=1}^d V_j\bm{u}_j\right),
\end{equation*}
where
\begin{equation*}
    V_j = \left\langle \bm{X},\bm{\omega}_j\right\rangle_\tau = \left\langle \tau^{-1}(\bm{X}),\bm{u}_j\right\rangle, \qquad (j=1,\ldots,d).
\end{equation*}
Rank-$p$ reconstructions of $\bm{X}$ are obtained by the truncated expansion
\begin{equation*}
    \hat{\bm{X}}^{[p]} = \bigoplus_{j=1}^d (V_j \odot \bm{\omega}_j) = \tau\left(\sum_{j=1}^d V_j\bm{u}_j\right), \qquad (p=1,\ldots,d).
\end{equation*}
The process follows analogously for PCA based on an independent sample $\bm{X}_1,\ldots,\bm{X}_n$ and the empirical TPDM $\hat{\Sigma}$.

The elements of the $\R^d$-valued random vector $\bm{V}=(V_1,\ldots,V_d)$ are called the extremal principal components of $\bm{X}$. The random vector $\bm{V}$ is MRV with the same tail index as $\bm{X}$, but its angular measure $H_V$ lives on the entire unit sphere, not just its restriction to the positive orthant. Although the dimension of $\bm{V}$ is the same as $\bm{X}$, the crucial difference is that its components are ordered according to their contribution to the extreme behaviour of $\bm{X}$. Proposition 6 in @cooleyDecompositionsDependenceHighdimensional2019 states that
\begin{equation*}
    \mathrm{scale}(|V_i|) = \lambda_i^{1/\alpha}, \qquad (i=1,\ldots,d),
\end{equation*}
and therefore $\mathrm{scale}(|V_1|)\geq \ldots \geq \mathrm{scale}(|V_d|)\geq 0$. The $i$th eigenvector $\bm{\omega}_i$ represents the direction of maximum scale after accounting for information contained in $\bm{\omega}_1,\ldots,\bm{\omega}_{i-1}$; sequential examination of the eigenvectors provides insight into the extremal dependence structure. 

#### Applications

The PCA method of @cooleyDecompositionsDependenceHighdimensional2019 has been applied for exploratory purposes in the context of climatology [@jiangPrincipalComponentAnalysis2020; @szemkusSpatialPatternsIndices2024], finance [@cooleyDecompositionsDependenceHighdimensional2019] and sport [@russellAnalyzingDependenceMatrices2018]. 

@jiangPrincipalComponentAnalysis2020 analyse the extremal behaviour of precipitation across the United States. They discover an increasing temporal trend in the coefficient of the first principal component $V_1$, and relate the eigenvectors to the El-Niño Southern Oscillation (ENSO), a cyclical phenomenon that is known to be a key climatological driver. They find that low-rank reconstructions of Hurricane Floyd broadly capture the event's large-scale structure, but a large number of eigenvectors are needed to recreate more localised features. The spatial extent of the study region and relatively localised behaviour of extreme behaviour leads them to consider a 'pairwise-thresholded' estimator of the TPDM instead of the usual estimator \eqref{eq-empirical-tpdm} thresholded on the norm of entire vector. This alternative estimator is given by
\begin{equation*}
    \tilde{\Sigma}=(\tilde{\sigma}_{ij}), \qquad \tilde{\sigma}_{ij} = \frac{2}{k} \sum_{l=1}^n \Theta_{li}\Theta_{lj}\ind\{R_l^{ij} > R_{(k+1)}^{ij}\},
\end{equation*}
where $R_l^{ij} = \|(X_{li},X_{lj})\|$ and $R_{(k+1)}^{ij}$ is the $(k+1)$th upper order statistic of $\{R_l^{ij}:l=1,\ldots,n\}$. The estimator $\tilde{\Sigma}$ is not positive semi-definite, so the PCA analysis is instead conducted using the nearest positive definite matrix in Frobenius norm. The ramifications of this ad-hoc step, in terms of the estimator's theoretical properties and practical performance, are not studied.

@szemkusSpatialPatternsIndices2024 devise an extension of the TPDM, called the cross-TPDM, to study the joint extremal behaviour between two sets of variables. They analyse two meteorological variables -- daily maximum temperature and a measure of accumulated precipitation deficit -- to describe the dynamics of summer heatwaves in Europe. The cross-TPDM is the analogue of the cross-covariance matrix. Letting $\bm{X}=(X_1,\ldots,X_p)\in\mathrm{RV}_+^p(2)$ and $\bm{Y}=(Y_1,\ldots,Y_q)\in\mathrm{RV}_+^q(2)$, the cross-TPDM is defined as the $p\times q$ matrix with entries
\begin{equation*}
    \sigma_{ij}^{XY} = \int_{\mathbb{S}_+^{p+q-1}} \theta_i^X\theta_j^Y \,\dee H(\bm{\theta}),
\end{equation*}
where $H$ is the angular measure of $(\bm{X},\bm{Y})=(X_1,\ldots,X_p,Y_1,\ldots,Y_q)\in\mathrm{RV}_+^{p+q}(2)$ and the variable of integration is indexed as $\bm{\theta}=(\theta_1^X,\ldots,\theta_p^X,\theta_1^Y,\ldots,\theta_q^Y)$. (This definition could be extended to cater for an arbitrary tail index by introducing the usual $\alpha/2$ exponents in the integrand.) In the context of their climatological study, the entry $\sigma_{ij}^{XY}$ represents the strength of extremal dependence between the maximum temperature at location $i$ and the precipitation deficit at location $j$. The singular-value decomposition of the cross-TPDM is used to analyse the dynamics of compound extreme events. They devise extremal pattern indices to quantify whether particular patterns of interest -- those signified by the singular vectors of the cross-TPDM -- are highly pronounced.

A more unusual application of the TPDM is found in @russellAnalyzingDependenceMatrices2018. Their study characterises the difference in performance between typical and elite-level National Football League (NFL) performers across the Scouting Combine event. The Combine comprises six physical tests: Bench Press, Vertical Jump, Broad Jump, 40-yard Sprint, the Shuttle Drill, and the Three Cone Drill. The tests afford teams the opportunity to gauge the athletic ability of prospective players, thereby influencing whether (or how highly) they are drafted for the upcoming season. @russellAnalyzingDependenceMatrices2018 explore how strongly player performance correlates across these tests. Intuitively, if two events exhibit strong association, then they may be measuring the same underlying skills (speed, strength, agility etc.). After standardising player performance to account for differences in playing position, they find significant differences between the bulk dependence structure and the extremal dependence structure. In particular, the leading eigenvectors of the covariance matrix reveal that the Combine events cluster into three distinct groups, corresponding to strength, agility, and explosiveness. On the other hand, the TPDM eigenvectors produce only two such groups: power and agility. This reveals differences between non-elite and elite performers; recommendations regarding the composition of the Combine events are made accordingly. 

@rohrbeckSimulatingFloodEvent2023 move beyond the use of the extremal PCA for purely exploratory purposes and demonstrate how it be used to generate synthetic extreme events. Hazard event sets are widely used in catastrophe modelling to assess exposure to extreme events. Imagine an insurance company insures against damage to a portfolio of properties, and wishes to gauge its exposure to claims caused by flooding. Given (i) the spatial locations of these properties, (ii) other relevant characteristics such as property value and construction standard, and (iii) a set of simulated flood events, one can derive a probabilistic loss distribution. If the exposure is unacceptably high, they might adjust their underwriting strategy or purchase reinsurance. @rohrbeckSimulatingFloodEvent2023 show how to generate approximate samples from $H$, even in high-dimensions, by leveraging the PCA method of @cooleyDecompositionsDependenceHighdimensional2019. Their generative framework hinges on the fact that the leading components of $\bm{V}$ account for the greatest proportion of extremal behaviour of $\bm{X}$. Thus, efforts may be concentrated towards modelling the dependence structure of the sub-vector $(V_1,\ldots,V_p)$ for some appropriately chosen $p<d$. To achieve this, they use a spherical kernel density estimate to flexibly model the dependence between $V_1,\ldots,V_p$ and additionally between $(V_1,\ldots,V_p)$ and $(V_{p+1},\ldots,V_d)$. The dependence structure of $(V_{p+1},\ldots,V_d)$ is simply modelled by a nearest-neighbours approach. The number of components $p$ entering into the complex model is selected by a leave-one-out cross validation procedure. This involves discarding an extreme observation $\bm{x}_{(i)}$, generating a large number of samples $\tilde{\bm{x}}_1^{[p]},\ldots,\tilde{\bm{x}}_N^{[p]}$ for a range of values $p$, and then assessing whether any of the generated samples resemble the discarded event using
\begin{equation*}
    D_i(p) = \min_{l=1,\ldots,N} \varrho \left(\bm{x}_{(i)},\tilde{\bm{x}}_{l}^{[p]}\right),
\end{equation*}
where $\varrho(\cdot,\cdot)$ is an angular dissimilarity measure. After repeating for all extreme events $i=1\ldots,k$, one chooses the optimal $p$ as that which minimises the average error
\begin{equation*}
    \bar{D}(p) = \frac{1}{k}\sum_{i=1}^k D_i(p).
\end{equation*}
Their approach is illustrated using historical river flow data across $d=45$ gauges in northern England and southern Scotland. They select $p=7$ and find reasonable agreement between the observed river flow extreme events and the synthetic ones generated by their algorithm, e.g. by examining QQ-plots comparing the observed and sampled distributions of $\max_{j\in\mathcal{G}}X_j$ or $\|(X_i:i\in\mathcal{G})\|$ for selected groups of gauges $\mathcal{G}\subset \{1,\ldots,d\}$.

\textit{Add more critical comments, especially about asymptotic independence when using large study regions or with localised extremes, e.g. rainfall. Or leave this to the 'bias' section?} 

### Clustering into asymptotically dependent groups

Within multivariate extremes, the umbrella term `clustering` can refer to a multitude of tasks. To avoid confusion, we briefly describe these and clarify which type we are referring to. 

- **Prototypical events.** Assume that the angular measure concentrates at/near a small number of points in $\mathbb{S}_{+}^{d-1}$. Then one might wish to identify cluster centres $\bm{w}_1,\ldots\bm{w}_K$ minimising some objective function of the form
\begin{equation}\label{eq-clustering-objective-function}
    \mathbb{E}_{\bm{\Theta}\sim H}\left[\min_{l=1,\ldots,K}\varrho(\bm{\Theta},\bm{w}_l)\right],
\end{equation}
where $\varrho:\mathbb{S}_+^{d-1}\times \mathbb{S}_+^{d-1}\to[0,1]$ is some distance/dissimilarity function. The cluster centres can be interpreted as the directions of prototypical extremes events. See @chautruDimensionReductionMultivariate2015, @janssenKmeansClusteringExtremes2020 and @medinaSpectralLearningMultivariate2021 for further details.
- **Identification of concomitant extremes.** Suppose that angular measure is supported on a set of $K\ll 2^{d-1}$ subspaces (faces) of the simplex $C_{\beta_1},\ldots,C_{\beta_K}$, where $\beta_1,\ldots,\beta_K\in\mathcal{P}(\{1,\ldots,d\})\setminus\emptyset$ and
\begin{equation*}
    C_\beta = \{\bm{\theta}\in\mathbb{S}_+^{d-1}:\theta_i>0 \iff i\in\beta\}.
\end{equation*}
Only those groups (`clusters') of components indexed by $\beta_1,\ldots,\beta_K$ may be simultaneously extreme. Identification of the support of the angular measure is notoriously challenging because the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ lie (almost surely) in the interior of the simplex. @goixSparseRepresentationMultivariate2017 and @simpsonDeterminingDependenceStructure2020 identify clusters according to whether observations fall within appropriately sized rectangular/conic neighbourhoods of the corresponding axis in $\R_+^d$. @meyerDetectionExtremalDirections2020 take a different approach, whereby the angular component is defined with respect to the Euclidean projection [@liuEfficientEuclideanProjections2009] rather than usual projection based on self-normalisation. The geometry of the projection is such that the projected data lie on subfaces of the simplex. The price paid is that the limiting conditional distribution of the angles is related to, but not identical to, the angular measure.
- **Partitioning into AD/AI groups components.** This notion of clustering is related to the previous type. We assume that the variables $X_1,\ldots,X_d$ can be partitioned into $K$ clusters, such that $X_i$ and $X_j$ are asymptotically dependent if and only if they belong to the same cluster. In other words, there exists $2\leq K \leq d$ and a partition $\beta_1,\ldots,\beta_K$ of $\{1,\ldots,d\}$ such that the angular measure is supported on $C_{\beta_1},\ldots,C_{\beta_K}$ or lower-dimensional subspaces thereof, i.e.
\begin{equation*}
    H\left(\bigcup_{l=1}^K \bigcup_{\beta'_l\subseteq \beta_l} C_{\beta'_l}\right) = m.
\end{equation*}
The task of modelling the dependence structure of $\bm{X}$ can be divided into lower-dimensional sub-problems involving the random sub-vectors $\bm{X}_{\beta_1},\ldots,\bm{X}_{\beta_K}$. If $K=d$, then all variables are asymptotically independent. The underlying hypothesis is very strong and unlikely to hold in practice. Nevertheless, it is often a useful simplifying modelling assumption. @bernardClusteringMaximaSpatial2013 propose grouping components using the $k$-medoids algorithm [@kaufmanFindingGroupsData1990] with a dissimilarity matrix populated with pairwise measures of tail dependence, similar to $\chi_{ij}$ and $\sigma_{ij}$. The approaches of @fomichovSphericalClusteringDetection2023 and @richardsModernExtremeValue2024 involve the TPDM; these are reviewed in greater detail below.

#### @fomichovSphericalClusteringDetection2023

@fomichovSphericalClusteringDetection2023 show that the latter kind of clustering may be performed using the framework of the first kind. They provide a link between the principal eigenvector $\bm{u}_1$ of the TPDM and the minimiser of the objective \eqref{eq-clustering-objective-function} with quadratic cost $\varrho(\bm{\theta},\bm{\phi})=\left\langle\bm{\theta},\bm{\phi}\right\rangle^2$ and $K=1$:
\begin{equation*}
    \min_{\bm{\theta}\in\mathbb{S}_{+(2)}^{d-1}} \mathbb{E}_{\bm{\Theta}\sim H}\left[\varrho(\bm{\Theta},\bm{\theta})\right] = \mathbb{E}_{\bm{\Theta}\sim H}\left[\varrho(\bm{\Theta},\bm{u}_1)\right].
\end{equation*}
Note that $\bm{u}_1\in\mathbb{S}_{+(2)}^{d-1}$ is assumed to be suitably normalised with all entries being non-negative; the Perron-Frobenius theorem guarantees this is possible. This result informs an iterative clustering procedure called spherical $k$-principal-components. Consider a set of extremal angles $\bm{\theta}_{(1)},\ldots,\bm{\theta}_{(k)}\in\mathbb{S}_{+(2)}^{d-1}$ and current centroids $\hat{\bm{w}}_{1},\ldots,\hat{\bm{w}}_{K}\in\mathbb{S}_{+(2)}^{d-1}$. A single iteration of their procedure yields new centroids $\hat{\bm{w}}_{1}^\star,\ldots,\hat{\bm{w}}_{K}^\star\in\mathbb{S}_{+(2)}^{d-1}$ given by the respective principal eigenvectors of
\begin{equation*}
    \hat{\Sigma}^{[i]} = \sum_{l=1}^k \bm{\theta}_{(l)}\bm{\theta}_{(l)}^T\ind\{\argmin_{j=1,\ldots,K}\varrho(\bm{\theta}_{(l)},\bm{w}_j)=i\}, \qquad (i=1,\ldots,K).
\end{equation*}
The matrix $\hat{\Sigma}^{[i]}$ represents the empirical TPDM (up to some multiplicative constant) based on the nearest neighbours of the $i$th centroid. @fomichovSphericalClusteringDetection2023 prove that, under certain conditions, the limiting centroids lie in a neighbourhood of the faces of interest $C_{\beta_1},\ldots,C_{\beta_K}$. Thresholding the centroid vectors yields the final partition $\beta_1,\ldots,\beta_K$.

#### @richardsModernExtremeValue2024

@richardsModernExtremeValue2024 apply hierarchical clustering using the empirical TPDM as the underlying similarity matrix. The clustering method constitutes a minor aspect of their submission to the EVA (2023) Data Challenge. Few methodological details are provided, so the following explanation constitutes our interpretation of their method, drawing on Figure 4 in @richardsModernExtremeValue2024 and the accompanying code made available at \url{https://github.com/matheusguerrero/yalla}. Define the dissimilarity between $X_i$ and $X_j$ as $\varrho_{ij} = 1 - \sigma_{ij}$. This satisfies the properties of a dissimilarity measure (CITE: A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES):
\begin{equation*}
    \varrho_{ij} \geq 0, \qquad \varrho_{ii} = 0, \qquad \varrho_{ij}=\varrho_{ji}.
\end{equation*}
The $d\times d$ dissimilarity matrix $\mathcal{D}=1-\Sigma=(\varrho_{ij})$ can be fed into standard hierarchical clustering algorithms. Agglomerative hierarchical clustering initially assigns each variable belongs to its own cluster, i.e. $\beta_i=\{i\}$ for $i=1,\ldots,d$. The algorithm proceeds iteratively, repeatedly joining together the two closest clusters until some stopping criterion is satisfied. Under complete-linkage clustering, the distance between clusters $\beta\neq\beta'$ is given by $\max\{\varrho_{ij} : i\in\beta,j\in\beta'\}$. The merging process may be stopped when there is a sufficiently small number of clusters or when the clusters are sufficiently separated.

### Parametric model fitting

Inference for max-linear models (i.e. estimating the parameter matrix $A$) is a challenging task. The lack of an angular density function precludes the use of standard maximum likelihood procedures. @einmahlContinuousUpdatingWeighted2018 propose a procedure that minimises a weighted least-squares distance to some initial (non-parametric) estimator. Their procedure becomes computationally intensive when $q$ is large. @janssenKmeansClusteringExtremes2020 and @medinaSpectralLearningMultivariate2021 cluster the angles of extreme observations and identify the normalised columns of $A$ with the $q$ cluster centres. The minimum-distance and clustering approaches assume $q$ is fixed; @kirilioukHypothesisTestingTail2020 present a hypothesis test to assist with choosing $q$. *Alternative strategies based on TPDM etc. are explained later; do I move this section to there?*

@fixSimultaneousAutoregressiveModels2021 consider the extremal behaviour of a spatial process $\{\bm{X}(\bm{s}):\bm{s}\in\R^2\}$ at fixed sites $\bm{s}_1,\ldots,\bm{s}_d\in\R^2$ by modelling $\bm{X}=(\bm{X}(\bm{s}_i) : i=1\ldots,d)\in\mathrm{RV}_+^d(2)$ as
\begin{equation}\label{eq-sar-model}
    \bm{X} = (I - \rho W)^{-1}\otimes\bm{Z}.
\end{equation}
This is called the extremal spatial auto-regressive (SAR) model. The $d\times d$ matrix $W$ contains the (known) pairwise spatial distances and $\rho\in(0,1/4)$ is a spatial dependence parameter. The extremal SAR model is a special case of the max-linear model \eqref{eq-max-linear-X-cooley} with $A=A(\rho)=(I-\rho W)^{-1}$. They propose estimating the model parameter $\rho$ by minimising the discrepancy between the empirical TPDM and the theoretical TPDM $\Sigma(\rho):=A(\rho)A(\rho)^T$, that is
\begin{equation}\label{eq-sar-rho-estimator}
    \hat{\rho} = \argmin_{\rho\in(0,1/4)}\|\hat{\Sigma} - \Sigma(\rho)\|_F^2.
\end{equation}
In fact, $\hat{\Sigma}$ is replaced with a bias-corrected version of the empirical TPDM; this will be discussed in Section XX.

@kirilioukEstimatingProbabilitiesMultivariate2022 consider the more general problem of modelling arbitrary max-linear random vectors $\bm{X}=A\times_{\max}\bm{Z}\in \mathrm{RV}_+^d(2)$. In a similar spirit to @fixSimultaneousAutoregressiveModels2021, they propose estimating $A$ so as to enforce conformity between the empirical and model TPDMs. This means that the estimate of $A$ belongs to the set
\begin{equation*}
    \mathcal{CP}(\hat{\Sigma}) := \left\lbrace \hat{A}\in\R_+^{d\times q} : q\geq 1, \,\hat{\Sigma}=\hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T\right\rbrace.
\end{equation*}
Choosing $\hat{A}\in\mathcal{CP}(\hat{\Sigma})$ guarantees that the pairwise dependencies of the fitted model match those exhibited by the data. The set $\mathcal{CP}(\hat{\Sigma})$ is in direct correspondence to the set of completely positive (CP) factors of $\hat{\Sigma}$; we call $\hat{A}\in\mathcal{CP}(\hat{\Sigma})$ a CP-estimate of $A$. The naive estimate \eqref{eq-empirical-A} belongs to this class, but @kirilioukEstimatingProbabilitiesMultivariate2022 provide an algorithm for efficiently obtaining further estimates $\hat{A}\in \R_+^{d\times d}\cap \mathcal{CP}(\hat{\Sigma})$. 

@fixSimultaneousAutoregressiveModels2021 and @kirilioukEstimatingProbabilitiesMultivariate2022 evaluate the practical performance of their estimators by computing tail event probabilities in a series of simulated/real-world scenarios. *More details here, when I've written up formulae for failure events.*

### Miscellaneous: time series and extremal graphical models

[@mhatreTransformedLinearModelsTime2021; @gongPartialTailCorrelationCoefficient2024; @leePartialTailCorrelation2023].

## Bias in the empirical TPDM in weak-dependence scenarios}

Section XX reviewed the asymptotic properties of the empirical TPDM. We recall in particular that it is asymptotically unbiased, meaning $\mathbb{E}[\hat{\Sigma}] \to \Sigma$ as $n\to\infty$. The associated rate of convergence is $\mathcal{O}(k^{-1/2})$, where $k$ represents the number of extreme observations and satisfies the rate conditions \eqref{eq-k-rate-conditions}. For example, choosing $k(n)=\sqrt{n}$ yields a convergence rate of $\mathcal{O}(n^{-1/4})$. In practical settings the number of extreme events $k$ is normally small, both in relative (by definition) and absolute terms. For example, commonly available climate records typically span approximately 50 years \parencite{boulaguiem_modeling_2022}. A study of temperature extremes might then be based on, say, $n\approx 50 \times 100 = 5,000$ daily observations recorded in the summer months over this time span. Working with small effective sample sizes means it is critical to understand the non-asymptotic, finite-sample performance of the empirical TPDM. 

### Bias in threshold-based estimators

At finite levels, the empirical TPDM exhibits an upwards bias in weak dependence scenarios [@cooleyDecompositionsDependenceHighdimensional2019; @fixSimultaneousAutoregressiveModels2021; @mhatreTransformedLinearModelsTime2021]. This is true more generally of threshold-based estimators in multivariate extremes [@huserLikelihoodEstimatorsMultivariate2016]. They conduct simulation studies with $d=2$ and $n=10^4$ examining the performance of various estimators of $\gamma$, the dependence parameter of the symmetric logistic model. The results show that block-maxima based estimators have a small bias but very high variability. On the other hand, each of the threshold-based estimators $\hat{\gamma}$ tend to overestimate the dependence strength, that is $\mathrm{Bias}(\hat{\gamma}) = \mathbb{E}[\hat{\gamma}] - \gamma < 0$. Moreover, the discrepancy increases as dependence weakens ($\gamma\to 1$). 

The empirical TPDM suffers from the same issue when dependence is weak. This can be summarised as
\begin{equation}\label{eq-empirical-tpdm-bias}
    \sigma_{ij} \ll 1 \implies \mathrm{Bias}(\hat{\sigma}_{ij}) = \mathbb{E}[\hat{\sigma}_{ij}] - \sigma_{ij} > 0.
\end{equation}
Note that overestimating the dependence strength now corresponds to a positive bias, so the inequality is reversed. 

### Simulation experiments

See @fig-parametric-chi-tpdm-small-n.


```{r background-make-fig-parametric-chi-tpdm-small-n}
#| label: fig-parametric-chi-tpdm-small-n
#| fig-cap: "True dependence strengths for the symmetric logistic (left) and Hüsler-Reiss (right) models, measured using the tail dependence coefficient (red line) and TPDM (blue line). The shaded regions represent the minimum/maximum values of empirical estimates over 10 repeated simulations using bivariate samples of size $n=5\\times 10^3$."
#| fig-scap: "Bias in estimation of $\\sigma$ for symmetric logistic and Hüsler-Reiss models."
#| fig-height: 3.5

data <- readRDS(file.path("scripts", "background", "results", "parametric_chi_tpdm_empirical_smalln.RDS")) 

p1 <- ggplot() + 
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) +
  geom_function(aes(colour = "chi"), fun = sl_chi, xlim = c(0, 1)) + 
  geom_function(aes(colour = "tpdm"), fun = sl_tpdm, xlim = c(0, 1)) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 5)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(gamma), 
       y = "Dependence strength", 
       title = "Symmetric logistic") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10))  

p2 <- ggplot() + 
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) + 
  geom_function(aes(colour = "chi"), fun = hr_chi, xlim = c(0, 3.5)) + 
  geom_function(aes(colour = "tpdm"), fun = hr_tpdm, xlim = c(0, 3.5)) + 
  scale_x_continuous(limits = c(0, 3.5), expand = c(0, 0), breaks = breaks_extended(n = 4)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(lambda), 
       y = "Dependence strength", 
       title = "Hüsler-Reiss") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10)) 

ggarrange(p1, p2, ncol = 2, common.legend = TRUE) 
``` 


\subsection{Existing approaches to bias-correction for the TPDM}

Estimation error in the empirical TPDM was first studied by @cooleyDecompositionsDependenceHighdimensional2019. In the Supplementary Material, they assess the accuracy of the eigenvalues/eigenvectors of the empirical TPDM. Their example is based on a Brown-Resnick process, for which the true TPDM is known (@exm-husler-reiss-tpdm). They find that the leading eigenvalue is overestimated ($\hat{\lambda}_1 > \lambda_1$) and subsequent eigenvalues are underestimated ($\hat{\lambda}_j < \lambda_j$ for $j\geq 2$). The bias reduces when the sample size and radial threshold are increased. In the non-extreme setting, the sample covariance matrix has the same deficiency, especially when the sample size and dimension are comparable in magnitude [@mestreImprovedEstimationEigenvalues2008]. Poor spectrum estimation can have important consequences in a downstream analysis, such as deciding how many principal components are retained in PCA. @cooleyDecompositionsDependenceHighdimensional2019 do not propose any solutions to improve TPDM estimation.

The bias issue \eqref{eq-empirical-tpdm-bias} is addressed more directly by @mhatreTransformedLinearModelsTime2021. In their Supplementary Material, they conduct simulation studies to examine the performance of the empirical TPDF, the time series analogue of the TPDM (see Section XX). They show that $\sigma(h)$ exhibits a positive bias, especially at higher lags where the theoretical TPDM should vanish to zero. Their bias-corrected TPDF estimator works by subtracting the mean from the time series in pre-processing. The rationale for their estimator is described in terms of the position of extreme points in a lag plot (i.e. a scatter plot of $(X_t, X_{t+h})$ for some fixed lag $h$). Subtracting the mean has little effect on points near the middle of this plot, but points close to the coordinate axes are driven even closer. 

The first bias-correction estimation procedure for the TPDM is found in @fixSimultaneousAutoregressiveModels2021. Recall from Section XX that they use the empirical TPDM to estimate the spatial dependence parameter $\rho$ of the extremal SAR model \eqref{eq-sar-model}. When the spatial extent of the study domain is large compared to that of the modelled phenomenon, their estimation procedure \eqref{eq-sar-rho-estimator} is liable to overestimate $\rho$. This is because the empirical TPDM fails to capture the weak dependence between distant pairs of sites. Their bias-correction procedure is founded on the assumption that the pairwise asymptotic dependence strength vanishes to zero as the distance between two sites increases. Consider a spatial process $\{X(\bm{s}) : s\in \R^2\}$ and fixed locations $\bm{s}_1,\ldots,\bm{s}_d\in \R^2$. Let $X_i=X(\bm{s}_i)$ represent the process at site $i$ and $h_{ij}$ the spatial distance between $\bm{s}_i$ and $\bm{s}_j$. Treating the empirical TPDM entries as functions of distance, they model the relationship between the empirical TPDM and spatial distance via
\begin{equation*}
    \hat{\sigma}(h) = \beta_0 \exp(-\beta_1 h) + \beta_2.
\end{equation*}
The parameters $\beta_0,\beta_1,\beta_2$ are estimated from the observed data $\{(\hat{\sigma}_{ij},h_{ij}):1\leq i<j\leq d\}$ by non-linear least squares estimation, e.g. using \texttt{nls()}. Since $\hat{\sigma}(h)\to\beta_2$ as $h\to\infty$, the horizontal asymptote $\hat{\beta}_2$ of the fitted model is used as a proxy for the bias at large distances. This determines the amount of shrinkage that should be applied to the off-diagonal entries, yielding the final estimator
\begin{equation}\label{eq-fix-bias-corrected-tpdm}
    \tilde{\Sigma}=(\tilde{\sigma}_{ij}), \qquad \tilde{\sigma}_{ij} = \begin{cases}
        \hat{\sigma}_{ij}, & i=j, \\
        (\hat{\sigma}_{ij} - \hat{\beta}_2)_+, & i\neq j.
    \end{cases}
\end{equation}
Estimates of the diagonal entries are found to be unbiased -- and their values are known if the margins are standardised -- so they are unaltered. @fixSimultaneousAutoregressiveModels2021 find that $\tilde{\Sigma}$ is effective in reducing the bias in estimation of $\rho$. Its performance more broadly as an estimator for $\Sigma$ is not studied. In any case, their procedure is only applicable in settings where there is a notion of distance between variables. The estimator \eqref{eq-fix-bias-corrected-tpdm} results from element-wise application of the soft-thresholding operator (with shrinkage parameter $\hat{\beta}_2$) to the empirical TPDM \parencite{rothman_generalized_2009}. This connection will be developed further in Chapter XX, where we propose alternative bias-corrected TPDM estimators.

