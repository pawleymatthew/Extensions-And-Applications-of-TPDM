# Literature review

```{r background-load-packages}
#| include: false
library(tidyverse)
library(tidyr)
library(magrittr)
library(scales)
library(ggh4x)
library(ggpubr)
library(colorspace)
library(kableExtra)
library(reshape2)
library(mev)
library(GGally)
library(ggforce)

options(dplyr.summarise.inform = FALSE)
options(knitr.kable.NA = "")
```

```{r background-source-functions}
#| include: false
sapply(list.files(path = "R/general", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
sapply(list.files(path = "R/background", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

## Univariate extreme value theory

### Block maxima and the generalised extreme value (GEV) distribution}

Let $X_1,X_2,\ldots$ be a sequence of independent, identically distributed, continuous random variables with distribution function $F$. For $n\geq 1$, define the random variable
\begin{equation}
    M_n := \max (X_1,\ldots, X_n) = \bigvee_{i=1}^n X_i.
\end{equation}
The exact distribution of $M_n$ is given by
\begin{equation*}
    \mathbb{P}(M_n \leq x) = \mathbb{P}(X_1\leq x,\ldots X_n \leq x) = \prod_{i=1}^n \mathbb{P}(X_i \leq x) = F^n(x), \qquad (x\in\R).
\end{equation*}
This result is not particularly useful in practice, where $F$ is typically unknown. Instead, we study the limiting behaviour of $F^n$ as $n\to\infty$. Clearly the asymptotic distribution of $M_n$ is degenerate, since $M_n \overset{p}{\to} x_F:=\sup\{x:F(x)<1\}$, the (possibly infinite) upper end-point of $F$. However, the Extremal Types Theorem states that, after suitable rescaling, there are three classes of non-degenerate asymptotic distribution (CITE).

:::{#thm-extremal-types}
Suppose there exist real sequences $\{a_n > 0\}$ and $\{b_n\in\R\}$ and a non-degenerate distribution function $G$ such that
\begin{equation}\label{eq-extremal-types}
    \mathbb{P}\left(\frac{M_n - b_n}{a_n} \leq x \right) \overset{d}{\to} G(x), \qquad (n\to\infty).
\end{equation}
Then $G$ belongs to one of three parametric families: Gumbel, Fréchet or negative Weibull.
:::

When \eqref{eq-extremal-types} holds, we say that $F$ lies in the maximum domain of attraction (MDA) of $G$. The three families are unified by the Generalised Extreme Value (GEV) distribution. Its distribution function is
\begin{equation}\label{eq-gev}
    G(x) = \exp\left\lbrace - \left[ 1 + \xi \left(\frac{x-\mu}{\sigma}\right)\right]_+^{-1/\xi} \right\rbrace,
\end{equation}
where $[x]_+:=\max(0,x)$ denote the positive part of $x$. The parameters $\mu\in\R$, $\sigma >0$ and $\xi\in\R$ are called the location, scale, and shape, respectively. The sign of the shape parameter determines the sub-class that $G$ belongs to: $\xi>0$ corresponds to the heavy-tailed Fréchet class, $\xi=0$ (with \eqref{eq-extremal-types} interpreted as $\xi\to 0$) corresponds the exponential-tailed Gumbel class, and $\xi<0$ the negative Weibull class, which has a finite upper limit.

The GEV distribution is used to model the upper tail of $X$ via the block maxima approach (CITE). Let $x_1,\ldots,x_n$ denote independent observations of $X_1,\ldots,X_n$. The data are partitioned into finite blocks of size $m$. Provided $m$ is sufficiently large, the maximum observation in each block is approximately GEV distributed by @thm-extremal-types. Once the block-wise maxima have been extracted, estimates of the GEV parameters may be obtained, e.g. by maximum likelihood inference. The performance of the fitted model is sensitive to the choice of block size. Selection of the tuning parameter $m$ requires managing a bias-variance trade-off. If the blocks are too small, then the underlying asymptotic approximation may not be valid and the maxima may not be representative as extreme events, biasing the estimates. Taking larger blocks reduces the amount of data available for inference, resulting in noisier estimation of the GEV parameter estimates.

### Threshold exceedances and the generalised Pareto distribution (GPD)

The block maxima procedure is considered inefficient, because it fails to exploit all the available information. Each block is summarised by a (single) maximum value, even if it contains other 'extreme' events that might be informative for the tail. The intimately related peaks-over-threshold method makes better use of the available data. If $X$ is in the maximum domain of attraction of a $\mathrm{GEV}(\mu,\sigma,\xi)$ distribution, then
\begin{equation}\label{eq-gpd}
    \lim_{u\to\infty} \mathbb{P}(X-u>x \mid X>u) = \left[1+\frac{\xi x}{\tilde{\sigma}}\right]_+^{-1/\xi}, \qquad (x>0),
\end{equation}
where $\tilde{\sigma}=\sigma + \xi(u-\mu)$ (CITE). The limiting conditional distribution is called the generalised Pareto distribution (GPD). The GPD describes the distribution of excesses over a high threshold. Given observations $x_1,\ldots,x_n$, the peaks-over-threshold method assumes that exceedances of some pre-specified high threshold $u>0$ are approximately GPD distributed. Maximum likelihood or Bayesian inference procedures may be used to estimate the GPD parameters $\bar{\sigma},\xi$. Threshold selection is subject to similar considerations as for the block size. Picking a low threshold risks model misspecification, causing bias in the fitted model. Choosing a high threshold directly reduces the number of threshold exceedances, increasing the uncertainty in the parameter estimates. Various diagnostics and procedures have been proposed to aid with this choice. Many approaches rely on inspecting diagnostic plots, such as mean residual life (MRL) plots (CITE) and parameter stability plots (CITE). Automated selection procedures aim to remove subjectivity by optimising with respect to some criterion. These include change-point methods (CITE Wadsworth 2016), cross-validation in a Bayesian framework (CITE Northrop et al. 2017), and minimising expected quantile discrepancies (CITE Murphy and Tawn 2024). 

### Non-stationary extremes

The block-maxima and peaks-over-threshold methods as presented above assume that the data are stationary over the observation period. In environmental applications, climate change threatens the validity of this assumption, with changes in the frequency and intensity of extreme weather events (CITE). Non-stationary models accommodate temporal dependence by allowing parameters to vary over time or in relation to covariates. For example, CITE Vanem 2015 incorporate trends into the GEV location and scale parameters by specifying
\begin{equation*}
\mu(t)=\mu_0 + \mu_1 t,\qquad \sigma(t) = \exp(\sigma_0 + \sigma_1 t).
\end{equation*}
If the parameters $\mu_1$ and $\sigma_1$ are significantly different from zero, it suggests the data exhibit non-stationarity. In principle the shape parameter may be extended analogously. Often the shape parameter is assumed constant because is notoriously difficult to estimate accurately and results (quantiles, return periods, etc.) are very sensitive to changes in its sign. *CITE further papers or a review?* 

## Multivariate extreme value theory

Multivariate extreme value theory (MEVT) generalises the study of extreme events from univariate to multivariate settings. Understanding the joint tail behaviour of several variables is critical in various fields. In environmental science, practitioners are tasked with assessing the risk of compound extreme events involving several variables. For example, the impact of drought -- defined by the IPCC (CITE) as a prolonged period of low precipitation -- is exacerbated by high temperatures. Similarly, extreme rainfall occurring simultaneously across multiple locations may lead to a widespread flood event. In finance, investors seek to diversify their portfolio to mitigate against the risk of simultaneous extreme losses across multiple assets. Each of these examples calls for a statistical analysis of the joint tail distribution of some random vector.

### Componentwise maxima

Consider a $d$-dimensional random vector $\bm{X}=(X_1,\ldots,X_d)$ with unknown joint distribution function $F$, meaning
\begin{equation*}
    F(\bm{x}) := \mathbb{P}(X_1\leq x_1, \ldots, X_d \leq x_d),
\end{equation*}
for any $\bm{x}=(x_1,\ldots,x_d)\in\R^d$. Let $\bm{X}_1,\bm{X}_2,\ldots$ be a sequence of independent copies of $\bm{X}$. The notion of `extremes' or a 'maximum' becomes subjective in the multivariate setting, because $\R^d$ is not an ordered set. One possibility is to define the maximum component-wise as
\begin{equation*}
    \bm{M}_n := \left( \bigvee_{i=1}^n X_{i1}, \ldots, \bigvee_{i=1}^n X_{id} \right). 
\end{equation*}
We say that $F$ lies in the multivariate MDA of a non-degenerate distribution $G$ if there exist $\R^d$-valued sequences $\{\bm{a}_n > \bm{0}\}$ and $\{\bm{b}_n \in \R^d\}$ such that
\begin{equation}\label{eq-multivariate-mda}
    \mathbb{P}\left(\frac{\bm{M}_n - \bm{b}_n}{\bm{a}_n} \leq \bm{x} \right) \overset{d}{\to} G(\bm{x}), \qquad (n\to\infty).
\end{equation}
Applying @thm-extremal-types to the marginal components reveals that the margins of $G$ follow a univariate GEV distribution. The crucial difference to the univariate setting is that now the limit (joint) distribution $G$ does *not* admit a parametric representation. The inherently challenging nature of MEVT largely stem from this fact. The problem of estimating/modelling $G$ is usually split into two (sequential) steps. First, one models the margins to describe the extreme behaviour of each variable individually (using univariate EVT). Then, one standardises to common margins and models the extremal dependence structure, i.e. the inter-relationships between extremes across multiple variables. Copula theory provides a rigorous justification for this two-step process.

### Copulae and marginal standardisation

In multivariate statistics, Sklar's theorem allows for the separation of the marginal distributions of variables from their joint dependence structure through the use of a copula. It states that any multivariate distribution can be expressed as a combination of individual marginal distributions and a copula that captures the dependence between them.

:::{#thm-sklar}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ has joint distribution function $F$ and continuous marginal distributions $X_i\sim F_i$ for $i=1,\ldots,d$. Then there exists a unique copula $C$ such that
\begin{equation}
    F(x_1,\ldots,x_d) = C\left(F_1(x_1),\ldots,F_d(x_d)\right).
\end{equation}
:::

The copula $C$ characterises the dependence structure of the variables, and represents the distribution function of $\bm{X}$ after transforming to standard uniform margins. Uniform margins are a standard choice in multivariate statistics, but copulae may be defined with alternative marginal distributions. In extreme value theory, it is common to use Fréchet, exponential or Gumbel margins. The different choices accentuate particular features of the extreme values. For example, heavy-tailed Fréchet margins serve to highlight the most extreme values, while Gumbel or exponential margins are often favoured for conditional extremes modelling (CITE Heffernan and Tawn). Although the marginal distribution is an important modelling choice, ultimately all choices are valid/equivalent in the sense that monotonic transformations of the univariate marginals do not change the nature of tail dependence [@resnickHeavytailPhenomenaProbabilistic2007].

There are broadly two ways of performing the preliminary marginal standardisation. Suppose $\bm{X}=(X_1,\ldots,X_d)$ has marginal distributions $X_i\sim F_i$ for $i=1,\ldots,d$. If the functions $F_i$ are known, then the marginal distributions can be transformed to some common target distribution $F_\star$ via the probability integral transform:
\begin{equation}\label{eq-marginal-transformation}
    X_i \mapsto F_\star^{-1}(F_i(X_i)) \sim F_\star, \qquad (i=1,\ldots,d).
\end{equation}
If the marginal distributions are unknown, as is usually the case, then $F_i$ is replaced with some estimate $\hat{F}_i$ in \eqref{eq-marginal-transformation}. A standard choice for $\hat{F}_i$ is the empirical CDF (non-parametric), perhaps with GPD tails above a high threshold (semi-parametric). Examples of these two approaches can be found in @russellAnalyzingDependenceMatrices2018 and @rohrbeckSimulatingFloodEvent2023, respectively. Throughout this thesis, uncertainty arising from estimation of the marginal distributions shall be neglected. Relaxing this assumption, as in @clemenconConcentrationBoundsEmpirical2023, represents an avenue for future work. 

### The exponent measure and angular measure

Suppose $\bm{X}$ is on unit Fréchet margins, that is
\begin{equation}\label{eq-unit-frechet}
    \mathbb{P}(X_i<x) = \exp(-1/x), \qquad (x>0),
\end{equation}
for $i=1,\ldots,d$. This corresponds to a GEV distribution with $\mu=\sigma=\xi=1$. The joint distribution $G$ in \eqref{eq-multivariate-mda} may be rewritten in the form 
\begin{equation}\label{eq-mevd}
    G(\bm{x}) = \exp(-V(\bm{x})),
\end{equation}
where $\bm{x}=(x_1,\ldots,x_d)$ and $x_i>0$ for $i=1,\ldots,d$. The exponent measure $V$ is a function of the form
\begin{equation}\label{eq-exponent-measure}
    V(\bm{x}) = d \int_{\mathbb{S}_{+(1)}^{d-1}} \bigvee_{i=1}^d \left(\frac{\theta_i}{x_i}\right)\,\dee H(\bm{\theta}).
\end{equation}
Here
\begin{equation}
    \mathbb{S}_{+(p)}^{d-1} := \{\bm{x}\in\R_+^d : \|\bm{x}\|_p = 1\}
\end{equation}
denotes the $L_p$-simplex in the non-negative orthant of $\R^d$ and the angular measure $H$ is a probability measure on $\mathbb{S}_{+(1)}^{d-1}$ satisfying the moment constraints
\begin{equation}\label{eq-H-mean-constraints}
    \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \,\dee H(\bm{\theta}) = 1/d, \qquad (i=1,\ldots,d).
\end{equation}
Our notation for the simplex is borrowed from @fixSimultaneousAutoregressiveModels2021. The exponent $d-1$ highlights the fact that the simplex is a $(d-1)$-dimensional set embedded in the $d$-dimensional space $\R^d$. The $+$ and $(p)$ in the subscript convey that the set is restricted to the non-negative orthant and is with respect to the $L_p$-norm, respectively. The constraints on $H$ arise due to tail equivalence of the margins. Functions $G$ satisfying \eqref{eq-mevd} are called multivariate extreme value distributions. If $V$ is differentiable, then the density $h$ of $H$ exists in the interior and on the low-dimensional boundaries of the simplex. The relation between $V$ and $h$ is given by
\begin{equation}
    h\left(\frac{\bm{x}}{\|\bm{x}\|_1}\right) = -\frac{\|\bm{x}\|_1^{d+1}}{d}\frac{\partial^d}{\partial x_1\cdots \partial x_d} V(\bm{x}).
\end{equation}
The benefit of introducing the exponent and angular measures is that models for $G$ may be specified in terms of $V$ or $H$. The extremal dependence structure of $\bm{X}$ is completely characterised by $H$: the angular measure determines $V$ via \eqref{eq-exponent-measure} and subsequently $G$ via \eqref{eq-mevd}. Modelling the angular measure now becomes our primary focus.

### Parametric multivariate extreme value models

The class of valid dependence structures is in direct correspondence to the infinite-dimensional class of valid measures $H$. This greatly hinders efforts to perform statistical inference: efficient estimation via likelihood inference, hypothesis testing, and inclusion of covariates immediately become unavailable. We may return to the parametric paradigm by postulating a suitable parametric sub-family. Ideally the chosen sub-family generates a wide class of valid dependence structures. A detailed review of popular models can be found in @gudendorfExtremeValueCopulas2010. 

There are several drawbacks to the parametric approach. Working with a parametric model instead of the general class runs the risk of model misspecification. Generating valid models is a challenging endeavour due to the moment constraints, resulting in models that are either overly simplistic or have unwieldy distribution functions and parameter constraints. Striking a balance between flexibility and parsimony becomes especially in high dimensions (i.e. when $d$ is large). For these reasons, parametric models are not a primary focus of this thesis. Nevertheless, we now review a small selection of models. These will feature regularly as data-generating processes for our numerical experiments. 

#### Logistic-type models

One of the oldest and simplest multivariate extreme value models is the symmetric logistic distribution [@gumbelBivariateExponentialDistributions1960].

:::{#def-symmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the symmetric logistic distribution is
\begin{equation}\label{eq-symmetric-logistic-V}
    V(\bm{x}) = \left(\sum_{i=1}^d x_i^{-1/\gamma}\right)^{\gamma}, \qquad \gamma \in (0,1].
\end{equation}
:::

The single dependence parameter $\gamma\in(0,1]$ characterises the strength of the association between all variables. Independence occurs when $\gamma=1$ and the variables approach complete dependence as $\gamma\to 0$. All variables are exchangeable, since the distribution function is invariant under coordinate permutation. A flexible extension is the asymmetric logistic model of @tawnModellingMultivariateExtreme1990. Greater control over the dependence structure is achieved by increasing the number of parameters.

:::{#def-asymmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the asymmetric logistic distribution is of the form 
\begin{equation}\label{eq-asymmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}\left[\sum_{i\in\beta}\left(\frac{\theta_{i,\beta}}{x_i}\right)^{1/\gamma_\beta}\right]^{\gamma_\beta}, \qquad 
    \begin{cases}
      \gamma_\beta\in(0,1], \\
      \theta_{i,\beta}\in[0,1], & \text{if }i\in\beta, \\
      \theta_{i,\beta}=0, & \text{if }i\notin\beta, \\
      \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset} \theta_{i,\beta}=1,
    \end{cases}
\end{equation}
where $\mathcal{P}(\{1,\ldots,d\})\setminus\emptyset$ denotes the set of non-empty subsets of $\{1,\ldots,d\}$.
:::

The set of parameters $\{\gamma_\beta:\beta\in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset\}$ control the dependence strength among the corresponding variables $\{X_i : i\in\beta\}$ in a similar way to the symmetric logistic model. The model's complexity is due to the set of asymmetry parameters $\bm{\theta}_\beta=(\theta_{i,\beta}:i\in\beta)$, which dictate the direction/composition of extreme events involving the variables $\{X_i : i\in\beta\}$. Further models can be generated by `inverting' the logistic and asymmetric models. **The purpose of inverting is...**. When applied to the models described above, inversion yields the negative symmetric logistic model [@galambosOrderStatisticsSamples1975] and the negative asymmetric logistic model [@joeFamiliesMinstableMultivariate1990], respectively.

:::{#def-negative-symmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the negative symmetric logistic distribution is
\begin{equation}\label{eq-negative-symmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}(-1)^{|\beta|+1}\left(\sum_{i\in\beta} x_i^{\gamma}\right)^{-1/\gamma}, \qquad \gamma>0.
\end{equation}
:::

:::{#def-negative-asymmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the negative asymmetric logistic distribution is
\begin{equation}\label{eq-negative-asymmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}(-1)^{|\beta|+1}\left(\sum_{i\in\beta} x_i^{\gamma}\right)^{-1/\gamma}, \qquad \gamma>0.
\end{equation}
:::

Other logistic-type models include the bilogistic @smithStatisticsMultivariateExtremes1990] and negative bilogistic [@colesStatisticalMethodsMultivariate1994].

#### Brown-Resnick processes and the Hüsler-Reiss distribution

Consider a Brown-Resnick process $\{X(\bm{s}):\bm{s}\in\R^2\}$ with semi-variogram
\begin{equation}\label{eq-fractal-variogram}
    \gamma(\bm{s}, \bm{s}') = (\|\bm{s}-\bm{s}'\|_2/\rho)^\kappa, \qquad \rho>0, \kappa\in(0,2].
\end{equation} 
Semi-variograms of the form \eqref{eq-fractal-variogram} are called fractal semi-variograms. The associated spatial process $X(\bm{s})$ is stationary and isotropic \parencite{engelke_estimation_2015}. The parameters $\rho$ and $\kappa$ control the range and smoothness, respectively. \textit{Davison et al. (2012) apply to rainfall data, finding $1/2<\kappa<1$.}

:::{#def-brown-resnick}
The bivariate exponent measure of a Brown-Resnick process $X(\bm{s})$ at sites $\{\bm{s}_i,\bm{s}_j\}$ is [@huserCompositeLikelihoodEstimation2013]
\begin{equation}\label{eq-brown-resnick-V}
    V(x_i,x_j) = \frac{1}{x_i} \Phi\left(\frac{a_{ij}}{2} + \frac{1}{a_{ij}}\log\frac{x_j}{x_i}\right) + \frac{1}{x_j} \Phi\left(\frac{a_{ij}}{2} + \frac{1}{a_{ij}}\log\frac{x_i}{x_j}\right),
\end{equation}
where $x_i = x(\bm{s}_i)$, $x_j = x(\bm{s}_j)$, and $a_{ij} = \sqrt{\gamma(\bm{s}_i,\bm{s}_j)}$.
:::

From \eqref{eq-brown-resnick-V} it is evident that the association between two sites is determined by their spatial proximity, since $a_{ij}$ depends on the spatial locations $\bm{s}_i$ and $\bm{s}_j$ only through $\|\bm{s}_i-\bm{s}_j\|_2$. This reflects the stationarity of the underlying spatial process.

The Brown-Resnick process is intimately related to the Hüsler-Reiss distribution of @huslerMaximaNormalRandom1989, which arises as the limit of suitably normalised Gaussian random vectors. The Hüsler-Reiss distribution is of fundamental importance in multivariate extremes; it has been labelled the Gaussian distribution for extremes \parencite{engelke_graphical_2019}. In $d\geq 2$ dimensions the distribution is parametrised by a matrix $\Lambda=(\lambda_{ij}^2)_{1\leq i,j\leq d}\in \mathcal{D}$, where $\mathcal{D}\subset\R_+^{d\times d}$ denotes the space of symmetric, strictly conditionally negative definite matrices
\begin{equation*}
    \mathcal{D} := \left\lbrace M \in \R_+^{d\times d} \, : \, M=M^T,\, \mathrm{diag}(M)=\bm{0},\, \bm{x}^TM\bm{x} < 0 \,\forall \bm{x}\in\R^d\setminus\{\bm{0}\} \text{ such that } \sum_{j=1}^d x_j=0\right\rbrace.
\end{equation*}
The class of Hüsler-Reiss distributions is closed, in the sense that if $\bm{X}=(X_1,\ldots,X_d)$ follows a Hüsler-Reiss distribution with parameter $\Lambda$, then $(X_i,X_j)$, $i\neq j$, is also Hüsler-Reiss distributed with parameter $\lambda_{ij}^2$. The dependence between any pairs of components can be controlled by modifying the corresponding $\lambda_{ij}>0$, subject to the constraint $\Lambda\in\mathcal{D}$. Its relation to the Brown-Resnick process is that the finite-dimensional distribution at locations $\bm{s}_1,\ldots,\bm{s}_d$ of a Brown-Resnick process is the Hüsler-Reiss distribution with $\Lambda = (\gamma(\bm{s}_i,\bm{s}_j)/4)_{1\leq i,j \leq d}$ \parencite{engelke_estimation_2015}. Due to this connection, the Hüsler-Reiss distribution is often parametrised by the variogram matrix $\Gamma=4\Lambda\in\mathcal{D}$ \parencite{engelke_sparse_2021, fomichov_spherical_2023}. The bivariate exponent measure of $(X_i,X_j)$ is given by \eqref{eq-brown-resnick-V} with $a_{ij}$ replaced with $2\lambda_{ij}$.

#### The max-linear model

*Preamble.*

:::{#def-max-linear}
Let $A=(\bm{a}_1,\ldots,\bm{a}_q)\in\R_+^{d\times q}$ for some $q\geq 1$. Assume that $\bm{a}_j\neq\bm{0}$ for all $j=1,\ldots,q$ and each row has unit sum, i.e $\sum_{j=1}^q a_{ij}=1$ for $i=1,\ldots,d$. A random vector $\bm{X}=(X_1,\ldots,X_d)$ with discrete angular measure
\begin{equation}\label{eq-max-linear-H}
    H(\cdot) = \frac{1}{\sum_{j=1}^q \|\bm{a}_j\|_1}\sum_{j=1}^q \|\bm{a}_j\|_1 \delta_{\bm{a}_j/\|\bm{a}_j\|_1}(\cdot)
\end{equation}
is said to follow the max-linear model with parameter matrix $A$.
:::

The unit-sum constraint on the rows of $A$ ensures that \eqref{eq-max-linear-H} is a valid angular measure since, for any $i=1,\ldots,d$,
\begin{equation*}
    \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \,\dee H(\bm{\theta}) 
    = \frac{1}{\sum_{j=1}^q \|\bm{a}_j\|_1} \sum_{j=1}^q \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \|\bm{a}_j\|_1 \delta_{\bm{a}_j/\|\bm{a}_j\|_1}(\bm{\theta})\,\dee \bm{\theta}
    = \frac{\sum_{j=1}^q a_{ij}}{\sum_{i=1}^d \sum_{j=1}^q  a_{ij}} 
    = \frac{1}{d}.
\end{equation*}
Due to the row constraints the max-linear model has $d\times(q-1)$ free parameters. Reordering the columns does not alter the angular measure. The factors $\bm{a}_1,\ldots,\bm{a}_q$ correspond to the possible directions that extremal observations may take, while $\|\bm{a}_1\|_1,\ldots,\|\bm{a}_q\|_1$ determine their respective weights.

To construct a random vector $\bm{X}=(X_1,\ldots,X_d)$ with angular measure \eqref{eq-max-linear-H}, we let $Z_1,\ldots,Z_q$ be independent unit Fréchet random variables and $\bm{Z}=(Z_1,\ldots,Z_q)$ and set
\begin{equation}\label{eq-max-linear-X}
    \bm{X} = A \times_{\max} \bm{Z} := \left( \bigvee_{j=1}^q a_{1j}Z_j, \ldots, \bigvee_{j=1}^q a_{dj}Z_j \right)
\end{equation}
or
\begin{equation}\label{eq-max-linear-X-cooley}
    \bm{X} = A \otimes \bm{Z} := \bigoplus_{j=1}^q (\bm{a}_j \odot Z_j).
\end{equation}
The operations $\oplus$ and $\odot$ relate to the vector space in @cooleyDecompositionsDependenceHighdimensional2019; they will be defined explicitly in Section XX. With this construction, $\bm{X}$ has angular measure \eqref{eq-max-linear-H} and unit Fréchet margins [@kirilioukEstimatingProbabilitiesMultivariate2022]. There is a direct correspondence between the class of discrete angular measure placing mass on $q<\infty$ points and the class of max-linear random vectors \eqref{eq-max-linear-X} with $q$ factors. The class of angular measures \eqref{eq-max-linear-H} is dense in the class of valid angular measures [@fougeresDenseClassesMultivariate2013]. In other words, any extremal dependence structure can be arbitrarily well-approximated by an angular measure generated by a max-linear model with sufficiently many factors. This makes the max-linear model a versatile and powerful modelling framework, despite its simplicity.

*Formulae for tail events under max-linear model.*

#### Sampling from parametric models

The logistic-type, Hüsler-Reiss and max-linear models will be used to generate synthetic data throughout this thesis. The R package \texttt{mev} provides functionalities for this purpose. The underlying sampling algorithms are formulated in @dombryExactSimulationMaxstable2016. The \texttt{rmev} function generates independent realisations $\bm{X}_1,\ldots,\bm{X}_n$ on unit Fréchet margins from the specified parametric multivariate extreme value model. The function \texttt{rmevspec} produces independent observations $\bm{\Theta}_1,\ldots,\bm{\Theta}_n$ directly from the angular measure $H$. Generally, we will be interested in using the observations $\bm{X}_1,\ldots,\bm{X}_n$ to learn a model for $H$. The samples $\bm{\Theta}_1,\ldots,\bm{\Theta}_n$ can be used for model validation.

*Make figure depicting the dependence structure of SL, HR and max-linear models.*

### Multivariate regular variation

#### Univariate regular variation

:::{#def-regular-variation-function}
A function $f:\R_+\to\R_+$ is regularly varying with index $\alpha\in\R$ if, for all $x>0$,
    \begin{equation}
        \lim_{t\to\infty}\frac{f(tx)}{f(t)} = x^\alpha.
    \end{equation}
:::

In the case $\alpha=0$, $f$ is called slowly-varying. This notion is extended to random variables by treating the distributional tail as the function of interest. 

:::{#def-regular-variation-random-variable}
A non-negative random variable $X$ is regularly varying with tail index $\alpha \geq 0$ if the right-tail of its distribution function is regularly varying with index $-\alpha$, i.e. for all $x>1$,
    \begin{equation*}
        \lim_{t\to\infty} \mathbb{P}(X > tx \mid X > t) = x^{-\alpha}.
    \end{equation*}
:::

If $X$ is regularly varying with index $\alpha$, then its survivor function is of the form
\begin{equation}\label{eq-regularly-varying-survivor-function}
    \mathbb{P}(X>x) = x^{-\alpha} L(x)
\end{equation}
for some slowly-varying function $L$ [@jessenRegularlyVaryingFunctions2006]. This says that regularly varying random variables are those with power law tails. In fact, a random variable $X$ is regularly varying if and only if it belongs to the Fréchet MDA. Moreover, \eqref{eq-regularly-varying-survivor-function} means that regularly varying distributions are asymptotically scale invariant, in the sense that for all $\lambda>0$,
\begin{equation*}
    \mathbb{P}(X > \lambda x) = (\lambda x)^{-\alpha} L(\lambda x) \sim \lambda^{-\alpha} \mathbb{P}(X > x).
\end{equation*}
This asymptotic homogeneity explains why regular variation is ubiquitous in extreme value theory.

Multivariate regular variation (MRV) provides an alternative characterisation of the probabilistic structure of multivariate extreme events. Under this framework, the asymptotic joint tail distribution is represented by a homogeneous limit measure. Although MRV can be formulated more generally, we focus on the case where $\bm{X}$ takes values on the positive orthant $\R_+^d:=[0,\infty)^d$. This common assumption is not as restrictive as it might initially seem. In most applications, the risk being assessed is directional. For example, a climatologist might focus on the lows or highs of precipitation records, depending on whether he is assessing drought risk or flood risk. Without loss of generality, and by means of a transformation if necessary, we can define this direction of interest to be ‘positive’.

:::{#def-mrv}
We say that $\bm{X}$ is multivariate regularly varying with tail index $\alpha>0$, denoted $\bm{X}\in\mathcal{RV}_+^d(\alpha)$, if it satisfies the following (equivalent) statements [@resnickHeavytailPhenomenaProbabilistic2007]: 

1. There exists a sequence $b_n\to\infty$ and a non-negative Radon measure $\nu$ on $\mathbb{E}_0:=[0,\infty]^d\setminus\{\bm{0}\}$ such that
\begin{equation}\label{eq-mrv}
n\mathbb{P}(b_n^{-1}\bm{X} \in \cdot) \stackrel{\mathrm{v}}{\rightarrow} \nu(\cdot),\qquad (n\to\infty),
\end{equation}
where $\stackrel{\mathrm{v}}{\rightarrow}$ denotes vague convergence in the space of non-negative Radon measures on $\mathbb{E}_0$. The exponent measure $\nu$ is homogeneous of order $-\alpha$, that is, for any $s>0$,
\begin{equation}
    \nu(s\,\cdot)=s^{-\alpha}\nu(\cdot).
\end{equation}
2. Let $\|\cdot\|$ be an arbitrary norm on $\R^d$. Denote the radial and angular components of $\bm{X}$ by $R:=\|\bm{X}\|$ and $\bm{\Theta}:=\bm{X}/\|\bm{X}\|$. Then there exists a sequence $b_n\to\infty$ and a finite measure $H$ on 
    \begin{equation}\label{eq-general-simplex}
        \mathbb{S}_{+}^{d-1}:=\{\bm{x}\in\R_+^d:\|\bm{x}\|=1\}
    \end{equation}
    such that
\begin{equation}\label{eq-polar-mrv}
n\mathbb{P}((b_n^{-1}R,\bm{\Theta}) \in \cdot) \stackrel{\mathrm{v}}{\rightarrow} \nu_{\alpha}\times H(\cdot),\qquad (n\to\infty),
\end{equation}
in the space of non-negative Radon measures on $(0,\infty]\times\mathbb{S}_{+}^{d-1}$, where $\nu_{\alpha}((x,\infty))=x^{-\alpha}$ for any $x>0$.
:::

The limit measures $\nu$ and $H$ in \eqref{eq-mrv} and \eqref{eq-polar-mrv} are related via 
\begin{equation}\label{eq-nu-H-relation}
    \nu(\{\bm{x}\in\mathbb{E}_0:\|\bm{x}\| > s,\bm{x}/\|\bm{x}\|\in\cdot\}) 
    = s^{-\alpha}H(\cdot),\qquad  \nu(\dee r\times \dee \bm{\theta})
    =\alpha r^{-\alpha-1}\dee r\,\dee H(\bm{\theta}).
\end{equation}
The pseudo-polar formulation \eqref{eq-polar-mrv} reveals the attractive feature of MRV. It states that the extremal behaviour of $\bm{X}$ is fully characterised by two objects. The tail index $\alpha$ represents the index of regular variation of the radial component, thereby governing the heavy-tailedness of $\|\bm{X}\|$. The angular measure $H$ fully characterises the dependence structure. Crucially, the right-hand side of \eqref{eq-polar-mrv} is a product measure. This is often called the radial-angular decomposition This signifies that the radial and angular components are independent in the limit. 

The MRV property implicitly requires that the marginal components $X_1,\ldots,X_d$ are heavy-tailed with a shared tail index. Recalling from Section XX that standard practice is to standardise the margins prior to modelling the dependence structure, this is not constraining. Fixing the marginal distributions determines the index $\alpha$. In this thesis, we will typically choose Fréchet margins with unit scale and shape parameter $\alpha>0$, that is
\begin{equation}\label{eq-alpha-frechet}
    \mathbb{P}(X_i<x) = \exp(-x^{-\alpha}), \qquad (x>0).
\end{equation}
A random vector with $\alpha$-Fréchet margins \eqref{eq-alpha-frechet} has tail index $\alpha$. 

The angular measure is unique with respect to a fixed norm $\|\cdot\|$ and lies on the corresponding unit simplex defined in \eqref{eq-general-simplex}. The exponent $d-1$ in $\mathbb{S}_{+}^{d-1}$ references that fact that the simplex is a $(d-1)$-dimensional set embedded in the $d$-dimensional Euclidean space $\R^d$. In this thesis, we will exclusively use the $L_p$-norm 
\begin{equation}
    \|\cdot\|_p : \R^d \to \R, \qquad \|\bm{x}\|_p = \left(\sum_{i=1}^d x_i^{p}\right)^{1/p}
\end{equation}
The corresponding simplex will be denoted by
\begin{equation} 
    \mathbb{S}_{+(p)}^{d-1} := \{\bm{x}\in\R_+^d:\|\bm{x}\|_p=1\}. \label{eq-lp-simplex}
\end{equation}

The mass of the angular measure is $m:=H(\mathbb{S}_{+}^{d-1})\in(0,\infty)$. The sequence $\{b_n\}$ and the quantity $m$ are jointly determined by \eqref{eq-polar-mrv}. To see this, note that replacing $\{b_n\}$ by $\{s\cdot b_n\}$ for some $s>0$ yields a new angular measure $H'=s^{-\alpha}H$ with total mass $m'=s^{-\alpha}m$. We are free to choose whether the scaling information is contained in $\{b_n\}$ or $H$. @fougeresDenseClassesMultivariate2013 explain possible reasons for preferring one over the other, but ultimately it is an arbitrary modelling choice. Conventionally $H$ is normalised to be a probability measure, that is $m=1$. At certain points during this thesis, we might instead specify $\{b_n\}$ and push the scaling information on to $H$. Irrespective of whether $H$ is normalised or not, we write $\bm{W}\sim H$ to denote a random vector $\bm{W}$ whose distribution is the probability measure $m^{-1}H$. 

With $\bm{X}$ standardised to common margins, the centre of mass of $H$ must lie in the simplex interior:
\begin{equation}\label{eq-H-mean-constraints-general}
    \int_{\mathbb{S}_{+}^{d-1}} \theta_i \,\dee H(\bm{\theta}) = \mu > 0, \qquad (i=1,\ldots,d).
\end{equation}
The value of $\mu$ depends on the choice of norm. If $\|\cdot\|=\|\cdot\|_1$, then $\mu=m/d$, in accordance with \eqref{eq-H-mean-constraints}. If $\|\cdot\|=\|\cdot\|_2$, then $m/d\leq \mu \leq m/\sqrt{d}$ [@fomichovSphericalClusteringDetection2023, Lemma 2.1]. 

### Extremal dependence

Extremal dependence is analogous to, but separate from, the notion of statistical dependence in non-extreme statistics. In particular, two random processes might appear independent in the bulk of the distribution but exhibit dependence in their extremes, or vice versa. The extremal dependence structure can be very complex, being subject only to the mean constraints \eqref{eq-H-mean-constraints}. For example, the extremal dependence between a meteorological variable measured at two locations may depend on the topography of the spatial domain, the physics of the underlying climatological processes, and the locations' spatial proximity. 

The extremal dependence structure of a random vector $\bm{X}$ can be quantified and classified using a plethora of summary measures [@colesDependenceMeasuresExtreme1999]. We focus on the tail dependence coefficient and the extremal dependence measure. 

### The tail dependence coefficient

:::{#def-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ with $X_i\sim F_i$ for $i=1,\ldots,d$. Let $\beta\subseteq\{1,\ldots,d\}$ with $|\beta|\geq 2$ and define $\bm{X}_\beta := \{X_i : i\in\beta\}$. The tail dependence coefficient associated with $\beta$ is (CITE e.g. Simpson et al 2020)
\begin{equation}
    \chi_\beta = \lim_{u\to 1} \chi_\beta (u) = \lim_{u\to 1} \frac{\mathbb{P}(F_i(X_i) > u : i \in \beta)}{1-u}.
\end{equation}
When $\beta=\{i,j\}$ for $i\neq j$, we write $\chi_\beta=:\chi_{ij}$.
:::

If $\chi_{ij}=0$, then we say that $X_i$ and $X_j$ are asymptotically independent. This means that $X_i$ and $X_j$ cannot take their largest values simultaneously. If $\chi_{ij}\in(0,1]$, then the variables exhibit asymptotic dependence and may be simultaneously extreme. The interpretation of $\chi_\beta$ for $|\beta|>2$ is more subtle. If $\chi_\beta\in(0,1]$, then all components of $\bm{X}_\beta$ may be simultaneously large. If $\chi_\beta=0$, then the corresponding variables may not be concomitantly extreme, but this does not preclude the possibility that $\chi_{\beta'}>0$ for some $\beta'\subset\beta$ with $|\beta'|\geq 2$.

The relation between the tail dependence coefficient and the angular measure is as follows: $\chi_\beta >0$ if and only if there exists $\beta'\supset\beta$ such that
\begin{equation}\label{eq-asy-dep-chi-H}
    H(\{\bm{\theta}\in\mathbb{S}_+^{d-1} : \theta_i > 0 \iff i\in\beta'\}) >0.
\end{equation}
For example, consider the measures
\begin{equation}\label{eq-ai-ad-H}
    H_1 = \frac{m}{d}\sum_{i=1}^d \delta_{\bm{e}_i}, \qquad H_2 = m\delta_{\bm{1}/\|\bm{1}\|},
\end{equation}
where $\bm{e}_1,\ldots,\bm{e}_d$ denote the canonical basis vectors of $\R^d$. The measure $H_1$ places all its mass on the coordinate axes. This corresponds to the case of full asymptotic independence, i.e. $\chi_\beta = 0$ for all (non-empty, non-singleton) $\beta\subseteq\{1,\ldots,d\}$. On the other hand, a random vector with angular measure $H_2$ possesses perfect/complete asymptotic dependence and $\chi_\beta > 0$ for all $\beta$. 

If the bivariate exponent measure $V(x_i,x_j)$ of $(X_i,X_j)$ is known, then the tail dependence coefficient can be computed using $\chi_{ij}=2-V(1,1)$. 

:::{#exm-symmetric-logistic-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ be symmetric logistic distributed with dependence parameter $\alpha\in(0,1]$. For any $i\neq j$, let $V_{ij}$ denote the bivariate exponent measure of $(X_i,X_j)$. Then
\begin{equation}
    \chi_{ij} = 2- V_{ij}(1,1) = 2 - \left[ \left(x_i^{-1/\alpha} + x_j^{-1/\alpha} \right)^\alpha \right] = 2-2^\alpha.
\end{equation}
    Therefore $X_i$ and $X_j$ approach asymptotic independence when $\alpha=1$ and exhibit asymptotic dependence when $\alpha\in(0,1)$.
:::

:::{#exm-asymmetric-logistic-chi}
See Simpson thesis page 18-19.
:::

:::{#exm-husler-reiss-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ be Hüsler-Reiss distributed with parameter matrix $\Lambda=(\lambda_{ij}^2)$. For any $i\neq j$, let $V_{ij}$ denote the bivariate exponent measure of $(X_i,X_j)$. Then
\begin{equation}
    \chi_{ij} = 2 - V_{ij}(1,1) = 2 - 2 \Phi\left(\lambda_{ij} + \frac{1}{2\lambda_{ij}}\log 1 \right) = 2 - 2\Phi(\lambda_{ij}).
\end{equation}
This concurs with, e.g. Remark 25 in CITE Kabluchko et al. (2009). We note that $X_i$ and $X_j$ are asymptotically dependent for all $\lambda>0$, with asymptotic independence in the limit as $\lambda\to\infty$.
:::

:::{#exm-max-linear-chi}
Suppose $\bm{X}=(X_1,X_2)$ is max-linear with parameter matrix $A\in\R_+^{2\times q}$. Then using the angular measure $\eqref{eq-max-linear-model-H}$ and its relation to the exponent measure \eqref{eq-exponent-measure}, we have
\begin{equation}
    \chi_{12} = 2 - V_{12}(1,1) = 2 - 2\int_{\mathbb{S}_{+(1)}^1} (\theta_1 \vee \theta_2) \,\dee H(\bm{\theta}) = 2 - 2 \sum_{j=1}^q (a_{1j} \vee a_{2j}).
\end{equation}
:::

The bivariate dependence measure $\chi_{ij}$ is usually estimated by computing the empirical probabilities $\hat{\chi}_{ij}(u)$ at a sequence of high quantiles $u$ approaching one. An example of the resulting diagnostic plot is provided in @fig-chi-estimation. The underlying data are generated from a symmetric logistic model with $\alpha=0.5$. The black points represent the empirical estimates $\hat{\chi}_{ij}(u)$ over the range $0.8 \leq u \leq 0.995$, with a 95\% confidence interval depicted by the grey region. The true value $\chi_{ij}=2-\sqrt{2}\approx 0.59$ (see \autoref{ex-symmetric-logistic-chi}) is indicated by the red horizontal line. The plot illustrates a clear example of the bias-variance trade-off in relation to the choice of quantile/threshold. This phenomenon is ubiquitous in threshold-based extreme value statistics and will be discussed in more detail in Section ??. 

```{r background-make-fig-chi-estimation}
#| label: fig-chi-estimation
#| fig-cap: "Empirical estimates $\\hat{\\chi}_{12}(u)$ of the tail dependence coefficient for bivariate symmetric logistic data with $\\gamma=0.5$ and $n=5,000$ observations. The true coefficient $\\chi_{12}=2-2^\\gamma\\approx 0.59$ is marked by the dashed line. The shaded region represents the 95\\% Wald confidence interval."
#| fig-scap: "Empirical estimates $\\hat{\\chi}_{12}(u)$ for bivariate symmetric logistic data."
#| fig-height: 2.8
#| fig-width: 5

set.seed(1)
gamma <- 0.5
X <- rmev(n = 5000, d = 2, par = 1 / gamma, model = "log")

taildep(X, meas = "chi", qlim = c(0.8, 0.995), plot = FALSE)$chi %>%
  as.data.frame() %>%
  mutate(q = seq(from = 0.8, to = 0.995, length.out = 40)) %>%
  ggplot(aes(x = q, y = coef)) +
  geom_ribbon(aes(ymax = upperci, ymin = lowerci), fill = "lightblue", colour = NA, alpha = 0.4) +
  geom_line(colour = "blue") +
  geom_point(colour = "blue") +
  geom_hline(yintercept = sl_chi(gamma), linetype = "dashed", colour = "black") +
  scale_x_continuous(limits = c(0.8, 1), expand = expansion(mult = c(0.01, 0)), breaks = breaks_extended(n = 8)) +
  theme_light() +
  labs(x = expression(u),
       y = expression(hat(chi)[12](u)))
```

Estimation of $\chi_\beta$ for $|\beta| > 2$ is more complicated. Determining the collection of $\beta$ for which $\chi_\beta > 0$ is equivalent to identifying the support of the angular measure, i.e. which faces of the simplex possess $H$-mass. *This will be explained later. Sparsity assumption, empirical angular measure only places mass on interior, etc.*

#### Extremal dependence measure

An alternative bivariate summary measure is the extremal dependence measure (EDM). It was originally proposed by @resnickExtremalDependenceMeasure2004 and later refined by @larssonExtremalDependenceMeasure2012.

:::{#def-edm}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with angular measure $H$. The EDM between $X_i$ and $X_j$ is
\begin{equation}
    \mathrm{EDM}_{ij} := \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta}) \,\dee H(\bm{\theta}), \qquad f(\bm{\theta}) = \theta_i\theta_j.
\end{equation}
:::

The EDM depends on the choice of norm via the angular measure. However, Proposition 3 in @larssonExtremalDependenceMeasure2012 states that EDMs under different norms are equivalent in the sense of Definition 1 in the same paper. The original definition of the EDM, restricted to the bivariate case $\bm{X}=(X_1,X_2)$, instead used
\begin{equation}
    f(\bm{\theta}) = \left(\frac{4}{\pi}\right)^2 \arctan\left(\frac{\theta_2}{\theta_1}\right)\left[\frac{\pi}{2} - \arctan\left(\frac{\theta_2}{\theta_1}\right) \right].
\end{equation}
The original and refined versions are equivalent in the same sense. 

The interpretation of the coefficient is that $\mathrm{EDM}_{ij}=0$ if and only if $X_i$ and $X_j$ are asymptotically independent. This follows directly from \eqref{eq-asy-dep-chi-H} with $\beta=\{i,j\}$, since if $\chi_{ij}=0$ then
\begin{equation}
    \mathrm{EDM}_{ij}  = \int_{\{\bm{\theta}\in\mathbb{S}_{+}^{d-1} \, : \, \theta_i,\theta_j > 0\}} \theta_i \theta_j \,\dee H(\bm{\theta}) = 0.
\end{equation}
The EDM is maximal when the variables are perfectly asymptotically dependent. The maximal value depends on the choice of norm and the mass of the angular measure. In the bivariate case with $\|\cdot\|=\|\cdot\|_p$ we have $\mathrm{EDM}_{ij} \leq 2^{-2/p}m$ with equality if and only if $H$ places all its mass at the simplex barycentre, that is $H(\{(2^{-1/p}, 2^{-1/p})\}) = m$.

### Inference

By imposing a regularity structure on the joint distributional tail, MRV facilitates -- and provides a rigorous theoretical justification for -- a straightforward way of extrapolating the probability law from moderately large values to more extreme tail regions. To see this, note that from ?? we have
\begin{equation}\label{eq-extremal-angles-converge-H}
    \bm{\Theta}\mid (R>t) \stackrel{d}{\to} H(\cdot), \qquad (t\to\infty).
\end{equation}
The measure $H$ represents the limiting distribution of the angles of high threshold exceedances. This interpretation informs the general approach underpinning multivariate extreme value statistics.

#### Framework and notation

Generally speaking, inference for multivariate extremes involves selecting a high threshold $t>0$ and using the information from angular components corresponding to radial threshold exceedances. Increasing the threshold reduces the number of observations that enter into the estimators, and vice versa. It is generally more convenient to specify the desired number of threshold exceedances, denoted $k$, and set the threshold accordingly. This approach is most conveniently described using order statistics.

Consider a $d$-dimensional MRV random vector $\bm{X}\in\mathcal{RV}_+^d(\alpha)$. Let $\bm{X}_1,\bm{X}_2,\ldots$ denote a sequence of independent copies of $\bm{X}$. Let $\|\cdot\|$ be a fixed norm on $\R^d$. For $i\geq 1$, denote by
\begin{equation}
    R_i := \|\bm{X}_i\|>0, \qquad \bm{\Theta}_i := (\Theta_{i1},\ldots,\Theta_{id})=\frac{\bm{X}_i}{\|\bm{X}_i\|}\in\mathbb{S}_{+}^{d-1},
\end{equation}
the radial and angular components of $\bm{X}_i$ with respect to some chosen norm $\|\cdot\|$. Assume that the distribution of $\|\bm{X}\|$ is continuous. For any $n\geq 1$, there exists a permutation $\sigma:\{1,\ldots,n\}\to\{1,\ldots,n\}$ of the indices such that 
\begin{equation*}
    \|\bm{X}_{(1),n}\| > \|\bm{X}_{(2),n}\| > \ldots > \|\bm{X}_{(n),n}\|,
\end{equation*}
where $\bm{X}_{(i),n}:=\bm{X}_{\sigma(i)}$ for $i=1,\ldots,n$. We call $\|\bm{X}_{(j),n}\|$ the $j$th (upper) order statistic of $\{\|\bm{X}_i\|:i=1,\ldots,n\}$. Henceforth, we suppress the dependence on $n$ in our order statistic notation. For $i=1,\ldots,n$, the radial and angular components of $\bm{X}_{(i)}$ shall be denoted by
\begin{equation}
    R_{(i)} = \|\bm{X}_{(i)}\|>0, \qquad \bm{\Theta}_{(i)} = (\Theta_{(i),1},\ldots,\Theta_{(i),d})=\frac{\bm{X}_{(i)}}{\|\bm{X}_{(i)}\|}\in\mathbb{S}_{+}^{d-1}.
\end{equation}
Inference based on the $k=k(n)$ largest observations is equivalent to setting the radial threshold as $t=\hat{t}_k:=R_{(k+1)}$. Only the angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ will enter into the estimators. 

In theoretical analyses, it is customary to choose the sequence $\{k(n):n\geq 1\}$ such that
\begin{equation}\label{eq-k-rate-conditions}
    \lim_{n\to\infty}k(n)=\infty, \qquad \lim_{n\to\infty}\frac{k(n)}{n}=0.
\end{equation}
These arise as sufficient conditions for proving asymptotic properties (e.g. consistency) of estimators. The first condition ensures that the effective sample size becomes arbitrarily large. The second condition means that the proportion of threshold exceedances becomes vanishingly small, so that the estimators focus further into the tail. The more challenging practical question of how to select $k$ will be discussed later in Section XXX.

#### The empirical angular measure

The empirical angular measure is the natural non-parametric estimator for the angular measure. It represents the empirical distribution of the angles of the set of threshold exceedances.

:::{#def-empirical-angular-measure}
The empirical angular measure based on $\bm{X}_1,\ldots,\bm{X}_n$ is the random measure on $\mathbb{S}_{+}^{d-1}$ defined as
\begin{equation}
    \hat{H}(\cdot) := \frac{m}{k}\sum_{i=1}^n \delta_{\bm{\Theta}_i}(\cdot) \ind\{R_i > \hat{t}_k\} = \frac{m}{k} \sum_{i=1}^k \delta_{\bm{\Theta}_{(i)}}(\cdot).
\end{equation}
:::

Note that $\hat{H}$ does not enforce the moment constraints \eqref{eq-H-mean-constraints}, so is not necessarily a valid angular measure. @einmahlMaximumEmpiricalLikelihood2009 propose an alternative non-parametric estimator that does enforce these restrictions, but it is limited to the bivariate setting. Proposition 3.3 in @janssenKmeansClusteringExtremes2020 establishes consistency $\hat{H}\overset{p}{\to}H$ of the empirical angular measure provided the level $k$ satisfies the rate conditions \eqref{eq-k-rate-conditions}. Their result holds for general norms in arbitrary dimensions. @clemenconConcentrationBoundsEmpirical2023 conduct a non-asymptotic (i.e. finite sample) analysis of $\hat{H}$, establishing high-probability bounds on the worst-case estimation error $\sup_{A\in\mathcal{A}}|H(A) - \hat{H}(A)|$ over classes $\mathcal{A}$ of Borel subsets on $\mathbb{S}_{+}^{d-1}$. Their result holds with $\|\cdot\|=\|\cdot\|_p$ for $p\in[1,\infty]$. The empirical angular measure is a discrete angular measure on $k$ points. Consequently, there exists a max-linear random vector with $k$ factors whose angular measure is $\hat{H}$ (ignoring the fact that $\hat{H}$ is not necessarily a valid angular measure). 

The empirical angular measure is used to construct further non-parametric estimators. One is often interested in quantities of the form
\begin{equation}\label{eq-expectation-f}
    \mathbb{E}_{\bm{\Theta}\sim H} [f(\bm{\Theta})] := \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee H(\bm{\theta}),
\end{equation}
where $f:\mathbb{S}_{+}^{d-1}\to\R$. Unlike @kluppelbergEstimatingExtremeBayesian2021, our notation retains the mass of $M$ in $H$ (rather than absorbing it into $f$), so that if $\tilde{H}=m^{-1}H$ denotes the normalised counterpart of $H$, then
\begin{equation*}
    \mathbb{E}_{\bm{\Theta}\sim H} [f(\bm{\Theta})] = \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee H(\bm{\theta}) = m \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee \tilde{H}(\bm{\theta}) = m\mathbb{E}_{\bm{\Theta}\sim \tilde{H}} [f(\bm{\Theta})].
\end{equation*}
The analogous relation for variances is 
\begin{equation*}
    \mathrm{Var}_{\bm{\Theta}\sim H} (f(\bm{\Theta})) = m^2 \mathrm{Var}_{\bm{\Theta}\sim \tilde{H}} (f(\bm{\Theta})).
\end{equation*}

A natural estimator of \eqref{eq-expectation-f} is obtained by replacing $H$ with the discrete random measure $\hat{H}$ in the right-hand side, yielding
\begin{equation}
    \hat{\mathbb{E}}_{\bm{\Theta}\sim H} [f(\bm{\Theta})] := \mathbb{E}_{\bm{\Theta}\sim\hat{H}}[f(\bm{\Theta})] = \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee \hat{H}(\bm{\theta}) = \frac{m}{k}\sum_{i=1}^k f(\bm{\Theta}_{(i)}).
\end{equation}
@kluppelbergEstimatingExtremeBayesian2021 prove asymptotic normality of these estimators by generalising a result in @larssonExtremalDependenceMeasure2012. 

:::{#thm-clt-extremes}
Let $f:\mathbb{S}_+^{d-1}\to\R$ be continuous and assume $k$ satisfies the rate conditions $\eqref{eq-k-rate-conditions}$. Moreover, suppose that
\begin{equation}\label{eq-clt-rate-condition}
    \lim_{n\to\infty} \sqrt{k}\left[\frac{n}{k}\mathbb{E}[f(\bm{\Theta}_1)\ind\{R_1 \geq b_{\lfloor n/k \rfloor} t^{-1/\alpha}\}] - \mathbb{E}_{\bm{\Theta}\sim H}[f(\bm{\Theta})]\frac{n}{k}\bar{F}_R(b_{\lfloor n/k \rfloor} t^{-1/\alpha})\right]=0
\end{equation}
holds locally uniformly for $t\in[0,\infty)$, where $\bar{F}_R(\cdot)=\mathbb{P}(R>\cdot)$ denotes the survivor function of $R$. Finally, assume that
\begin{equation}\label{eq-clt-variance-condition}
    \sigma^2 := \mathrm{Var}_{\bm{\Theta}\sim H}(f(\bm{\Theta})) > 0.
\end{equation}
Then
\begin{equation}
    \sqrt{k}\left[\hat{\mathbb{E}}_{\bm{\Theta}\sim H} [f(\bm{\Theta})] - \mathbb{E}_{\bm{\Theta}\sim H} [f(\bm{\Theta})]\right]\to N(0, \sigma^2), \qquad (n\to\infty).
\end{equation}
:::

The rate condition \eqref{eq-clt-rate-condition} requires that the dependence between the radius and angle decays sufficiently quickly. This condition is non-observable and must be assumed.
For $f(\bm{\theta})=\theta_i\theta_j$ the condition \eqref{eq-clt-variance-condition} excludes the case of asymptotic independence, i.e. $\mathrm{EDM}_{ij}=0$, since the limit distribution is degenerate. This prevents us from, say, establishing asymptotic normality of $\widehat{\mathrm{EDM}}_{ij}=\hat{\mathbb{E}}_{\bm{\Theta}\sim H} [\Theta_i\Theta_j]$ under asymptotic independence. In that case, the above result would only prove consistency $\widehat{\mathrm{EDM}}_{ij}\to 0$. Possible strategies for circumventing this issue are proposed in @lehtomaaAsymptoticIndependenceSupport2020. 


## Tail pairwise dependence matrix (TPDM)

### Equivalent definitions

*Preamble here.*

:::{#def-tpdm}
Let $\bm{X}\in\mathcal{RV}_+^d(2)$ with normalising sequence $b_n=n^{1/2}$. Let $H$ denote the angular measure with respect to $\|\cdot\|_2$. The TPDM of $\bm{X}$ is the $d\times d$ matrix
    \begin{equation}\label{eq-tpdm}
        \Sigma = (\sigma_{ij}), \qquad \sigma_{ij} = \int_{\mathbb{S}_{+(2)}^{d-1}} \theta_i \theta_j \,\dee H(\bm{\theta}) = \mathbb{E}_{\bm{\Theta}\sim H} [\Theta_i\Theta_j].
    \end{equation}
:::

This definition was generalised to permit any $\alpha\geq 1$ by @kirilioukEstimatingProbabilitiesMultivariate2022.

:::{#def-tpdm-alpha}
For $\alpha\geq 1$, let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with normalising sequence $b_n=n^{1/\alpha}$. Let $H$ denote the angular measure with respect to $\|\cdot\|_\alpha$. The TPDM of $\bm{X}$ is the $d\times d$ matrix
\begin{equation}\label{eq-tpdm-alpha}
    \Sigma = (\sigma_{ij}), \qquad \sigma_{ij} = \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee H(\bm{\theta}) = \mathbb{E}_{\bm{\Theta}\sim H} [\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}].
\end{equation}
:::

Note that the parameter $\alpha$ does not solely represent the tail index of $\bm{X}$. It also dictates the normalisation sequence and the choice of norm. TPDM theory requires that all these choices conform in this way. Clearly the two definitions above coincide when $\alpha=2$, but @kirilioukEstimatingProbabilitiesMultivariate2022 provide no direct rationale for why \eqref{eq-tpdm-alpha} is the natural generalisation of \eqref{eq-tpdm}. We aim to shed light on this matter by showing in the bivariate setting that the TPDM (with respect to some $\alpha\geq 1$) is independent of $\alpha$. The following lemma helps us achieve this: it gives the formula for transforming between angular densities defined with different $\alpha$ values.

:::{#lem-angular-density-transformation}
Suppose $\bm{X}=(X_i,X_j)\in\mathcal{RV}_+^2(\alpha)$ for some $\alpha\geq 1$. Let $H_\alpha$ denote the normalised angular measure with respect to $\|\cdot\|_\alpha$ and $h_\alpha:\mathbb{S}_{+(\alpha)}\to\R_+$ the corresponding angular density (assuming it exists). Moreover, we define
\begin{equation*}
    \tilde{h}_\alpha:[0,1]\to\R_+, \qquad \theta \mapsto h_\alpha\left(\left(\theta,(1-\theta^\alpha)^{1/\alpha}\right)\right).
\end{equation*}
Then
\begin{equation}
    \tilde{h}_\alpha(\theta) = \alpha \theta^{\alpha-1} \tilde{h}_1(\theta^\alpha).
\end{equation}
:::

::: {.proof}
The proof generalises the procedure described in Section 3.2 of the Supplementary Material of @fixSimultaneousAutoregressiveModels2021. First, we transform from $L_1$ polar coordinates $(r,\bm{\theta})$ to Cartesian coordinates $\bm{z}=(z_i,z_j)=(r\theta_i,r\theta_j)$. The Jacobian of the transformation is $\|\bm{z}\|_1^{-1}$ (CITE Prop 1in Cooley et al 2012). Using \eqref{eq-nu-H-relation} with $\alpha=1$ and $H_1(\dee \bm{\theta})=h_1(\bm{\theta})\dee \bm{\theta}$,
    \begin{align*}
        \nu(\dee r \times \dee \bm{\theta}) 
        &= r^{-2} h_1(\bm{\theta}) \, \dee r\,\dee\bm{\theta} \\
        &= \|\bm{z}\|_1^{-2} h_1(\bm{z}/\|\bm{z}\|_1) \|\bm{z}\|_1^{-1} \dee\bm{z} \\
        &= \|\bm{z}\|_1^{-3} h_1(\bm{z}/\|\bm{z}\|_1) \dee\bm{z} \\
        &= \nu(\dee \bm{z}).
    \end{align*}
    Next, we transform from tail index $\alpha=1$ to arbitrary $\alpha$. Let $\bm{y}=(y_i,y_j)=(z_i^{1/\alpha},z_j^{1/\alpha})$. The Jacobian of this transformation is $\alpha^2 y_i^{\alpha-1}y_j^{\alpha-1}$. Note that $\|\bm{z}\|_1=y_i^\alpha+y_j^\alpha=\|\bm{y}\|_\alpha^\alpha$.
    \begin{equation*}
        \nu(\bm{z}) 
        = \left[\|\bm{y}\|_\alpha^{\alpha}\right]^{-3} h_1\left(\frac{y_i^\alpha}{\|\bm{y}\|_\alpha^{\alpha}},\frac{y_j^\alpha}{\|\bm{y}\|_\alpha^{\alpha}}\right) \alpha^2 y_i^{\alpha-1}y_j^{\alpha-1} \dee\bm{y} 
        = \nu(\dee \bm{y}).
    \end{equation*}
    Finally, we transform to $L_\alpha$ polar coordinates $(s,\bm{\phi})$ with $s=\|\bm{y}\|_\alpha$ and $\bm{\phi}=(\phi_i,\phi_j)=\bm{y}/s$. By (CITE Lemma 1.1 in Song and Gupta (1997)), the Jacobian is $s(1 - \phi_i^\alpha)^{(1-\alpha)/a} = s\phi_j^{1-\alpha}$. We now have
    \begin{align*}
        \nu(\dee \bm{y}) 
        &= \left[s^{\alpha}\right]^{-3} h_1\left(\phi_i^\alpha,\phi_j^\alpha \right) \alpha^2 (s\phi_i)^{\alpha-1}(s\phi_j)^{\alpha-1} s\phi_j^{1-\alpha} \,\dee s \,\dee\bm{\phi} \\
        &= \alpha s^{-\alpha-1} \alpha \phi_i^{\alpha-1} h_1\left(\phi_i^\alpha,\phi_j^\alpha \right) \,\dee s\,\dee \bm{\phi} \\
        &= \alpha s^{-\alpha-1} h_\alpha(\bm{\phi}) \,\dee s\,\dee \bm{\phi} \\
        &= \nu(\dee s \times \dee \bm{\phi}),
    \end{align*}
    where $h_\alpha(\bm{\phi}):=\alpha \phi_i^{\alpha-1} h_1\left(\phi_i^\alpha,\phi_j^\alpha \right)$. The final step is to compute $\tilde{h}_\alpha$ by projecting the density $h_\alpha$, which lives on $\mathbb{S}_{+(\alpha)}^1$, down to $[0,1]$. Writing $\bm{\phi}$ as $(\phi,(1-\phi^\alpha)^{1/\alpha})$ gives
    \begin{equation*}
        \tilde{h}_{\alpha}(\phi) = h_\alpha\left(\left(\phi, (1-\phi^\alpha)^{1/\alpha}\right)\right) = \alpha \phi^{\alpha-1}h_1\left((\phi^\alpha, 1-\phi^\alpha)\right) = \alpha \phi^{\alpha-1} \tilde{h}_1(\phi^\alpha).
    \end{equation*}
:::

In the trivial case $\alpha=1$ the formula reduces to $\tilde{h}_1(\theta) = \tilde{h}_1(\theta)$, as one would hope. Setting $\alpha=2$ yields $\tilde{h}_2(\theta) = 2\theta\tilde{h}_1(\theta^2)$, which matches the formula gives in @fixSimultaneousAutoregressiveModels2021. Note that $\tilde{h}_\alpha$ is well-defined (i.e. is a normalised density), since
\begin{equation*}
    \int_0^1 \tilde{h}_\alpha (\theta)\,\dee \theta = \int_0^1 \alpha\theta^{\alpha-1}\tilde{h}_1 (\theta^\alpha )\,\dee \theta = \int_0^1 \tilde{h}_1(\phi)\,\dee \phi = 1.
\end{equation*}
We now apply the transformation formula to express the TPDM for any $\alpha\geq 1$ in terms of the angular density $\tilde{h}_1$.

:::{#prp-tpdm-h1-formula}
Using the notation of @lem-angular-density-transformation, the off-diagonal entry in the TPDM of $\bm{X}$ is
\begin{equation}\label{eq-tpdm-h1-formula}
    \sigma_{ij} = m \int_0^1 \sqrt{u(1-u)} \, \tilde{h}_1(u)\,\dee \phi.
\end{equation}
:::

::: {.proof}
The relation between the normalised measure $H_\alpha$ and the measure $H$ in @def-tpdm-alpha is $H_\alpha=m^{-1}H$, where $m$ is the mass of $H$. Therefore, \eqref{eq-tpdm-alpha} can be equivalently restated as
    \begin{equation*}
        \sigma_{ij} = m \int_{\mathbb{S}_{+(\alpha)}} \theta_i^{\alpha/2}\theta_j^{\alpha/2} \,\dee H_\alpha (\bm{\theta})
    \end{equation*}
    Rewriting this in terms of the angular density and re-parametrising yields
    \begin{align*}
        \sigma_{ij} 
        &= m \int_{\mathbb{S}_{+(\alpha)}} \theta_i^{\alpha/2}\theta_j^{\alpha/2} h_{\alpha}(\bm{\theta})\,\dee \bm{\theta} \\
        &= m \int_{\mathbb{S}_{+(\alpha)}} \theta_i^{\alpha/2}[(1-\theta_i^\alpha)^{1/\alpha}]^{\alpha/2} h_\alpha(\bm{\theta})\,\dee \bm{\theta} \\
        &= m \int_0^1 \theta^{\alpha/2} (1-\theta^\alpha)^{1/2} \tilde{h}_\alpha(\theta)\,\dee \theta.
    \end{align*}
    Finally, we apply @lem-angular-density-transformation and substitute $u=\theta^\alpha$ to obtain the final result
    \begin{equation*}
        \sigma_{ij} 
        = m \int_0^1 \theta^{\alpha/2} (1-\theta^\alpha)^{1/2} \alpha \theta^{\alpha-1}\tilde{h}_1(\theta^\alpha)\,\dee \theta 
        = m \int_0^1 \sqrt{u(1-u)}\,\tilde{h}_1(u)\,\dee \phi.
    \end{equation*}
:::

This means the TPDM is invariant under the choice of $\alpha$. (Later we will show that the quantity $m$ does not depend on $\alpha$ when the margins are pre-processed in a suitable way.) In principle we are free to leave $\alpha$ unspecified or set at some arbitrary value. Typically we will choose $\alpha=2$, since much of the original theory and accompanying methods were developed in this setting. It also eases the notation by allowing us to omit the cumbersome $\alpha/2$ exponents. The exception to this is in Chapter XXX, where we will choose $\alpha=1$. This affords us the ability to leverage statistical theory from the field of compositional data analysis, which pertains to random vectors on $\mathbb{S}_{+(1)}^{d-1}$.


### Interpretation of the TPDM entries

Instead of defining the TPDM entry-wise, one can write it more succinctly as
\begin{equation}\label{eq-tpdm-covariance-form}
    \Sigma = \mathbb{E}_{\bm{\Theta}\sim H}\left[\bm{\Theta}^{\alpha/2}(\bm{\Theta}^{\alpha/2})^T\right],
\end{equation}
Not coincidentally, this bears a striking resemblance to the definition of a covariance matrix in the non-extreme setting. Recall that the covariance matrix represents the second-order (central) moment of a random vector. Its diagonal entries correspond to the scale (variance) of the components. Its off-diagonal entries summarise the strength of association (unnormalised correlation) between pairs of variables. The TPDM entries can be interpreted analogously, except the notions of scale and association are adapted to refer to properties of the joint distributional tail.

:::{#def-scale}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with fixed normalisation sequence $b_n$. For $i=1,\ldots,d$, the scale of $X_i$ is defined as [@kluppelberg_estimating_2021]
    \begin{equation}
        \mathrm{scale}(X_i) = \left[\int_{\mathbb{S}_+^{d-1}}\theta_i^\alpha\,\dee H(\bm{\theta})\right]^{1/\alpha}.
    \end{equation}
:::

The quantity is so called because it yields information about the scale of the marginal distributions, since
\begin{equation*}
    \lim_{n\to\infty} n\mathbb{P}(b_n^{-1}X_i > x)
    = \int_{\mathbb{S}_{+(\alpha)}^{d-1}}\int_{x/\theta_i}^\infty \alpha r^{-\alpha-1}\,\dee r\,\dee H(\bm{\theta}) 
    = \int_{\mathbb{S}_{+(\alpha)}^{d-1}} [r^{-\alpha}]_{\infty}^{x/\theta_i} \,\dee H(\bm{\theta}) 
    = x^{-\alpha} [\mathrm{scale}(X_i)]^\alpha,
\end{equation*}
Moreover, it behaves as a measure of scale since for any $c>0$,
\begin{equation*}
    \mathrm{scale}(cX_i) = \left[\frac{\lim_{n\to\infty} n\mathbb{P}(b_n^{-1}cX_i > x)}{x^{-\alpha}}\right]^{1/\alpha} = \left[c^\alpha\frac{\lim_{n\to\infty} n\mathbb{P}(b_n^{-1}X_i > x/c)}{(x/c)^{-\alpha}}\right]^{1/\alpha}=c\cdot \mathrm{scale}(X_i).
\end{equation*}

The relation between the diagonal entries and the marginal scales is $\mathrm{scale}(X_i)=\sigma_{ii}^{1/\alpha}$. 

:::{#lem-tpdm-scale}
Assume $\bm{X}$ is pre-processed to have Fréchet margins \eqref{eq-alpha-frechet}. Then

1. For all $i=1,\ldots,d$, $\sigma_{ii}=1$.
2. The trace of the TPDM is $\mathrm{trace}(\Sigma)=d$.
3. The mass of the angular measure is $m=d$.
:::

::: {.proof}
For (i), we simply substitute the Fréchet survivor function, yielding 
        \begin{equation}
        \sigma_{ii} 
        = \mathrm{scale}(X_i)^\alpha 
        = \frac{\lim_{n\to\infty} n\mathbb{P}(X_i > n^{1/\alpha}x)}{x^{-\alpha}} 
        = \frac{\lim_{n\to\infty} n \left\lbrace 1 - \exp\left[-(n^{1/\alpha}x)^{-\alpha}\right] \right\rbrace}{x^{-\alpha}} 
        = 1.
    \end{equation}
    Statement (ii) is an obvious corollary of (i). For (iii), recall that the norm index $p$ matches the tail index $\alpha$ and note that $\sum_{i=1}^d \theta_i^\alpha = \|\bm{\theta}\|_\alpha^\alpha = 1$ for any $\bm{\theta}\in\mathbb{S}_{+(\alpha)}^{d-1}$. 
    It follows that
    \begin{equation}\label{eq-H-mass}
    \mathrm{trace}(\Sigma)
    = \sum_{i=1}^d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^\alpha \,\dee H(\bm{\theta}) 
    =  \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \sum_{i=1}^d \theta_i^\alpha \,\dee H(\bm{\theta}) 
    = \int_{\mathbb{S}_{+(\alpha)}^{d-1}}\dee H(\bm{\theta})
    = m.
\end{equation}
Combining (ii) and \eqref{eq-H-mass} completes the proof.
:::

This result means that standardising to Fréchet margins is akin to working with re-scaled variables with unit variance in the non-extremes setting. The appropriate analogue then becomes the correlation rather than covariance matrix.

Comparing @def-tpdm with @def-edm reveals that the TPDM's off-diagonal entries are pairwise EDMs. Thus the interpretation of these entries is inherited from the EDM: $X_i$ and $X_j$ are asymptotically independent if and only $\sigma_{ij}=\sigma_{ji}=0$; the magnitude of $\sigma_{ij}>0$ reveals the strength of tail dependence between $X_i$ and $X_j$. 

In summary, the diagonal entries pertain to the marginal scales while the off-diagonals quantify the pairwise dependence strengths. This underlines the clear analogy between the TPDM and covariance matrices that we are familiar with in non-extreme settings. Throughout this thesis, we will employ `heatmap' plots to visualise matrices, including the TPDM. An example is provided in FIGURE XXX, which depicts a Hüsler-Reiss parameter matrix and the corresponding TPDM. The method used to derive the model TPDM is explained in the following section -- see @exm-husler-reiss-tpdm}. 

*Figure with an example TPDM.*

### TPDMs under parametric models

We now compute the TPDM for a selection of parametric models. Parametric angular densities are typically specified for the $\alpha=1$ case, i.e. with respect to standard Fréchet margins and the $L_1$-norm. Happily, @prp-tpdm-h1-formula provides the formula for calculating the TPDM from such functions. We assume Fréchet margins as in \eqref{eq-alpha-frechet}, so that we may substitute $m=2$ into \eqref{eq-tpdm-h1-formula}. We reiterate that the following expressions hold for any choice of $\alpha\geq 1$. Invariably these expressions will involve intractable integrals. The angular densities are provided by \textit{(CITE thesis entitled Inference on the Angular Distribution of Extremes.)}. 

:::{#exm-symmetric-logistic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ follows the symmetric logistic distribution with dependence parameter $\gamma\in(0,1)$. Then
    \begin{align}
        \tilde{h}_1(\theta;\gamma) &= \frac{1-\gamma}{2\gamma}[\theta(1-\theta)]^{\frac{1}{\gamma}-2}[\theta^{1/\gamma} + (1-\theta)^{1/\gamma}]^{\gamma-2}, \\
        \sigma_{ij}(\gamma) &= \frac{1-\gamma}{\gamma} \int_0^1 [u(1-u)]^{\frac{1}{\gamma}-\frac{3}{2}}[(1-u)^{1/\gamma} + u^{1/\gamma}]^{\gamma-2}\,\dee u.
    \end{align}
    The limiting cases are $\lim_{\gamma\to 0}\sigma_{ij}(\gamma) = 1$ (full asymptotic dependence) and $\lim_{\gamma\to 1}\sigma_{ij} = 0$ (asymptotic independence).
:::

:::{#exm-husler-reiss-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ follows the Hüsler-Reiss distribution with parameter matrix $\Lambda=(\lambda_{ij}^2)$. Then, 
    \begin{align}
        \tilde{h}_1(\theta;\lambda) &= \frac{\exp\left(-\lambda/4\right)}{4\lambda [\theta(1-\theta)]^{3/2}}\phi\left(\frac{1}{2\lambda}\log\left(\frac{\theta}{1-\theta}\right)\right), \\
        \sigma_{ij}(\Lambda) &= \int_0^1 \frac{\exp(-\lambda_{ij}/4)}{2\lambda_{ij} u(1-u)} \phi\left(\frac{1}{2\lambda_{ij}}\log\left(\frac{u}{1-u}\right)\right) \,\dee u.
    \end{align}
:::

The solid lines in @fig-parametric-chi-tpdm depict $\sigma_{ij}$ (blue) and $\chi_{ij}$ (red) as functions of the dependence parameter for the bivariate symmetric logistic and Hüsler-Reiss distributions. The dependence measures take different values (i.e. $\sigma_{ij}\neq\chi_{ij}$ in general) but the qualitative features of the curves are the same. In each case, the strength of association is a decreasing function of the model parameter. Perfect asymptotic dependence and asymptotic independence occur as the parameter approaches zero and its upper limit, respectively. For the Hüsler-Reiss distribution, both metrics indicate that dependence essentially vanishes beyond $\lambda\approx 3$. In order to empirically verify our analytical formulae, we overlay sample-based estimates of $\sigma_{ij}$ (blue points) and $\chi_{ij}$ (red points). Each estimate is derived from $n=5\times 10^5$ independent samples and $\alpha=2$. The data are generated using the \texttt{rmev} function in the \texttt{mev} package. Due to the abundance of samples, it is reasonable to neglect the influence of estimation error; this aspect will be examined in Section XXX. The empirical estimates of the tail dependence coefficient are taken as $\hat{\chi}_{ij}(0.9995)$. Estimates of $\sigma_{ij}$ are derived from the empirical TPDM, to be defined later. Reassuringly, the empirical estimates closely align with the curves, corroborating our formulae.

```{r background-make-fig-parametric-chi-tpdm}
#| label: fig-parametric-chi-tpdm
#| fig-cap: "True dependence strengths for the symmetric logistic (left) and Hüsler-Reiss (right) models, measured using the tail dependence coefficient (red line) and TPDM (blue line). The shaded regions represent the minimum/maximum values of empirical estimates over 10 repeated simulations using bivariate samples of size $n=5\\times 10^5$."
#| fig-scap: "Dependence $\\chi$ and $\\sigma$ for symmetric logistic and Hüsler-Reiss models."
#| fig-height: 3.5

data <- readRDS(file.path("scripts", "background", "results", "parametric_chi_tpdm_empirical.RDS")) 

p1 <- ggplot() + 
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) +
  geom_function(aes(colour = "chi"), fun = sl_chi, xlim = c(0, 1)) + 
  geom_function(aes(colour = "tpdm"), fun = sl_tpdm, xlim = c(0, 1)) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 5)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(gamma), 
       y = "Dependence strength", 
       title = "Symmetric logistic") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10))  

p2 <- ggplot() + 
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) + 
  geom_function(aes(colour = "chi"), fun = hr_chi, xlim = c(0, 3.5)) + 
  geom_function(aes(colour = "tpdm"), fun = hr_tpdm, xlim = c(0, 3.5)) + 
  scale_x_continuous(limits = c(0, 3.5), expand = c(0, 0), breaks = breaks_extended(n = 4)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(lambda), 
       y = "Dependence strength", 
       title = "Hüsler-Reiss") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10)) 

ggarrange(p1, p2, ncol = 2, common.legend = TRUE) 
``` 


:::{#exm-max-linear-tpdm}
$\Sigma=A^{\alpha/2}(A^{\alpha/2})^T$.
:::

:::{#exm-asymmetric-logistic-tpdm}
*To do. See density function in §3.1 of Beranger and Padoan (2015). Try a similar example to page 19 of Simpson thesis, e.g. trivariate with $\chi_{\{1,2,3\}}=0$ to simplify the integrals. Simpson gives formulae for $\chi_{ij}$ in this case.*
:::

### Decompositions of the TPDM

We have established that the TPDM is useful as a summary statistic for quantifying pairwise dependencies. But one could just as easily use $\chi=(\chi_{ij})$ for this purpose, so what sets the TPDM apart? The answer lies in its additional mathematical properties. In particular, it admits two types of decomposition: eigendecomposition and the completely positive decomposition \parencite{cooley_decompositions_2019}. These factorisations underpin most statistical applications of the TPDM, which will be reviewed in Section XXX.

:::{#prp-tpdm-symmetric-positive-definite}
The TPDM is symmetric and positive semi-definite [@kirilioukEstimatingProbabilitiesMultivariate2022, Proposition 2.1]. 
:::

::: {.proof}
For any $i,j=1,\ldots,d$,
    \begin{equation*}
        \sigma_{ij} = \int_{\mathbb{S}_+^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee H(\bm{\theta}) = \int_{\mathbb{S}_+^{d-1}} \theta_j^{\alpha/2} \theta_i^{\alpha/2} \,\dee H(\bm{\theta}) =\sigma_{ji}.
    \end{equation*}
    Hence $\Sigma=\Sigma^T$. For any $\bm{y}\in\R^d\setminus\{\bm{0}\}$. By \eqref{eq-tpdm-covariance-form},
    \begin{equation*}
        \bm{y}^T \Sigma \bm{y} 
        \propto \bm{y}^T \mathbb{E}_{\bm{\Theta}\sim H}[\bm{\Theta}^{\alpha/2}(\bm{\Theta}^{\alpha/2})^T] \bm{y} 
        = \mathbb{E}_{\bm{\Theta}\sim H}\left[\left(\bm{y}^T \bm{\Theta}^{\alpha/2} \right)^2 \right] \geq 0.
    \end{equation*}
:::

By standard linear algebra results, the TPDM can be decomposed as $\Sigma=UDU^T$, where $D\in\R^{d\times d}$ is a diagonal matrix of eigenvalues $\lambda_1 \geq \ldots \geq \lambda_d \geq 0$ and $U\in\R^{d\times d}$ is an orthogonal matrix whose columns are the corresponding eigenvectors $\bm{u}_1,\ldots,\bm{u}_d\in\R^d$. 

:::{#def-cp}
A matrix $M\in\R^{d\times d}$ is completely positive if there exists a matrix $B\in\R_+^{d\times q}$ such that $M=BB^T$.
:::

:::{#prp-tpdm-symmetric-positive-definite}
The TPDM is completely positive. [@kirilioukEstimatingProbabilitiesMultivariate2022, Proposition 2.2(ii)]
:::

::: {.proof}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with angular measure $H$ and TPDM $\Sigma$. By Proposition 5 in @fougeresDenseClassesMultivariate2013, there exists a sequence of matrices $\{A_q\in\R_+^{d\times q}:q\geq 1\}$ such that $H_q\overset{v}{\to} H$, where $H_q$ is the angular measure of $\bm{X}_q\sim\mathrm{MaxLinear}(A_q,\alpha)$. For $q\geq 1$, the TPDM of $\bm{X}_q$ is $\Sigma_q=A_q^{\alpha/2}(A_q^{\alpha/2})^T$ by @exm-max-linear-tpdm. By construction, $\{\Sigma_q : q\geq 1\}$ is a sequence of completely positive matrices. By Theorem 2.2 in CITE Berman \& Shaked-Monderer (2003), the limit $\Sigma=\lim_{q\to\infty}\Sigma_q$ is also completely positive.
:::

@kirilioukEstimatingProbabilitiesMultivariate2022 provide an iterative algorithm for constructing completely positive factorisation of an arbitrary TPDM. \textit{Summarise the algorithm and give details about CP decomposition, e.g. estimating $q$.}

### The empirical TPDM}

:::{#def-empirical-tpdm}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ on Fréchet margins \eqref{eq-alpha-frechet} and let $H$ be the angular measure with respect to $\|\cdot\|_\alpha$ and normalising sequence $b_n=n^{1/\alpha}$. Let $\bm{X}_1,\ldots,\bm{X}_n$ be an iid sample of $\bm{X}$. The empirical TPDM is the $d\times d$ matrix
    \begin{equation}\label{eq-empirical-tpdm}
        \hat{\Sigma} = (\hat{\sigma}_{ij}), \qquad \hat{\sigma}_{ij} = \int_{\mathbb{S}_+^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee \hat{H}(\bm{\theta})=\frac{d}{k}\sum_{l=1}^k \Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2}.
    \end{equation}
:::

Note that the empirical TPDM implicitly depends on the customary tuning parameter $k$ -- or equivalently a radial threshold $t>0$ -- via the empirical angular measure. 

#### Finite-sample properties

:::{#prp-empirical-tpdm-completely-positive}
The empirical TPDM is completely positive.
:::

::: {.proof}
Consider the matrix
\begin{equation}\label{eq-empirical-A}
    \hat{A} := \left(\frac{d}{k}\right)^{1/\alpha}\left(\bm{\Theta}_{(1)}, \ldots, \bm{\Theta}_{(k)}\right) \in\R_+^{d\times k}.
\end{equation}
Note that $A$ is $d\times k$ with non-negative entries. Then
\begin{equation}
    \hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T = \frac{d}{k} \sum_{i=1}^k \bm{\Theta}_{(i)}^{\alpha/2} \left(\bm{\Theta}_{(i)}^{\alpha/2}\right)^T = \hat{\Sigma}.
\end{equation}
:::

:::{#prp-empirical-tpdm-symmetric-positive-definite}
The empirical TPDM is symmetric and positive semi-definite.
:::

::: {.proof}
Let $\hat{A}$ be as in \eqref{eq-empirical-A}. Then for any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation}
    \bm{y}^T\hat{\Sigma}\bm{y} = \bm{y}^T\hat{A}\hat{A}^T\bm{y} = \|\hat{A}^T\bm{y}\|_2^2 \geq 0.
\end{equation}
Since $\mathrm{rank}(\hat{\Sigma})=\mathrm{rank}(\hat{A}\hat{A}^T)=\mathrm{rank}(\hat{A})$, the empirical TPDM is positive definite if the columns of $\hat{A}$ are linearly independent.
:::

#### Asymptotic properties

:::{#prp-empirical-tpdm-normality-entries}
Assume the conditions of @thm-clt-extremes hold. Then the entries of $\hat{\Sigma}$ are consistent and asymptotically normal, that is, for any $i,j=1,\ldots,d$,
    \begin{equation}
        \sqrt{k}(\hat{\sigma}_{ij} - \sigma_{ij}) \to\mathrm{N}(0,\nu_{ij}^2),
    \end{equation}
    where
    \begin{equation*}
        \nu_{ij}^2 := \mathrm{Var}_{\bm{\Theta}\sim H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}).
    \end{equation*}
:::  

::: {.proof}
Follows by application of @thm-clt-extremes with the continuous function $f(\bm{\theta})=\theta_i^{\alpha/2}\theta_j^{\alpha/2}$.
:::

Adopting the notation of @prp-tpdm-h1-formula, the asymptotic variance can be expressed in terms of the angular density $\tilde{h}_1$ of $(X_,X_j)$. Using $\mathrm{Var}(Y)=\mathbb{E}[Y^2]-\mathbb{E}[Y]^2$, we have
\begin{equation*}
    \nu_{ij}^2 
    = m^2 \int_{\mathbb{S}_{+(\alpha)}^{d-1}}(\theta_i\theta_j)^\alpha \,\dee H_\alpha(\bm{\theta}) - \sigma_{ij}^2 
    = m^2 \int_0^1 \theta^\alpha (1-\theta^\alpha) \tilde{h}_\alpha(\theta) \,\dee \theta - \sigma_{ij}^2.
\end{equation*}
Substituting $u=\theta^\alpha$ and using @prp-tpdm-h1-formula gives the final expression
\begin{equation}\label{eq-nu-squared-h1-formula}
    \nu_{ij}^2 = m^2 \int_0^1 u(1-u)\,\tilde{h}_1(u)\,\dee u - \left[m\int_0^1 \sqrt{u(1-u)}\,\tilde{h}_1(u)\,\dee u \right]^2.
\end{equation}
The asymptotic distribution of $\hat{\sigma}_{ij}$ does not depend on $\alpha$. By \autoref{prop-empirical-tpdm-normality-entries} we have that
\begin{equation*}
    \lim_{n\to\infty}\mathbb{P}\left[\hat{\sigma}_{ij} \in \left(\sigma_{ij}-z_{\beta/2}\frac{\nu_{ij}}{\sqrt{k}},\sigma_{ij}+z_{\beta/2}\frac{\nu_{ij}}{\sqrt{k}}\right)\right] = 1-\beta,
\end{equation*}
where $z_{\beta/2}=\Phi^{-1}(1-\beta/2)$. If the angular density of $(X_i,X_j)$ is known, then the bounds of the interval can be computed and their values do not depend on $\alpha$.

:::{exm-symmetric-logistic-asymptotic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is symmetric logistic with dependence parameter $\gamma=0.6$. Using the density function in @exm-symmetric-logistic-tpdm and the formulae \eqref{eq-tpdm-h1-formula} and \eqref{eq-nu-squared-h1-formula}, we obtain by numerical integration $\sigma_{ij} \approx 0.760$ and $\nu_{ij}^2 = 0.065$ for all $i\neq j$. For sufficiently large $n$, 
\begin{equation*}
    \mathbb{P}\left[\hat{\sigma}_{ij} \in \left(0.847 \pm \frac{1.96 \sqrt{0.0358}}{\sqrt{k}}\right)\right] \approx 0.95.
\end{equation*}
    For example, setting $n=10^4$ and $k=\sqrt{n}$ yields $\mathbb{P}(0.710 < \hat{\sigma}_{ij} < 0.810 )\approx 0.95.$
:::

The following result generalises asymptotic normality of the empirical TPDM to the entire matrix, rather than just individual entries.

:::{#prp-empirical-tpdm-normality}
$\hat{\Sigma}$ possesses consistency and asymptotically normality. By this, we mean that the upper-half vectorised empirical TPDM
        \begin{equation*}
            \hat{\bm{\sigma}}:=\mathrm{vecu}(\hat{\Sigma}) := (\hat{\sigma_{12}},\hat{\sigma}_{13},\ldots,\hat{\sigma}_{1d},\hat{\sigma}_{23},\ldots,\hat{\sigma}_{2d},\ldots,\hat{\sigma}_{d-1,d})
        \end{equation*}
        is asymptotically multivariate normal,
        \begin{equation*}
            \sqrt{k}(\hat{\bm{\sigma}}-\bm{\sigma}) \to N(\bm{0},V),
        \end{equation*}
        where $\bm{\sigma}:=\mathrm{vecu}(\Sigma)$ is defined analogously to $\hat{\bm{\sigma}}$. The diagonal and off-diagonal entries of the ${d \choose 2}\times{d \choose 2}$ asymptotic covariance matrix $V=(v_{ij,lm})$ are given by
        \begin{equation}
            v_{ij,lm} := \lim_{k\to\infty}k\mathrm{Cov}(\hat{\sigma}_{ij},\hat{\sigma}_{lm}) = 
            \begin{cases}
            \nu_{ij}^2, & (i,j) = (l,m),\\
            \rho_{ij,lm} & \text{otherwise},
            \end{cases}
        \end{equation}
        where
        \begin{equation}
            \rho_{ij,lm} := \frac{1}{2}\left[\mathrm{Var}_{\bm{\Theta}\sim H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) - \nu_{ij}^2 - \nu_{lm}^2\right].
        \end{equation}
:::

::: {.proof}
We follow the proof of Theorem 5.23 in CITE Krali Thesis but adapt it to the general $\alpha$ case. By the Cramér-Wold device (CITE), it is sufficient to show asymptotic normality of $\sqrt{k}\bm{\beta}^T(\hat{\bm{\sigma}}-\bm{\sigma})$ for all $\bm{\beta}\in\R^{d\choose 2}$. For convenience, the components of $\bm{\beta}$ are indexed to match the sub-indices of $\bm{\sigma}$. Then
\begin{equation*}
    \bm{\beta}^T\bm{\sigma}
        = \sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\sigma_{ij} = \mathbb{E}_{\bm{\Theta}\sim H}\left[\sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}\right] =: \mathbb{E}_{\bm{\Theta}\sim H}[g(\bm{\Theta};\bm{\beta})],
    \end{equation*}
where 
\begin{equation*}
        g(\bm{\theta};\bm{\beta}):=\sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\theta_i^{\alpha/2}\theta_j^{\alpha/2}
\end{equation*}
The corresponding empirical estimator is
\begin{equation*}
        \hat{\mathbb{E}}_{\bm{\Theta}\sim H}[g(\bm{\Theta};\bm{\beta})] = \frac{m}{k} \sum_{l=1}^{k}\sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2} = \sum_{i=1}^d\sum_{j=i}^d \beta_{ij}\left(\frac{m}{k}\sum_{l=1}^k\Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2} \right) = \bm{\beta}^T\hat{\bm{\sigma}}.
\end{equation*}
Noting that $g(\cdot\,;\bm{\beta})$ is continuous and applying \autoref{prop-clt-extremes}, we have
\begin{equation*}
        \sqrt{k}\bm{\beta}^T (\hat{\bm{\sigma}}-\bm{\sigma}) = \sqrt{k}\left(\hat{\mathbb{E}}_{\bm{\Theta}\sim H}[g(\bm{\Theta};\bm{\beta})] - \mathbb{E}_{\bm{\Theta}\sim H}[g(\bm{\Theta};\bm{\beta})]\right) \to N(0,v(\bm{\beta})).
\end{equation*}
where $v(\bm{\beta}):=\mathrm{Var}_{\bm{\Theta}\sim H}(g(\bm{\Theta};\bm{\beta}))$. The asymptotic normality of $\hat{\bm{\sigma}}$ follows by the Cramér-Wold device. The diagonal elements of the covariance matrix $V$ are as in @prp-empirical-tpdm-normality-entries. The off-diagonal entries are given by
\begin{align*}
        2\mathrm{Cov}\left(\sqrt{k}(\hat{\sigma}_{ij} - \sigma_{ij}),\sqrt{k}(\hat{\sigma}_{lm} - \sigma_{lm})\right) 
        &=2k\,\mathrm{Cov}(\hat{\sigma}_{ij}, \hat{\sigma}_{lm}) \\
        &= k\left[\mathrm{Var}(\hat{\sigma}_{ij} + \hat{\sigma}_{lm}) - \mathrm{Var}(\hat{\sigma}_{ij}) - \mathrm{Var}(\hat{\sigma}_{lm})\right] \\
        &\to \mathrm{Var}_{\bm{\Theta}\sim H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) - \nu_{ij}^2 - \nu_{lm}^2.
\end{align*}
:::

Note that in the vectorisation step we only include the strictly upper triangular elements of the TPDM. One could include the diagonal entries and the result still holds, but the limiting distribution would be degenerate. The reason for this is that the diagonal TPDM entries sum to $d$, so $V$ would be singular. The following example illustrates a rare case where it is possible to compute the exact asymptotic distribution of $\mathrm{vecu}(\hat{\Sigma})$.

:::{exm-max-linear-asymptotic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is max-linear with $q$ factors and parameter matrix $A$. Then, for any $i,j=1,\ldots,d$, we have $\sigma_{ij}=\sum_{l=1}^q a_{il}^{\alpha/2}a_{jl}^{\alpha/2}$ and
\begin{equation*}
        \nu_{ij}^2 
        = d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} (\theta_i\theta_j)^\alpha \,\dee H(\bm{\theta}) - \sigma_{ij}^2 
        = d\sum_{s=1}^q \|\bm{a}_s\|_\alpha^\alpha \left(\frac{a_{is}a_{js}}{\|\bm{a}_s\|_\alpha^2}\right)^\alpha - \sigma_{ij}^2 
        = d \sum_{s=1}^q \frac{(a_{is}a_{js})^\alpha}{\|\bm{a}_s\|_\alpha^\alpha} - \sigma_{ij}^2.
\end{equation*}
For any pair of upper-triangular index pairs $(i,j)$ and $(l,m)$, we have
\begin{align*}
     \mathrm{Var}_{\bm{\Theta}\sim H} & (\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) \\
     &= d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} [(\theta_i\theta_j)^\alpha + 2(\theta_i\theta_j\theta_l\theta_m)^{\alpha/2} +(\theta_l\theta_m)^\alpha] \,\dee H(\bm{\theta}) - \left[ \sigma_{ij} + \sigma_{lm} \right]^2 \\
     &= d \sum_{s=1}^q \frac{(a_{is}a_{js})^\alpha + 2(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2} + (a_{ls}a_{ms})^\alpha}{\|\bm{a}_s\|_\alpha^\alpha} - \left[ \sigma_{ij} + \sigma_{lm} \right]^2 \\
     &= \nu_{ij}^2 + \nu_{lm}^2 + d \sum_{s=1}^q \frac{2(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2}}{\|\bm{a}_s\|_\alpha^\alpha} - 2\sigma_{ij}\sigma_{lm}
\end{align*}
and therefore
\begin{equation*}
    2\rho_{ij,lm}
        = d \sum_{s=1}^q \frac{2(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2}}{\|\bm{a}_s\|_\alpha^\alpha} - 2\sigma_{ij}\sigma_{lm}.
\end{equation*}
The expressions for $\nu_{ij}^2$ and $\rho_{ij,lm}$ can be summarised as
\begin{equation}\label{eq-max-linear-V}
        v_{ij,lm} = d \sum_{s=1}^q \frac{(a_{is}a_{js}a_{ls}a_{ms})^{\alpha/2}}{\|\bm{a}_s\|_\alpha^\alpha} - \sigma_{ij}\sigma_{lm}.
\end{equation}
Suppose $A$ is as shown in Figure XXX (left). This matrix has $q=8$ columns and $d=4$ rows summing to unity. The TPDM of $\bm{X}=A\times_{\max}\bm{Z}$ (see \eqref{eq-max-linear-X}) is displayed in the middle plot. The right-hand plot shows the asymptotic covariance matrix $V$, calculated using \eqref{eq-max-linear-V}. The number of rows/columns in $V$ is ${4\choose 2}=6$. Figure XXX shows pairwise plots of $(\hat{\sigma}_{ij},\hat{\sigma}_{lm})$ derived from 1000 samples of size $n=10^4$ with $k=\sqrt{n}$. First, consider the diagonal sub-panels. These depict the empirical (red histogram) and asymptotic distributions (blue curves) of $\hat{\sigma}_{ij}$. Specifically, each blue curve represents the density function of $\mathrm{N}(\sigma_{ij}, \nu_{ij}^2/k)$ random variable. The distributions are a close match. We conclude that $n$ is sufficiently large for the asymptotic approximation suggested by @prp-empirical-tpdm-normality-entries to hold. Now we exaine the numerical values printed in the upper triangular panels. The blue numbers are the true entries $v_{ij,lm}$ of $V$. The red numbers are sample-based estimates $\hat{v}_{ij,lm}$ of $v_{ij,lm}$, i.e. the sample covariance of $\hat{\sigma}_{ij}$ and $\hat{\sigma}_{lm}$, multiplied by $k$. For all pairs these values show good agreement. Finally, consider the scatter plots in the lower triangular portion of the plot. The grey points represent realisations of $(\hat{\sigma}_{ij},\hat{\sigma}_{lm})$ over the 1000 simulations. By @prp-empirical-tpdm-normality, for $n$ sufficiently large,
\begin{equation*}
        \begin{pmatrix}
            \hat{\sigma}_{ij} \\ \hat{\sigma}_{lm}
        \end{pmatrix} \mathrel{\dot\sim} \mathrm{N}\left(
        \begin{pmatrix}
            \sigma_{ij} \\ \sigma_{lm}
        \end{pmatrix},
        \frac{1}{k}\begin{pmatrix}
            \nu_{ij}^2 & \rho_{ij,lm} \\ \rho_{ij,lm} & \nu_{lm}^2
        \end{pmatrix}\right).
\end{equation*}
The blue ellipses are the true 95\% confidence ellipses centred at the true TPDM values (blue crosses). The angle of the ellipse relates to the association $\rho_{ij,lm}$ between $\hat{\sigma}_{ij}$ and $\hat{\sigma}_{lm}$, while the lengths of the major and minor axes are dictated by the variances $\nu_{ij}^2,\nu_{lm}^2$. The red ellipses and red crosses represent the sample-based 95\% confidence region and sample mean, respectively. \textit{Conclusions and comments.} 
:::

```{r make-fig-max-linear-example-A-Sigma-V}
#| label: fig-max-linear-example-A-Sigma-V
#| fig-cap: "A randomly generated max-linear parameter matrix $A$ with $d=4$ and $q=12$ (top), the corresponding TPDM $\\Sigma$ (bottom left), and the asymptotic covariance matrix $V$ of the empirical TPDM (bottom right)."
#| fig-scap: "Max-linear parameter matrix, TPDM, and asymptotic covariance matrix."
#| fig-height: 6

data <- readRDS("scripts/background/results/max-linear-asymptotic-tpdm.RDS")

p1 <- plot_tpdm(data$A, y_labels = FALSE)
p2 <- plot_tpdm(data$Sigma)
p3 <- plot_tpdm_eigen(data$V)

ggarrange(p1, ggarrange(p2, p3, ncol = 2), nrow = 2)
```

```{r make-fig-max-linear-example-ggpairs}
#| label: fig-max-linear-example-ggpairs
#| fig-cap: "Pairs plot illustrating asymptotic normality of the empirical TPDM. Based on 1,000 empirical TPDMs estimated from samples with $n=10^4$ and $k=100$. The data are generated from a max-linear model with parameter matrix $A$ as in @fig-max-linear-example-A-Sigma-V. All panels: red represents the empirical quantity based on the 1,000 repeated simulations; blue represents the theoretical limit based on asymptotic normality. Diagonal panels: the distribution (histogram or density function) of $\\hat{\\sigma}_{ij}$. Lower triangular panels: pairwise scatter plots of $(\\hat{\\sigma}_{ij},\\hat{\\sigma}_{lm})$ (grey points) along with the mean (crosses) and the 95\\% data ellipse. Upper triangular panels: the entries $v_{ij,lm}$ of $V$."
#| fig-scap: "Pairs plot illustrating asymptotic normality of empirical TPDM."
#| fig-height: 6
#| warning: false
#| message: false

tpdm_max_linear_ggpairs(data = data$emp_V, Sigma = data$Sigma, V = data$V, k = data$k)
```


## Existing applications and extensions of the TPDM

The general goal of this thesis is to develop novel statistical applications of the TPDM for analysing extremal dependence. Before outlining our contributions, it seems logical to first familiarise the reader with existing TPDM-based methods in the literature. Our methods will either build upon these (e.g. compositional PCA in Chapter XXX) or address gaps in the field.

Our survey divides TPDM-related tools into four categories: principal components analysis (PCA), clustering, model fitting, and miscellaneous. The PCA methods leverage the TPDM eigendecomposition to perform dimension reduction. The extremal dependence structure is thereby represented by a low-dimensional object, facilitating exploratory analysis [@jiangPrincipalComponentAnalysis2020; @russellAnalyzingDependenceMatrices2018; @szemkusSpatialPatternsIndices2024]


and generation of synthetic extreme events [@rohrbeckSimulatingFloodEvent2023]. The clustering techniques use the TPDM to partition a collection of random variables into groups according to asymptotic (in)dependence [@fomichovSphericalClusteringDetection2023; @richardsModernExtremeValue2024]. When applied as a preliminary step, this splits a high-dimensional problem into several `independent', low-dimensional, sub-problems. The model fitting section explains how the TPDM can be used to aid inference for the max-linear model [@fixSimultaneousAutoregressiveModels2021; @kirilioukEstimatingProbabilitiesMultivariate2022]. Fitting a parametric model permits straightforward estimation of tail event probabilities. We conclude with a summary of other applications and extensions of the TPDM, such as in time series [@mhatreTransformedLinearModelsTime2021] and graphical models [@gongPartialTailCorrelationCoefficient2024; @leePartialTailCorrelation2023].

### Principal component analysis (PCA) for extremes

:::{#def-sparsity-i}
The support of the angular measure has dimension $p^\star \ll d$.
:::

This means the angular measure can be represented by a low-dimensional object, prompting the application of dimension reduction methods. 

#### PCA in general finite-dimensional Hilbert spaces

In classical multivariate analysis, principal component analysis (PCA) is the flagship method for reducing the dimension of a random vector. PCA identifies linear subspaces that minimise the distance between the data and its low-dimensional projections.

PCA revolves around an underlying algebraic-geometric structure. Specifically, PCA assumes one is working in a Hilbert space $\mathcal{H}$. Without this theoretical foundation, it is meaningless to speak of principal components as orthogonal basis vectors or consider low-rank reconstructions as unique projections onto a subspace. A Hilbert space comprises a $d$-dimensional vector space with operations $\oplus$ and $\odot$ endowed with an inner product $\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}$. The induced norm and metric are $\|\cdot\|_{\mathcal{H}}=\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}^{1/2}$ and $d_{\mathcal{H}}(\bm{x},\bm{y})=\|\bm{x}\ominus\bm{y}\|_{\mathcal{H}}$, respectively. In most applications $\mathcal{H}=\R^d$ with the usual Euclidean geometry. This thesis will additionally consider PCA in alternative spaces, including $\R_+^d$ and $\mathbb{S}_{+(1)}^{d-1}$. However, in each case, the Hilbert space in question will be isometric to the usual Euclidean space $(\R^d, \left\langle\cdot,\cdot\right\rangle)$. That is, there exists an isomorphism $h:\mathcal{H}\to\R^d$ such that for any $\bm{x},\bm{y}\in\mathcal{H}$,
\begin{equation*}
    \left\langle\bm{x},\bm{y}\right\rangle_{\mathcal{H}} = \left\langle h(\bm{x}),h(\bm{y})\right\rangle, \qquad \|\bm{x}\ominus \bm{y}\|_{\mathcal{H}} = \|h(\bm{x})-h(\bm{y})\|_2.
\end{equation*}
We present PCA for random vectors in $\R^d$, with the understanding that the data may have undergone an isometric transformation in pre-processing and outputs may need to be back-transformed to lie in the original space. This transform/back-transform approach is equivalent to conducting the analysis in the original space with appropriately generalised notions of mean, variance, etc. [@pawlowsky-glahnGeometricApproachStatistical2001].

\begin{table}[]
\small
\begin{tabular}{@{}llll@{}}
\toprule
$\mathcal{H}$ & $\R^d$ & $\R_+^d$ @cooleyDecompositionsDependenceHighdimensional2019 & $\mathbb{S}_{+(1)}^{d-1}$ @aitchisonPrincipalComponentAnalysis1983 \\ \midrule
$h:\mathcal{H}\to\R^d$ & $h(\bm{x})=\bm{x}$ & $h(\bm{x}) = \tau^{-1}(\bm{x}) = \log[\exp(\bm{x})-1]$ & $h(\bm{x})=\mathrm{clr}(\bm{x})=\log[\bm{x}/\bar{g}(\bm{x})]$ \\
$h^{-1}:\R^d\to\mathcal{H}$ & $h^{-1}(\bm{y})=\bm{y}$ & $h^{-1}(\bm{y}) = \tau(\bm{y}) = \log[1+\exp(\bm{y})]$ & $h^{-1}(\bm{y}) = \mathrm{clr}^{-1}(\bm{y})=\mathcal{C}\exp(\bm{y})$ \\
$\bm{x}\oplus\bm{y}$ & $\bm{x} + \bm{y}$ & $\tau[\tau^{-1}(\bm{x})+\tau^{-1}(\bm{y})]$ & $\mathcal{C}(x_1y_1,\ldots,x_dy_d)$ \\
$\alpha\odot\bm{x}$ & $\alpha \bm{x}$ & $\tau[\alpha \tau^{-1}(\bm{x})]$ & $\mathcal{C}(x_1^\alpha,\ldots,x_d^\alpha)$ \\
$\left\langle \bm{x},\bm{y}\right\rangle_{\mathcal{H}}$ & $\sum_{i=1}^d x_iy_i$ & $\sum_{i=1}^d \tau^{-1}(x_i)\tau^{-1}(y_i)$ & $\sum_{i=1}^d \log[x_i/\bar{g}(\bm{x})]\log[y_i/\bar{g}(\bm{x})]$ \\ \bottomrule
\end{tabular}
\end{table}

Suppose $\bm{Y}=(Y_1,\ldots,Y_d)$ is a random vector in $\R^d$ satisfying $\mathbb{E}[\|\bm{Y}\|_2^2]<\infty$. Let $\bm{Y}_1,\ldots,\bm{Y}_n$ be independent copies of $\bm{Y}$. The reconstruction error of a subspace $\mathcal{S}\subseteq\R^d$ is measured as
\begin{equation}\label{eq-pca-true-risk}
    R(\mathcal{S}) := \mathbb{E}[\|\bm{Y}-\Pi_{\mathcal{S}}\bm{Y}\|_2^2] 
\end{equation}
Fundamental to PCA are the eigenvectors $\bm{u}_1,\ldots,\bm{u}_d\in\R^d$ and respective eigenvalues $\lambda_1\geq \ldots \geq \lambda_d \geq 0$ of the positive semi-definite matrix
\begin{equation*}
    \Sigma=\mathbb{E}[\bm{Y}\bm{Y}^T].
\end{equation*}
The entries of $\Sigma$, herein referred to as the non-centred covariance matrix, are the second-order moments of $\bm{Y}$.  By a change of basis, the random vector $\bm{Y}$ may be equivalently decomposed as 
\begin{equation*}
    \bm{Y} = \sum_{j=1}^d \left\langle \bm{Y},\bm{u}_j\right\rangle \bm{u}_j.
\end{equation*}
The scores $V_j:=\left\langle \bm{Y},\bm{u}_j \right\rangle$ represent the stochastic basis coefficients when $\bm{Y}$ is decomposed into the basis $\{\bm{u}_1,\ldots,\bm{u}_d\}$. They satisfy $\mathbb{E}[V_iV_j]=\lambda_i\ind\{i=j\}$. For $1\leq p < d$, the truncated expansion 
\begin{equation*}
    \hat{\bm{Y}}^{[p]} := \sum_{j=1}^p V_j \bm{u}_j = \Pi_{\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_p\}}\bm{Y}.
\end{equation*}
produces the optimal $p$-dimensional projection of $\bm{Y}$. In other words, the subspace $\mathcal{S}_p=\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_p\}$ minimises the criterion \eqref{eq-pca-true-risk} over $\mathcal{V}_p$, the set of all linear subspaces of dimension $p$ of $\R^d$. It is the unique minimiser provided the multiplicity of $\lambda_p$ is one. The corresponding risk is determined by the eigenvalues of the discarded components via $R(\mathcal{S}_p)=\sum_{j>p}\lambda_j$. 

In practice, the covariance matrix is unknown so \eqref{eq-pca-true-risk} cannot be minimised directly. Instead we resort to an empirical risk minimisation (ERM) approach, whereby the risk is replaced by
\begin{equation}\label{eq-pca-empirical-risk}
    \hat{R}(\mathcal{S}) := \frac{1}{n} \sum_{i=1}^n \|\bm{Y}_i-\Pi_{\mathcal{S}}\bm{Y}_i\|_2^2
\end{equation} 
Minimisation of the empirical risk follows analogously based on the empirical non-centred covariance matrix 
\begin{equation*}
    \hat{\Sigma}=\frac{1}{n}\sum_{i=1}^n \bm{Y}_i\bm{Y}_i^T
\end{equation*}
and its ordered eigenpairs $(\hat{\lambda}_j,\hat{\bm{u}}_j)$ for $j=1,\ldots,d$. For $p=1,\ldots,d$ and $i=1,\ldots,n$, the rank-$p$ reconstruction of $\bm{Y}_i$ is given by
\begin{equation*}
    \hat{\bm{Y}}_i^{[p]} := \sum_{j=1}^p \hat{V}_{ij} \bm{u}_j = \Pi_{\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\}}\bm{Y},
\end{equation*}
where $\hat{V}_{ij}:=\left\langle \bm{Y}_i,\bm{u}_j\right\rangle$.
The subspace $\hat{\mathcal{S}}_p=\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_p\}$ minimises \eqref{eq-pca-empirical-risk} in $\mathcal{V}_p$; the objective at the minimum is $\hat{R}(\hat{\mathcal{S}}_p)=\sum_{j>p}\hat{\lambda}_j$.

Usually the dimension of the target subspace (if it exists) is unknown, so the number of retained components $p$ must be selected according to some criterion. At the heart of this choice is a trade-off between dimension reduction and approximation error. Selecting $p=\max\{j:\hat{\lambda}_j>0\}$ results in perfect reconstructions but the reduction in dimension will be minimal if any. Excessive compression incurs information loss and destroys key features of the data. Several criteria for selecting the number of retained components based on the eigenvalues have been proposed. These include stopping when the reconstruction error $\sum_{j>p}\hat{\lambda}_j$ is acceptably small, cutting off components with $\lambda_j<1$, or retaining components based on where the `scree plot' forms an elbow.  

If $\bm{Y}$ is mean-zero (or the $n\times d$ data matrix is column-centred in pre-processing), then $\Sigma$ is the covariance matrix of $\bm{Y}$ and the procedure is termed centred PCA. In this case, PCA can be equivalently reformulated in terms of finding low-dimensional projections that maximally preserve variance. In the non-centred case this interpretation is not valid, the projections merely maximise variability around the origin. A detailed comparison between centred PCA and non-centred PCA is conducted in @cadimaRelationshipsUncentredColumnCentred2009. They obtain relationships between and bounds on the eigenvectors/eigenvalues of the non-centred and standard covariance matrices. Based on their theoretical analysis and a series of example, they conclude that both types of PCA generally produce similar results. In particular, the leading eigenvector (up to sign and scaling) of the non-centred covariance matrix is very often close to the vector of the column means of the data matrix. Thus the first non-centred principal component essentially relates to the centre of the data.

We now return to the context of multivariate extremes. Suppose $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ has sparse angular measure $H$ and $\bm{X}_1,\ldots,\bm{X}_n$ is a sample of $\bm{X}$. There are several reasons why the  the low-dimensional structure of the angular measure cannot be identified by naively applying standard PCA to $\bm{X}_1,\ldots,\bm{X}_n$. At a practical level, the components $X_1,\ldots,X_d$ are heavy-tailed, so the requirement that second-order moments exist may be violated. The variance of an $\alpha$-regularly varying random variable is infinite if $\alpha<2$. More pertinently, standard PCA reveals relationships between variables in the centre rather than the tail of the joint distribution, because it arises from the covariance matrix. Moreover, the non-centred/centred covariance matrix captures dependence in both directions around the origin/mean, whereas we focus on extremes in a particular direction of interest (`positive'). Finally, standard PCA fails to capitalise on the probabilistic structure inherent to MRV random vectors. The one-dimensional radial component is (asymptotically) independent of the angular component. This points towards targetting dimension reduction at the angular component $\bm{\Theta}$ rather than the original vector $\bm{X}$. Indeed, the two key PCA methods of @dreesPrincipalComponentAnalysis2021 and @cooleyDecompositionsDependenceHighdimensional2019 follow this approach. Despite emerging almost simultaneously, both are essentially based on eigendecomposition of the TPDM. 

#### @dreesPrincipalComponentAnalysis2021

Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathrm{RV}_+^{d-1}(2)$ with angular measure $H$ with respect to the Euclidean norm $\|\cdot\|_2$. The aim is to identify a low-dimensional linear subspace of $\R^d$ supporting $H$. For any subspace $\mathcal{S}\subset\R^d$, define the risk
\begin{equation*}
    R(\mathcal{S}) = \mathbb{E}_{\bm{\Theta}\sim H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_2^2].
\end{equation*}
This represents the expected reconstruction error under the limit model. By assumption, there exists a linear subspace $\mathcal{S}^\star\in\mathcal{V}_{p^\star}$ of dimension $p^\star \ll d$ such that $R(\mathcal{S}^\star)=0$, and $R(\mathcal{S})>0$ for all $\mathcal{S}\in\mathcal{V}_p$ with $p<p^\star$. The angular measure is unknown, so they adopt an ERM approach following the intuition that above a sufficiently high threshold the extremal angles will lie in a neighbourhood of $\mathcal{S}^\star$. The empirical risk is defined by replacing $H$ with the empirical angular measure $\hat{H}$ based on the $k$ largest observations in norm among a sample $\bm{X}_1,\ldots,\bm{X}_n$. That is
\begin{equation*}
    \hat{R}(\mathcal{S}) 
    := \hat{\mathbb{E}}_{\bm{\Theta}\sim H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_2^2] 
    = \frac{m}{k}\sum_{i=1}^k \|\bm{\Theta}_{(i)}-\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}\|_2^2.
\end{equation*}
This setup is almost identical to classical PCA on the random vector $\bm{\Theta}$. Note that boundedness of the simplex guarantees $\mathbb{E}[\|\bm{\Theta}\|_2^2]<\infty$. Let $\Sigma=\mathbb{E}_{\bm{\Theta}\sim H}[\bm{\Theta}\bm{\Theta}^T]$ be the TPDM of $\bm{X}$ and $(\bm{u}_j,\lambda_j)$ its (ordered) eigenpairs for $j=1,\ldots,d$. Then $\mathcal{S}_p=\mathrm{span}\{\bm{u}_1,\ldots,\bm{u}_d\}$ minimises $R$ in $\mathcal{V}_p$ and $R(\mathcal{S}_p)=\sum_{j>p}\lambda_p$. Choosing $p\geq p^\star$ yields $R(\mathcal{S}_p)=0$. Analogously, the minimiser $\hat{\mathcal{S}}_p\in\mathcal{V}_p$ of $\hat{R}$ is the subspace spanned by the leading $p$ eigenvectors of the empirical TPDM $\hat{\Sigma}$. 

 @dreesPrincipalComponentAnalysis2021 derive theoretical statistical guarantees for their approach. Most importantly, they prove that the learnt subspace converges to the optimal one as the sample size increases to infinity. Provided $k(n)$ satisfies the rate conditions \eqref{eq-k-rate-conditions}, then $\hat{\mathcal{S}}_{p}\to \mathcal{S}_{p}$ in the sense that
\begin{equation*}
    \lim_{n\to\infty} \sup_{\bm{\theta}\in\mathbb{S}_{+(2)}^{d-1}} \|\Pi_{\hat{\mathcal{S}}_{p}}\bm{\theta} - \Pi_{\mathcal{S}_{p}}\bm{\theta}\|_2  = 0.
\end{equation*}
If the target dimension is chosen correctly as $p=p^\star$, then $\hat{\mathcal{S}}_{p^\star}\to \mathcal{S}^\star$. They also provide high probability bounds on $|\hat{R}(\mathcal{S})-R(\mathcal{S})|$ for fixed $n$.

Basing their approach on the angles viewed as points in $\R^d$ eases the derivation of theoretical guarantees, but creates interpretability issues. Consider $\hat{\bm{\Theta}}_i^{[p]}=\Pi_{\mathcal{S}_p}\bm{\Theta}_i$, the rank-$p$ reconstruction of an extremal angle $\bm{\Theta}_i$. Its components need not satisfy the unit-norm constraint and may even be negative. This may be remedied by shifting/normalising $\hat{\bm{\Theta}}_i^{[p]}$ appropriately, but its optimality properties will be destroyed in the process. One can also question whether Euclidean distances are an appropriate measure of angular reconstruction error; angular distances such as cosine distance may be better suited. Similarly, the hypothesis that the angular measure's low-dimensional structure manifests in a linear fashion may be unrealistic, since data in the simplex are prone to exhibit curvature @aitchisonPrincipalComponentAnalysis1983.

#### @cooleyDecompositionsDependenceHighdimensional2019

The PCA technique developed by @cooleyDecompositionsDependenceHighdimensional2019 focusses on reconstruction and exploration of extreme events in terms of the original vector $\bm{X}$. As such, their PCA is grounded on an inner product space on $\mathcal{H}=\R_+^d$, the natural sample space of the data. The vector space is based on the softplus transformation
\begin{equation*}
    \tau:\R\to \R_+, \qquad \tau(x) = \log[1+\exp(x)].
\end{equation*}
This transformation is bijective with inverse function $\tau^{-1}(y)=\log[\exp(y)-1]$. The reason for choosing this particular mapping is that it is tail-preserving, i.e. $\lim_{x\to 1}\tau(x)/x = 1$. This provides an avenue for moving between the spaces $\R^d$ and $\R_+^d$ with negligible effect on the tails.

The linear-transformed inner product space is constructed as follows. For any $\bm{x},\bm{y}\in\R_+^d$ and $\alpha\in\R$, define
\begin{align*}
    \bm{x} \oplus \bm{y} &= \tau[\tau^{-1}(\bm{x})+\tau^{-1}(\bm{y})] \\
    \alpha \odot \bm{x} &= \tau[a\tau^{-1}(\bm{x})].
\end{align*}
Then the vector space $(\R_+^d,\oplus,\odot)$ is endowed with an inner product and norm
\begin{align*}
    \left\langle \bm{x},\bm{y}\right\rangle_\tau &= \sum_{i=1}^d \tau^{-1}(x_i)\tau^{-1}(y_i) = \left\langle \tau^{-1}(\bm{x}),\tau^{-1}(\bm{y})\right\rangle \\
    \|\bm{x}\|_\tau &= \left\langle \bm{x},\bm{x}\right\rangle_\tau^{1/2} = \|\tau^{-1}(\bm{x})\|_2.
\end{align*}
The transform $\tau^{-1}$ is an isometry linking their inner product space on the positive orthant to the standard Euclidean space $\R^d$. Thus the PCA of @cooleyDecompositionsDependenceHighdimensional2019 can be equivalently formulated in $\R_+^d$ with regards to the original data in the space or in $\R^d$ using the transform/back-transform approach articulated earlier.

Suppose $\bm{X}\in\mathrm{RV}_+^d(\alpha)$ has TPDM $\Sigma$. Denote the ordered eigenpairs of $\Sigma$ in $\R^d$ by $(\bm{u}_j,\lambda_j)$ for $j=1,\ldots,d$. Then $\{\bm{\omega}_1,\ldots,\bm{\omega}_d\}=\{\tau({\bm{u}}_1),\ldots,\tau({\bm{u}}_d)\}$ forms an orthonormal basis of $\R_+^d$. In this new basis, the random vector $\bm{X}$ may be decomposed as
\begin{equation*}
    \bm{X} = \bigoplus_{j=1}^d (V_j \odot \bm{\omega}_j) = \tau\left(\sum_{j=1}^d V_j\bm{u}_j\right),
\end{equation*}
where
\begin{equation*}
    V_j = \left\langle \bm{X},\bm{\omega}_j\right\rangle_\tau = \left\langle \tau^{-1}(\bm{X}),\bm{u}_j\right\rangle, \qquad (j=1,\ldots,d).
\end{equation*}
Rank-$p$ reconstructions of $\bm{X}$ are obtained by the truncated expansion
\begin{equation*}
    \hat{\bm{X}}^{[p]} = \bigoplus_{j=1}^d (V_j \odot \bm{\omega}_j) = \tau\left(\sum_{j=1}^d V_j\bm{u}_j\right), \qquad (p=1,\ldots,d).
\end{equation*}
The process follows analogously for PCA based on an independent sample $\bm{X}_1,\ldots,\bm{X}_n$ and the empirical TPDM $\hat{\Sigma}$.

The elements of the $\R^d$-valued random vector $\bm{V}=(V_1,\ldots,V_d)$ are called the extremal principal components of $\bm{X}$. The random vector $\bm{V}$ is MRV with the same tail index as $\bm{X}$, but its angular measure $H_V$ lives on the entire unit sphere, not just its restriction to the positive orthant. Although the dimension of $\bm{V}$ is the same as $\bm{X}$, the crucial difference is that its components are ordered according to their contribution to the extreme behaviour of $\bm{X}$. Proposition 6 in @cooleyDecompositionsDependenceHighdimensional2019 states that
\begin{equation*}
    \mathrm{scale}(|V_i|) = \lambda_i^{1/\alpha}, \qquad (i=1,\ldots,d),
\end{equation*}
and therefore $\mathrm{scale}(|V_1|)\geq \ldots \geq \mathrm{scale}(|V_d|)\geq 0$. The $i$th eigenvector $\bm{\omega}_i$ represents the direction of maximum scale after accounting for information contained in $\bm{\omega}_1,\ldots,\bm{\omega}_{i-1}$; sequential examination of the eigenvectors provides insight into the extremal dependence structure. 

#### Applications

The PCA method of @cooleyDecompositionsDependenceHighdimensional2019 has been applied for exploratory purposes in the context of climatology [@jiangPrincipalComponentAnalysis2020; @szemkusSpatialPatternsIndices2024], finance [@cooleyDecompositionsDependenceHighdimensional2019] and sport [@russellAnalyzingDependenceMatrices2018]. 

@jiangPrincipalComponentAnalysis2020 analyse the extremal behaviour of precipitation across the United States. They discover an increasing temporal trend in the coefficient of the first principal component $V_1$, and relate the eigenvectors to the El-Niño Southern Oscillation (ENSO), a cyclical phenomenon that is known to be a key climatological driver. They find that low-rank reconstructions of Hurricane Floyd broadly capture the event's large-scale structure, but a large number of eigenvectors are needed to recreate more localised features. The spatial extent of the study region and relatively localised behaviour of extreme behaviour leads them to consider a 'pairwise-thresholded' estimator of the TPDM instead of the usual estimator \eqref{eq-empirical-tpdm} thresholded on the norm of entire vector. This alternative estimator is given by
\begin{equation*}
    \tilde{\Sigma}=(\tilde{\sigma}_{ij}), \qquad \tilde{\sigma}_{ij} = \frac{2}{k} \sum_{l=1}^n \Theta_{li}\Theta_{lj}\ind\{R_l^{ij} > R_{(k+1)}^{ij}\},
\end{equation*}
where $R_l^{ij} = \|(X_{li},X_{lj})\|$ and $R_{(k+1)}^{ij}$ is the $(k+1)$th upper order statistic of $\{R_l^{ij}:l=1,\ldots,n\}$. The estimator $\tilde{\Sigma}$ is not positive semi-definite, so the PCA analysis is instead conducted using the nearest positive definite matrix in Frobenius norm. The ramifications of this ad-hoc step, in terms of the estimator's theoretical properties and practical performance, are not studied.

@szemkusSpatialPatternsIndices2024 devise an extension of the TPDM, called the cross-TPDM, to study the joint extremal behaviour between two sets of variables. They analyse two meteorological variables -- daily maximum temperature and a measure of accumulated precipitation deficit -- to describe the dynamics of summer heatwaves in Europe. The cross-TPDM is the analogue of the cross-covariance matrix. Letting $\bm{X}=(X_1,\ldots,X_p)\in\mathrm{RV}_+^p(2)$ and $\bm{Y}=(Y_1,\ldots,Y_q)\in\mathrm{RV}_+^q(2)$, the cross-TPDM is defined as the $p\times q$ matrix with entries
\begin{equation*}
    \sigma_{ij}^{XY} = \int_{\mathbb{S}_+^{p+q-1}} \theta_i^X\theta_j^Y \,\dee H(\bm{\theta}),
\end{equation*}
where $H$ is the angular measure of $(\bm{X},\bm{Y})=(X_1,\ldots,X_p,Y_1,\ldots,Y_q)\in\mathrm{RV}_+^{p+q}(2)$ and the variable of integration is indexed as $\bm{\theta}=(\theta_1^X,\ldots,\theta_p^X,\theta_1^Y,\ldots,\theta_q^Y)$. (This definition could be extended to cater for an arbitrary tail index by introducing the usual $\alpha/2$ exponents in the integrand.) In the context of their climatological study, the entry $\sigma_{ij}^{XY}$ represents the strength of extremal dependence between the maximum temperature at location $i$ and the precipitation deficit at location $j$. The singular-value decomposition of the cross-TPDM is used to analyse the dynamics of compound extreme events. They devise extremal pattern indices to quantify whether particular patterns of interest -- those signified by the singular vectors of the cross-TPDM -- are highly pronounced.

A more unusual application of the TPDM is found in @russellAnalyzingDependenceMatrices2018. Their study characterises the difference in performance between typical and elite-level National Football League (NFL) performers across the Scouting Combine event. The Combine comprises six physical tests: Bench Press, Vertical Jump, Broad Jump, 40-yard Sprint, the Shuttle Drill, and the Three Cone Drill. The tests afford teams the opportunity to gauge the athletic ability of prospective players, thereby influencing whether (or how highly) they are drafted for the upcoming season. @russellAnalyzingDependenceMatrices2018 explore how strongly player performance correlates across these tests. Intuitively, if two events exhibit strong association, then they may be measuring the same underlying skills (speed, strength, agility etc.). After standardising player performance to account for differences in playing position, they find significant differences between the bulk dependence structure and the extremal dependence structure. In particular, the leading eigenvectors of the covariance matrix reveal that the Combine events cluster into three distinct groups, corresponding to strength, agility, and explosiveness. On the other hand, the TPDM eigenvectors produce only two such groups: power and agility. This reveals differences between non-elite and elite performers; recommendations regarding the composition of the Combine events are made accordingly. 

@rohrbeckSimulatingFloodEvent2023 move beyond the use of the extremal PCA for purely exploratory purposes and demonstrate how it be used to generate synthetic extreme events. Hazard event sets are widely used in catastrophe modelling to assess exposure to extreme events. Imagine an insurance company insures against damage to a portfolio of properties, and wishes to gauge its exposure to claims caused by flooding. Given (i) the spatial locations of these properties, (ii) other relevant characteristics such as property value and construction standard, and (iii) a set of simulated flood events, one can derive a probabilistic loss distribution. If the exposure is unacceptably high, they might adjust their underwriting strategy or purchase reinsurance. @rohrbeckSimulatingFloodEvent2023 show how to generate approximate samples from $H$, even in high-dimensions, by leveraging the PCA method of @cooleyDecompositionsDependenceHighdimensional2019. Their generative framework hinges on the fact that the leading components of $\bm{V}$ account for the greatest proportion of extremal behaviour of $\bm{X}$. Thus, efforts may be concentrated towards modelling the dependence structure of the sub-vector $(V_1,\ldots,V_p)$ for some appropriately chosen $p<d$. To achieve this, they use a spherical kernel density estimate to flexibly model the dependence between $V_1,\ldots,V_p$ and additionally between $(V_1,\ldots,V_p)$ and $(V_{p+1},\ldots,V_d)$. The dependence structure of $(V_{p+1},\ldots,V_d)$ is simply modelled by a nearest-neighbours approach. The number of components $p$ entering into the complex model is selected by a leave-one-out cross validation procedure. This involves discarding an extreme observation $\bm{x}_{(i)}$, generating a large number of samples $\tilde{\bm{x}}_1^{[p]},\ldots,\tilde{\bm{x}}_N^{[p]}$ for a range of values $p$, and then assessing whether any of the generated samples resemble the discarded event using
\begin{equation*}
    D_i(p) = \min_{l=1,\ldots,N} \varrho \left(\bm{x}_{(i)},\tilde{\bm{x}}_{l}^{[p]}\right),
\end{equation*}
where $\varrho(\cdot,\cdot)$ is an angular dissimilarity measure. After repeating for all extreme events $i=1\ldots,k$, one chooses the optimal $p$ as that which minimises the average error
\begin{equation*}
    \bar{D}(p) = \frac{1}{k}\sum_{i=1}^k D_i(p).
\end{equation*}
Their approach is illustrated using historical river flow data across $d=45$ gauges in northern England and southern Scotland. They select $p=7$ and find reasonable agreement between the observed river flow extreme events and the synthetic ones generated by their algorithm, e.g. by examining QQ-plots comparing the observed and sampled distributions of $\max_{j\in\mathcal{G}}X_j$ or $\|(X_i:i\in\mathcal{G})\|$ for selected groups of gauges $\mathcal{G}\subset \{1,\ldots,d\}$.

\textit{Add more critical comments, especially about asymptotic independence when using large study regions or with localised extremes, e.g. rainfall. Or leave this to the 'bias' section?} 

### Clustering into asymptotically dependent groups

Within multivariate extremes, the umbrella term `clustering` can refer to a multitude of tasks. To avoid confusion, we briefly describe these and clarify which type we are referring to. 

- **Prototypical events.** Assume that the angular measure concentrates at/near a small number of points in $\mathbb{S}_{+}^{d-1}$. Then one might wish to identify cluster centres $\bm{w}_1,\ldots\bm{w}_K$ minimising some objective function of the form
\begin{equation}\label{eq-clustering-objective-function}
    \mathbb{E}_{\bm{\Theta}\sim H}\left[\min_{l=1,\ldots,K}\varrho(\bm{\Theta},\bm{w}_l)\right],
\end{equation}
where $\varrho:\mathbb{S}_+^{d-1}\times \mathbb{S}_+^{d-1}\to[0,1]$ is some distance/dissimilarity function. The cluster centres can be interpreted as the directions of prototypical extremes events. See @chautruDimensionReductionMultivariate2015, @janssenKmeansClusteringExtremes2020 and @medinaSpectralLearningMultivariate2021 for further details.
- **Identification of concomitant extremes.** Suppose that angular measure is supported on a set of $K\ll 2^{d-1}$ subspaces (faces) of the simplex $C_{\beta_1},\ldots,C_{\beta_K}$, where $\beta_1,\ldots,\beta_K\in\mathcal{P}(\{1,\ldots,d\})\setminus\emptyset$ and
\begin{equation*}
    C_\beta = \{\bm{\theta}\in\mathbb{S}_+^{d-1}:\theta_i>0 \iff i\in\beta\}.
\end{equation*}
Only those groups (`clusters') of components indexed by $\beta_1,\ldots,\beta_K$ may be simultaneously extreme. Identification of the support of the angular measure is notoriously challenging because the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ lie (almost surely) in the interior of the simplex. @goixSparseRepresentationMultivariate2017 and @simpsonDeterminingDependenceStructure2020 identify clusters according to whether observations fall within appropriately sized rectangular/conic neighbourhoods of the corresponding axis in $\R_+^d$. @meyerDetectionExtremalDirections2020 take a different approach, whereby the angular component is defined with respect to the Euclidean projection [@liuEfficientEuclideanProjections2009] rather than usual projection based on self-normalisation. The geometry of the projection is such that the projected data lie on subfaces of the simplex. The price paid is that the limiting conditional distribution of the angles is related to, but not identical to, the angular measure.
- **Partitioning into AD/AI groups components.** This notion of clustering is related to the previous type. We assume that the variables $X_1,\ldots,X_d$ can be partitioned into $K$ clusters, such that $X_i$ and $X_j$ are asymptotically dependent if and only if they belong to the same cluster. In other words, there exists $2\leq K \leq d$ and a partition $\beta_1,\ldots,\beta_K$ of $\{1,\ldots,d\}$ such that the angular measure is supported on $C_{\beta_1},\ldots,C_{\beta_K}$ or lower-dimensional subspaces thereof, i.e.
\begin{equation*}
    H\left(\bigcup_{l=1}^K \bigcup_{\beta'_l\subseteq \beta_l} C_{\beta'_l}\right) = m.
\end{equation*}
The task of modelling the dependence structure of $\bm{X}$ can be divided into lower-dimensional sub-problems involving the random sub-vectors $\bm{X}_{\beta_1},\ldots,\bm{X}_{\beta_K}$. If $K=d$, then all variables are asymptotically independent. The underlying hypothesis is very strong and unlikely to hold in practice. Nevertheless, it is often a useful simplifying modelling assumption. @bernardClusteringMaximaSpatial2013 propose grouping components using the $k$-medoids algorithm [@kaufmanFindingGroupsData1990] with a dissimilarity matrix populated with pairwise measures of tail dependence, similar to $\chi_{ij}$ and $\sigma_{ij}$. The approaches of @fomichovSphericalClusteringDetection2023 and @richardsModernExtremeValue2024 involve the TPDM; these are reviewed in greater detail below.

#### @fomichovSphericalClusteringDetection2023

@fomichovSphericalClusteringDetection2023 show that the latter kind of clustering may be performed using the framework of the first kind. They provide a link between the principal eigenvector $\bm{u}_1$ of the TPDM and the minimiser of the objective \eqref{eq-clustering-objective-function} with quadratic cost $\varrho(\bm{\theta},\bm{\phi})=\left\langle\bm{\theta},\bm{\phi}\right\rangle^2$ and $K=1$:
\begin{equation*}
    \min_{\bm{\theta}\in\mathbb{S}_{+(2)}^{d-1}} \mathbb{E}_{\bm{\Theta}\sim H}\left[\varrho(\bm{\Theta},\bm{\theta})\right] = \mathbb{E}_{\bm{\Theta}\sim H}\left[\varrho(\bm{\Theta},\bm{u}_1)\right].
\end{equation*}
Note that $\bm{u}_1\in\mathbb{S}_{+(2)}^{d-1}$ is assumed to be suitably normalised with all entries being non-negative; the Perron-Frobenius theorem guarantees this is possible. This result informs an iterative clustering procedure called spherical $k$-principal-components. Consider a set of extremal angles $\bm{\theta}_{(1)},\ldots,\bm{\theta}_{(k)}\in\mathbb{S}_{+(2)}^{d-1}$ and current centroids $\hat{\bm{w}}_{1},\ldots,\hat{\bm{w}}_{K}\in\mathbb{S}_{+(2)}^{d-1}$. A single iteration of their procedure yields new centroids $\hat{\bm{w}}_{1}^\star,\ldots,\hat{\bm{w}}_{K}^\star\in\mathbb{S}_{+(2)}^{d-1}$ given by the respective principal eigenvectors of
\begin{equation*}
    \hat{\Sigma}^{[i]} = \sum_{l=1}^k \bm{\theta}_{(l)}\bm{\theta}_{(l)}^T\ind\{\argmin_{j=1,\ldots,K}\varrho(\bm{\theta}_{(l)},\bm{w}_j)=i\}, \qquad (i=1,\ldots,K).
\end{equation*}
The matrix $\hat{\Sigma}^{[i]}$ represents the empirical TPDM (up to some multiplicative constant) based on the nearest neighbours of the $i$th centroid. @fomichovSphericalClusteringDetection2023 prove that, under certain conditions, the limiting centroids lie in a neighbourhood of the faces of interest $C_{\beta_1},\ldots,C_{\beta_K}$. Thresholding the centroid vectors yields the final partition $\beta_1,\ldots,\beta_K$.

#### @richardsModernExtremeValue2024

@richardsModernExtremeValue2024 apply hierarchical clustering using the empirical TPDM as the underlying similarity matrix. The clustering method constitutes a minor aspect of their submission to the EVA (2023) Data Challenge. Few methodological details are provided, so the following explanation constitutes our interpretation of their method, drawing on Figure 4 in @richardsModernExtremeValue2024 and the accompanying code made available at \url{https://github.com/matheusguerrero/yalla}. Define the dissimilarity between $X_i$ and $X_j$ as $\varrho_{ij} = 1 - \sigma_{ij}$. This satisfies the properties of a dissimilarity measure (CITE: A MATHEMATICAL THEORY FOR CLUSTERING IN METRIC SPACES):
\begin{equation*}
    \varrho_{ij} \geq 0, \qquad \varrho_{ii} = 0, \qquad \varrho_{ij}=\varrho_{ji}.
\end{equation*}
The $d\times d$ dissimilarity matrix $\mathcal{D}=1-\Sigma=(\varrho_{ij})$ can be fed into standard hierarchical clustering algorithms. Agglomerative hierarchical clustering initially assigns each variable belongs to its own cluster, i.e. $\beta_i=\{i\}$ for $i=1,\ldots,d$. The algorithm proceeds iteratively, repeatedly joining together the two closest clusters until some stopping criterion is satisfied. Under complete-linkage clustering, the distance between clusters $\beta\neq\beta'$ is given by $\max\{\varrho_{ij} : i\in\beta,j\in\beta'\}$. The merging process may be stopped when there is a sufficiently small number of clusters or when the clusters are sufficiently separated.

### Parametric model fitting

@fixSimultaneousAutoregressiveModels2021 consider the extremal behaviour of a spatial process $\{\bm{X}(\bm{s}):\bm{s}\in\R^2\}$ at fixed sites $\bm{s}_1,\ldots,\bm{s}_d\in\R^2$ by modelling $\bm{X}=(\bm{X}(\bm{s}_i) : i=1\ldots,d)\in\mathrm{RV}_+^d(2)$ as
\begin{equation}\label{eq-sar-model}
    \bm{X} = (I - \rho W)^{-1}\otimes\bm{Z}.
\end{equation}
This is called the extremal spatial auto-regressive (SAR) model. The $d\times d$ matrix $W$ contains the (known) pairwise spatial distances and $\rho\in(0,1/4)$ is a spatial dependence parameter. The extremal SAR model is a special case of the max-linear model \eqref{eq-max-linear-X-cooley} with $A=A(\rho)=(I-\rho W)^{-1}$. They propose estimating the model parameter $\rho$ by minimising the discrepancy between the empirical TPDM and the theoretical TPDM $\Sigma(\rho):=A(\rho)A(\rho)^T$, that is
\begin{equation}\label{eq-sar-rho-estimator}
    \hat{\rho} = \argmin_{\rho\in(0,1/4)}\|\hat{\Sigma} - \Sigma(\rho)\|_F^2.
\end{equation}
In fact, $\hat{\Sigma}$ is replaced with a bias-corrected version of the empirical TPDM; this will be discussed in Section XX.

@kirilioukEstimatingProbabilitiesMultivariate2022 consider the more general problem of modelling arbitrary max-linear random vectors $\bm{X}=A\times_{\max}\bm{Z}\in \mathrm{RV}_+^d(2)$. In a similar spirit to @fixSimultaneousAutoregressiveModels2021, they propose estimating $A$ so as to enforce conformity between the empirical and model TPDMs. This means that the estimate of $A$ belongs to the set
\begin{equation*}
    \mathcal{CP}(\hat{\Sigma}) := \left\lbrace \hat{A}\in\R_+^{d\times q} : q\geq 1, \,\hat{\Sigma}=\hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T\right\rbrace.
\end{equation*}
Choosing $\hat{A}\in\mathcal{CP}(\hat{\Sigma})$ guarantees that the pairwise dependencies of the fitted model match those exhibited by the data. The set $\mathcal{CP}(\hat{\Sigma})$ is in direct correspondence to the set of completely positive (CP) factors of $\hat{\Sigma}$; we call $\hat{A}\in\mathcal{CP}(\hat{\Sigma})$ a CP-estimate of $A$. The naive estimate \eqref{eq-empirical-A} belongs to this class, but @kirilioukEstimatingProbabilitiesMultivariate2022 provide an algorithm for efficiently obtaining further estimates $\hat{A}\in \R_+^{d\times d}\cap \mathcal{CP}(\hat{\Sigma})$. 

@fixSimultaneousAutoregressiveModels2021 and @kirilioukEstimatingProbabilitiesMultivariate2022 evaluate the practical performance of their estimators by computing tail event probabilities in a series of simulated/real-world scenarios. *More details here, when I've written up formulae for failure events.*

### Miscellaneous: time series and extremal graphical models

[@mhatreTransformedLinearModelsTime2021; @gongPartialTailCorrelationCoefficient2024; @leePartialTailCorrelation2023].

## Bias in the empirical TPDM in weak-dependence scenarios}

Section XX reviewed the asymptotic properties of the empirical TPDM. We recall in particular that it is asymptotically unbiased, meaning $\mathbb{E}[\hat{\Sigma}] \to \Sigma$ as $n\to\infty$. The associated rate of convergence is $\mathcal{O}(k^{-1/2})$, where $k$ represents the number of extreme observations and satisfies the rate conditions \eqref{eq-k-rate-conditions}. For example, choosing $k(n)=\sqrt{n}$ yields a convergence rate of $\mathcal{O}(n^{-1/4})$. In practical settings the number of extreme events $k$ is normally small, both in relative (by definition) and absolute terms. For example, commonly available climate records typically span approximately 50 years \parencite{boulaguiem_modeling_2022}. A study of temperature extremes might then be based on, say, $n\approx 50 \times 100 = 5,000$ daily observations recorded in the summer months over this time span. Working with small effective sample sizes means it is critical to understand the non-asymptotic, finite-sample performance of the empirical TPDM. 

### Bias in threshold-based estimators

At finite levels, the empirical TPDM exhibits an upwards bias in weak dependence scenarios [@cooleyDecompositionsDependenceHighdimensional2019; @fixSimultaneousAutoregressiveModels2021; @mhatreTransformedLinearModelsTime2021]. This is true more generally of threshold-based estimators in multivariate extremes [@huserLikelihoodEstimatorsMultivariate2016]. They conduct simulation studies with $d=2$ and $n=10^4$ examining the performance of various estimators of $\gamma$, the dependence parameter of the symmetric logistic model. The results show that block-maxima based estimators have a small bias but very high variability. On the other hand, each of the threshold-based estimators $\hat{\gamma}$ tend to overestimate the dependence strength, that is $\mathrm{Bias}(\hat{\gamma}) = \mathbb{E}[\hat{\gamma}] - \gamma < 0$. Moreover, the discrepancy increases as dependence weakens ($\gamma\to 1$). 

The empirical TPDM suffers from the same issue when dependence is weak. This can be summarised as
\begin{equation}\label{eq-empirical-tpdm-bias}
    \sigma_{ij} \ll 1 \implies \mathrm{Bias}(\hat{\sigma}_{ij}) = \mathbb{E}[\hat{\sigma}_{ij}] - \sigma_{ij} > 0.
\end{equation}
Note that overestimating the dependence strength now corresponds to a positive bias, so the inequality is reversed. 

### Simulation experiments

See @fig-parametric-chi-tpdm-small-n.


```{r background-make-fig-parametric-chi-tpdm-small-n}
#| label: fig-parametric-chi-tpdm-small-n
#| fig-cap: "True dependence strengths for the symmetric logistic (left) and Hüsler-Reiss (right) models, measured using the tail dependence coefficient (red line) and TPDM (blue line). The shaded regions represent the minimum/maximum values of empirical estimates over 10 repeated simulations using bivariate samples of size $n=5\\times 10^3$."
#| fig-scap: "Bias in estimation of $\\sigma$ for symmetric logistic and Hüsler-Reiss models."
#| fig-height: 3.5

data <- readRDS(file.path("scripts", "background", "results", "parametric_chi_tpdm_empirical_smalln.RDS")) 

p1 <- ggplot() + 
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) +
  geom_function(aes(colour = "chi"), fun = sl_chi, xlim = c(0, 1)) + 
  geom_function(aes(colour = "tpdm"), fun = sl_tpdm, xlim = c(0, 1)) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 5)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(gamma), 
       y = "Dependence strength", 
       title = "Symmetric logistic") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10))  

p2 <- ggplot() + 
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) + 
  geom_function(aes(colour = "chi"), fun = hr_chi, xlim = c(0, 3.5)) + 
  geom_function(aes(colour = "tpdm"), fun = hr_tpdm, xlim = c(0, 3.5)) + 
  scale_x_continuous(limits = c(0, 3.5), expand = c(0, 0), breaks = breaks_extended(n = 4)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(lambda), 
       y = "Dependence strength", 
       title = "Hüsler-Reiss") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10)) 

ggarrange(p1, p2, ncol = 2, common.legend = TRUE) 
``` 


\subsection{Existing approaches to bias-correction for the TPDM}

Estimation error in the empirical TPDM was first studied by @cooleyDecompositionsDependenceHighdimensional2019. In the Supplementary Material, they assess the accuracy of the eigenvalues/eigenvectors of the empirical TPDM. Their example is based on a Brown-Resnick process, for which the true TPDM is known (@exm-husler-reiss-tpdm). They find that the leading eigenvalue is overestimated ($\hat{\lambda}_1 > \lambda_1$) and subsequent eigenvalues are underestimated ($\hat{\lambda}_j < \lambda_j$ for $j\geq 2$). The bias reduces when the sample size and radial threshold are increased. In the non-extreme setting, the sample covariance matrix has the same deficiency, especially when the sample size and dimension are comparable in magnitude [@mestreImprovedEstimationEigenvalues2008]. Poor spectrum estimation can have important consequences in a downstream analysis, such as deciding how many principal components are retained in PCA. @cooleyDecompositionsDependenceHighdimensional2019 do not propose any solutions to improve TPDM estimation.

The bias issue \eqref{eq-empirical-tpdm-bias} is addressed more directly by @mhatreTransformedLinearModelsTime2021. In their Supplementary Material, they conduct simulation studies to examine the performance of the empirical TPDF, the time series analogue of the TPDM (see Section XX). They show that $\sigma(h)$ exhibits a positive bias, especially at higher lags where the theoretical TPDM should vanish to zero. Their bias-corrected TPDF estimator works by subtracting the mean from the time series in pre-processing. The rationale for their estimator is described in terms of the position of extreme points in a lag plot (i.e. a scatter plot of $(X_t, X_{t+h})$ for some fixed lag $h$). Subtracting the mean has little effect on points near the middle of this plot, but points close to the coordinate axes are driven even closer. 

The first bias-correction estimation procedure for the TPDM is found in @fixSimultaneousAutoregressiveModels2021. Recall from Section XX that they use the empirical TPDM to estimate the spatial dependence parameter $\rho$ of the extremal SAR model \eqref{eq-sar-model}. When the spatial extent of the study domain is large compared to that of the modelled phenomenon, their estimation procedure \eqref{eq-sar-rho-estimator} is liable to overestimate $\rho$. This is because the empirical TPDM fails to capture the weak dependence between distant pairs of sites. Their bias-correction procedure is founded on the assumption that the pairwise asymptotic dependence strength vanishes to zero as the distance between two sites increases. Consider a spatial process $\{X(\bm{s}) : s\in \R^2\}$ and fixed locations $\bm{s}_1,\ldots,\bm{s}_d\in \R^2$. Let $X_i=X(\bm{s}_i)$ represent the process at site $i$ and $h_{ij}$ the spatial distance between $\bm{s}_i$ and $\bm{s}_j$. Treating the empirical TPDM entries as functions of distance, they model the relationship between the empirical TPDM and spatial distance via
\begin{equation*}
    \hat{\sigma}(h) = \beta_0 \exp(-\beta_1 h) + \beta_2.
\end{equation*}
The parameters $\beta_0,\beta_1,\beta_2$ are estimated from the observed data $\{(\hat{\sigma}_{ij},h_{ij}):1\leq i<j\leq d\}$ by non-linear least squares estimation, e.g. using \texttt{nls()}. Since $\hat{\sigma}(h)\to\beta_2$ as $h\to\infty$, the horizontal asymptote $\hat{\beta}_2$ of the fitted model is used as a proxy for the bias at large distances. This determines the amount of shrinkage that should be applied to the off-diagonal entries, yielding the final estimator
\begin{equation}\label{eq-fix-bias-corrected-tpdm}
    \tilde{\Sigma}=(\tilde{\sigma}_{ij}), \qquad \tilde{\sigma}_{ij} = \begin{cases}
        \hat{\sigma}_{ij}, & i=j, \\
        (\hat{\sigma}_{ij} - \hat{\beta}_2)_+, & i\neq j.
    \end{cases}
\end{equation}
Estimates of the diagonal entries are found to be unbiased -- and their values are known if the margins are standardised -- so they are unaltered. @fixSimultaneousAutoregressiveModels2021 find that $\tilde{\Sigma}$ is effective in reducing the bias in estimation of $\rho$. Its performance more broadly as an estimator for $\Sigma$ is not studied. In any case, their procedure is only applicable in settings where there is a notion of distance between variables. The estimator \eqref{eq-fix-bias-corrected-tpdm} results from element-wise application of the soft-thresholding operator (with shrinkage parameter $\hat{\beta}_2$) to the empirical TPDM \parencite{rothman_generalized_2009}. This connection will be developed further in Chapter XX, where we propose alternative bias-corrected TPDM estimators.

