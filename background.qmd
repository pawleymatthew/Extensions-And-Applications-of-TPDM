# Background \& literature review

```{r background-load-packages}
#| include: false
library(tidyverse)
library(tidyr)
library(magrittr)
library(scales)
library(ggh4x)
library(ggpubr)
library(colorspace)
library(kableExtra)
library(reshape2)
library(mev)
library(GGally)
library(ggforce)

options(dplyr.summarise.inform = FALSE)
options(knitr.kable.NA = "")
```

```{r background-source-functions}
#| include: false
sapply(list.files(path = "R/general", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
sapply(list.files(path = "R/background", pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

## Univariate extreme value theory

### Block maxima and the generalised extreme value (GEV) distribution

Let $X_1,X_2,\ldots$ be a sequence of independent, identically distributed, continuous random variables with distribution function $F$. For $n\geq 1$, define the random variable
\begin{equation}
    M_n := \max (X_1,\ldots, X_n) = \bigvee_{i=1}^n X_i.
\end{equation}
The exact distribution of $M_n$ is given by
\begin{equation*}
    \mathbb{P}(M_n \leq x) = \mathbb{P}(X_1\leq x,\ldots X_n \leq x) = \prod_{i=1}^n \mathbb{P}(X_i \leq x) = F^n(x), \qquad (x\in\R).
\end{equation*}
This result is not particularly useful in practice, where $F$ is typically unknown. Instead, we study the limiting behaviour of $F^n$ as $n\to\infty$. Clearly the asymptotic distribution of $M_n$ is degenerate, since $M_n \overset{p}{\to} x_F:=\sup\{x:F(x)<1\}$, the (possibly infinite) upper end-point of $F$. However, the Extremal Types Theorem states that, after suitable rescaling, there are three classes of non-degenerate asymptotic distribution (CITE).

:::{#thm-extremal-types}
Suppose there exist real sequences $\{a_n > 0\}$ and $\{b_n\in\R\}$ and a non-degenerate distribution function $G$ such that
\begin{equation}\label{eq-extremal-types}
    \mathbb{P}\left(\frac{M_n - b_n}{a_n} \leq x \right) \overset{d}{\to} G(x), \qquad (n\to\infty).
\end{equation}
Then $G$ belongs to one of three parametric families: Gumbel, Fréchet or negative Weibull.
:::

When \eqref{eq-extremal-types} holds, we say that $F$ lies in the maximum domain of attraction (MDA) of $G$. The three families are unified by the Generalised Extreme Value (GEV) distribution. Its distribution function is
\begin{equation}\label{eq-gev}
    G(x) = \exp\left\lbrace - \left[ 1 + \xi \left(\frac{x-\mu}{\sigma}\right)\right]_+^{-1/\xi} \right\rbrace,
\end{equation}
where $[x]_+:=\max(0,x)$ denote the positive part of $x$. The parameters $\mu\in\R$, $\sigma >0$ and $\xi\in\R$ are called the location, scale, and shape, respectively. The sign of the shape parameter determines the sub-class that $G$ belongs to: $\xi>0$ corresponds to the heavy-tailed Fréchet class, $\xi=0$ (with \eqref{eq-extremal-types} interpreted as $\xi\to 0$) corresponds the exponential-tailed Gumbel class, and $\xi<0$ the negative Weibull class, which has a finite upper limit.

The GEV distribution is used to model the upper tail of $X$ via the block maxima approach (CITE). Let $x_1,\ldots,x_n$ denote independent observations of $X_1,\ldots,X_n$. The data are partitioned into finite blocks of size $m$. Provided $m$ is sufficiently large, the maximum observation in each block is approximately GEV distributed by @thm-extremal-types. Once the block-wise maxima have been extracted, estimates of the GEV parameters may be obtained, e.g. by maximum likelihood inference. The performance of the fitted model is sensitive to the choice of block size. Selection of the tuning parameter $m$ requires managing a bias-variance trade-off. If the blocks are too small, then the underlying asymptotic approximation may not be valid and the maxima may not be representative as extreme events, biasing the estimates. Taking larger blocks reduces the amount of data available for inference, resulting in noisier estimation of the GEV parameter estimates.

### Threshold exceedances and the generalised Pareto distribution (GPD)

The block maxima procedure is considered inefficient, because it fails to exploit all the available information. Each block is summarised by a (single) maximum value, even if it contains other 'extreme' events that might be informative for the tail. The intimately related peaks-over-threshold method makes better use of the available data. If $X$ is in the maximum domain of attraction of a $\mathrm{GEV}(\mu,\sigma,\xi)$ distribution, then
\begin{equation}\label{eq-gpd}
    \lim_{u\to\infty} \mathbb{P}(X-u>x \mid X>u) = \left[1+\frac{\xi x}{\tilde{\sigma}}\right]_+^{-1/\xi}, \qquad (x>0),
\end{equation}
where $\tilde{\sigma}=\sigma + \xi(u-\mu)$ (CITE). The limiting conditional distribution is called the generalised Pareto distribution (GPD). The GPD describes the distribution of excesses over a high threshold. Given observations $x_1,\ldots,x_n$, the peaks-over-threshold method assumes that exceedances of some pre-specified high threshold $u>0$ are approximately GPD distributed. Maximum likelihood or Bayesian inference procedures may be used to estimate the GPD parameters $\bar{\sigma},\xi$. Threshold selection is subject to similar considerations as for the block size. Picking a low threshold risks model misspecification, causing bias in the fitted model. Choosing a high threshold directly reduces the number of threshold exceedances, increasing the uncertainty in the parameter estimates. Various diagnostics and procedures have been proposed to aid with this choice. Many approaches rely on inspecting diagnostic plots, such as mean residual life (MRL) plots (CITE) and parameter stability plots (CITE). Automated selection procedures aim to remove subjectivity by optimising with respect to some criterion. These include change-point methods (CITE Wadsworth 2016), cross-validation in a Bayesian framework (CITE Northrop et al. 2017), and minimising expected quantile discrepancies (CITE Murphy and Tawn 2024). 

### Non-stationary extremes

The block-maxima and peaks-over-threshold methods as presented above assume that the data are stationary over the observation period. In environmental applications, climate change threatens the validity of this assumption, with changes in the frequency and intensity of extreme weather events (CITE). Non-stationary models accommodate temporal dependence by allowing parameters to vary over time or in relation to covariates. For example, CITE Vanem 2015 incorporate trends into the GEV location and scale parameters by specifying
\begin{equation*}
\mu(t)=\mu_0 + \mu_1 t,\qquad \sigma(t) = \exp(\sigma_0 + \sigma_1 t).
\end{equation*}
If the parameters $\mu_1$ and $\sigma_1$ are significantly different from zero, it suggests the data exhibit non-stationarity. In principle the shape parameter may be extended analogously. Often the shape parameter is assumed constant because is notoriously difficult to estimate accurately and results (quantiles, return periods, etc.) are very sensitive to changes in its sign. *CITE further papers or a review?* 

## Multivariate extreme value theory

Multivariate extreme value theory (MEVT) generalises the study of extreme events from univariate to multivariate settings. Understanding the joint tail behaviour of several variables is critical in various fields. In environmental science, practitioners are tasked with assessing the risk of compound extreme events involving several variables. For example, the impact of drought -- defined by the IPCC (CITE) as a prolonged period of low precipitation -- is exacerbated by high temperatures. Similarly, extreme rainfall occurring simultaneously across multiple locations may lead to a widespread flood event. In finance, investors seek to diversify their portfolio to mitigate against the risk of simultaneous extreme losses across multiple assets. Each of these examples calls for a statistical analysis of the joint tail distribution of some random vector.

### Componentwise maxima

Consider a $d$-dimensional random vector $\bm{X}=(X_1,\ldots,X_d)$ with unknown joint distribution function $F$, meaning
\begin{equation*}
    F(\bm{x}) := \mathbb{P}(X_1\leq x_1, \ldots, X_d \leq x_d),
\end{equation*}
for any $\bm{x}=(x_1,\ldots,x_d)\in\R^d$. Let $\bm{X}_1,\bm{X}_2,\ldots$ be a sequence of independent copies of $\bm{X}$. The notion of `extremes' or a 'maximum' becomes subjective in the multivariate setting, because $\R^d$ is not an ordered set. One possibility is to define the maximum component-wise as
\begin{equation*}
    \bm{M}_n := \left( \bigvee_{i=1}^n X_{i1}, \ldots, \bigvee_{i=1}^n X_{id} \right). 
\end{equation*}
We say that $F$ lies in the multivariate MDA of a non-degenerate distribution $G$ if there exist $\R^d$-valued sequences $\{\bm{a}_n > \bm{0}\}$ and $\{\bm{b}_n \in \R^d\}$ such that
\begin{equation}\label{eq-multivariate-mda}
    \mathbb{P}\left(\frac{\bm{M}_n - \bm{b}_n}{\bm{a}_n} \leq \bm{x} \right) \overset{d}{\to} G(\bm{x}), \qquad (n\to\infty).
\end{equation}
Applying @thm-extremal-types to the marginal components reveals that the margins of $G$ follow a univariate GEV distribution. The crucial difference to the univariate setting is that now the limit (joint) distribution $G$ does *not* admit a parametric representation. The inherently challenging nature of MEVT largely stem from this fact. The problem of estimating/modelling $G$ is usually split into two (sequential) steps. First, one models the margins to describe the extreme behaviour of each variable individually (using univariate EVT). Then, one standardises to common margins and models the extremal dependence structure, i.e. the inter-relationships between extremes across multiple variables. Copula theory provides a rigorous justification for this two-step process.

### Copulae and marginal standardisation

In multivariate statistics, Sklar's theorem allows for the separation of the marginal distributions of variables from their joint dependence structure through the use of a copula. It states that any multivariate distribution can be expressed as a combination of individual marginal distributions and a copula that captures the dependence between them.

:::{#thm-sklar}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ has joint distribution function $F$ and continuous marginal distributions $X_i\sim F_i$ for $i=1,\ldots,d$. Then there exists a unique copula $C$ such that
\begin{equation}
    F(x_1,\ldots,x_d) = C\left(F_1(x_1),\ldots,F_d(x_d)\right).
\end{equation}
:::

The copula $C$ characterises the dependence structure of the variables, and represents the distribution function of $\bm{X}$ after transforming to standard uniform margins. Uniform margins are a standard choice in multivariate statistics, but copulae may be defined with alternative marginal distributions. In extreme value theory, it is common to use Fréchet, exponential or Gumbel margins. The different choices accentuate particular features of the extreme values. For example, heavy-tailed Fréchet margins serve to highlight the most extreme values, while Gumbel or exponential margins are often favoured for conditional extremes modelling (CITE Heffernan and Tawn). Although the marginal distribution is an important modelling choice, ultimately all choices are valid/equivalent in the sense that monotonic transformations of the univariate marginals do not change the nature of tail dependence [@resnickHeavytailPhenomenaProbabilistic2007].

There are broadly two ways of performing the preliminary marginal standardisation. Suppose $\bm{X}=(X_1,\ldots,X_d)$ has marginal distributions $X_i\sim F_i$ for $i=1,\ldots,d$. If the functions $F_i$ are known, then the marginal distributions can be transformed to some common target distribution $F_\star$ via the probability integral transform:
\begin{equation}\label{eq-marginal-transformation}
    X_i \mapsto F_\star^{-1}(F_i(X_i)) \sim F_\star, \qquad (i=1,\ldots,d).
\end{equation}
If the marginal distributions are unknown, as is usually the case, then $F_i$ is replaced with some estimate $\hat{F}_i$ in \eqref{eq-marginal-transformation}. A standard choice for $\hat{F}_i$ is the empirical CDF (non-parametric), perhaps with GPD tails above a high threshold (semi-parametric). Examples of these two approaches can be found in @russellAnalyzingDependenceMatrices2018 and @rohrbeckSimulatingFloodEvent2023, respectively. Throughout this thesis, uncertainty arising from estimation of the marginal distributions shall be neglected. Relaxing this assumption, as in @clemenconConcentrationBoundsEmpirical2023, represents an avenue for future work. 

### The exponent measure and angular measure

Suppose $\bm{X}$ is on unit Fréchet margins, that is
\begin{equation}\label{eq-unit-frechet}
    \mathbb{P}(X_i<x) = \exp(-1/x), \qquad (x>0),
\end{equation}
for $i=1,\ldots,d$. This corresponds to a GEV distribution with $\mu=\sigma=\xi=1$. The joint distribution $G$ in \eqref{eq-multivariate-mda} may be rewritten in the form 
\begin{equation}\label{eq-mevd}
    G(\bm{x}) = \exp(-V(\bm{x})),
\end{equation}
where $\bm{x}=(x_1,\ldots,x_d)$ and $x_i>0$ for $i=1,\ldots,d$. The exponent measure $V$ is a function of the form
\begin{equation}\label{eq-exponent-measure}
    V(\bm{x}) = d \int_{\mathbb{S}_{+(1)}^{d-1}} \bigvee_{i=1}^d \left(\frac{\theta_i}{x_i}\right)\,\dee H(\bm{\theta}).
\end{equation}
Here
\begin{equation}\label{eq-Lp-simplex}
    \mathbb{S}_{+(p)}^{d-1} := \{\bm{x}\in\R_+^d : \|\bm{x}\|_p = 1\}
\end{equation}
denotes the $L_p$-simplex in the non-negative orthant of $\R^d$ and the angular measure $H$ is a probability measure on $\mathbb{S}_{+(1)}^{d-1}$ satisfying the moment constraints
\begin{equation}\label{eq-H-mean-constraints}
    \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \,\dee H(\bm{\theta}) = 1/d, \qquad (i=1,\ldots,d).
\end{equation}
Our notation for the simplex is borrowed from @fixSimultaneousAutoregressiveModels2021. The exponent $d-1$ highlights the fact that the simplex is a $(d-1)$-dimensional set embedded in the $d$-dimensional space $\R^d$. The $+$ and $(p)$ in the subscript convey that the set is restricted to the non-negative orthant and is with respect to the $L_p$-norm, respectively. The constraints on $H$ arise due to tail equivalence of the margins. Functions $G$ satisfying \eqref{eq-mevd} are called multivariate extreme value distributions. If $V$ is differentiable, then the density $h$ of $H$ exists in the interior and on the low-dimensional boundaries of the simplex. The relation between $V$ and $h$ is given by
\begin{equation}
    h\left(\frac{\bm{x}}{\|\bm{x}\|_1}\right) = -\frac{\|\bm{x}\|_1^{d+1}}{d}\frac{\partial^d}{\partial x_1\cdots \partial x_d} V(\bm{x}).
\end{equation}
The benefit of introducing the exponent and angular measures is that models for $G$ may be specified in terms of $V$ or $H$. The extremal dependence structure of $\bm{X}$ is completely characterised by $H$: the angular measure determines $V$ via \eqref{eq-exponent-measure} and subsequently $G$ via \eqref{eq-mevd}. Modelling the angular measure now becomes our primary focus.

### Parametric multivariate extreme value models

The class of valid dependence structures is in direct correspondence to the infinite-dimensional class of valid measures $H$. This greatly hinders efforts to perform statistical inference: efficient estimation via likelihood inference, hypothesis testing, and inclusion of covariates immediately become unavailable. We may return to the parametric paradigm by postulating a suitable parametric sub-family. Ideally the chosen sub-family generates a wide class of valid dependence structures. A detailed review of popular models can be found in @gudendorfExtremeValueCopulas2010. 

There are several drawbacks to the parametric approach. Working with a parametric model instead of the general class runs the risk of model misspecification. Generating valid models is a challenging endeavour due to the moment constraints, resulting in models that are either overly simplistic or have unwieldy distribution functions and parameter constraints. Striking a balance between flexibility and parsimony becomes especially in high dimensions (i.e. when $d$ is large). For these reasons, parametric models are not a primary focus of this thesis. Nevertheless, we now review a small selection of models. These primarily feature as data-generating processes for our numerical experiments. Functionality for generating independent observations $\bm{x}_1,\ldots,\bm{x}_n$ of $\bm{X}$ or $\bm{\theta}_1,\ldots,\bm{\theta}_n\sim H$ based on the sampling algorithms formulated in @dombryExactSimulationMaxstable2016 is provided in the R package \texttt{mev}. 

#### Logistic-type models

One of the oldest and simplest multivariate extreme value models is the symmetric logistic distribution [@gumbelBivariateExponentialDistributions1960].

:::{#def-symmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the symmetric logistic distribution is
\begin{equation}\label{eq-symmetric-logistic-V}
    V(\bm{x}) = \left(\sum_{i=1}^d x_i^{-1/\gamma}\right)^{\gamma}, \qquad \gamma \in (0,1].
\end{equation}
:::

The single dependence parameter $\gamma\in(0,1]$ characterises the strength of the association between all variables. Independence occurs when $\gamma=1$ and the variables approach complete dependence as $\gamma\to 0$. All variables are exchangeable, since the distribution function is invariant under coordinate permutation. A flexible extension is the asymmetric logistic model of @tawnModellingMultivariateExtreme1990. Greater control over the dependence structure is achieved by increasing the number of parameters.

:::{#def-asymmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the asymmetric logistic distribution is of the form 
\begin{equation}\label{eq-asymmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}\left[\sum_{i\in\beta}\left(\frac{\theta_{i,\beta}}{x_i}\right)^{1/\gamma_\beta}\right]^{\gamma_\beta}, \qquad 
    \begin{cases}
      \gamma_\beta\in(0,1], \\
      \theta_{i,\beta}\in[0,1], & \text{if }i\in\beta, \\
      \theta_{i,\beta}=0, & \text{if }i\notin\beta, \\
      \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset} \theta_{i,\beta}=1,
    \end{cases}
\end{equation}
where $\mathcal{P}(\{1,\ldots,d\})\setminus\emptyset$ denotes the set of non-empty subsets of $\{1,\ldots,d\}$.
:::

The set of parameters $\{\gamma_\beta:\beta\in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset\}$ control the dependence strength among the corresponding variables $\{X_i : i\in\beta\}$ in a similar way to the symmetric logistic model. The model's complexity arises from the set of asymmetry parameters $\bm{\theta}_\beta=(\theta_{i,\beta}:i\in\beta)$, which dictate the direction/composition of extreme events involving the variables $\{X_i : i\in\beta\}$. Further models can be generated by `inverting' the logistic and asymmetric models. **The purpose of inverting is...**. When applied to the models described above, inversion yields the negative symmetric logistic model [@galambosOrderStatisticsSamples1975] and the negative asymmetric logistic model [@joeFamiliesMinstableMultivariate1990], respectively.

:::{#def-negative-symmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the negative symmetric logistic distribution is
\begin{equation}\label{eq-negative-symmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}(-1)^{|\beta|+1}\left(\sum_{i\in\beta} x_i^{\gamma}\right)^{-1/\gamma}, \qquad \gamma>0.
\end{equation}
:::

:::{#def-negative-asymmetric-logistic}
The exponent measure of a random vector $\bm{X}=(X_1,\ldots,X_d)$ following the negative asymmetric logistic distribution is
\begin{equation}\label{eq-negative-asymmetric-logistic-V}
    V(\bm{x}) = \sum_{\beta \in \mathcal{P}(\{1,\ldots,d\})\setminus\emptyset}(-1)^{|\beta|+1}\left(\sum_{i\in\beta} x_i^{\gamma}\right)^{-1/\gamma}, \qquad \gamma>0.
\end{equation}
:::

Other logistic-type models include the bilogistic @smithStatisticsMultivariateExtremes1990] and negative bilogistic [@colesStatisticalMethodsMultivariate1994].

#### The Brown-Resnick process and Hüsler-Reiss distribution

The Brown-Resnick process of @brownExtremeValuesIndependent1977 is a class of stochastic processes commonly used to model the extremal dependence structure of spatial phenomena, including rainfall [@davisonStatisticalModelingSpatial2012], snow depths [@schellanderModelingSnowDepth2018] and wind gusts [@oestingStatisticalPostprocessingForecasts2017]. It is naturally defined through a transformation of a Gaussian process -- a formal construction can be found in CITE Kabluchko et al. (2009). Let $\Omega\in\R^2$ be a spatial domain. Consider a Brown-Resnick process $\{X(\bm{s}):\bm{s}\in\Omega\}$ with semi-variogram
\begin{equation}\label{eq-fractal-variogram}
    \gamma(\bm{s}, \bm{s}') = (\|\bm{s}-\bm{s}'\|_2/\rho)^\kappa, \qquad \rho>0, \kappa\in(0,2].
\end{equation} 
Semi-variograms of this form are called fractal semi-variograms and the associated process $\{X(\bm{s})\bm{s}\in\Omega\}$ is stationary and isotropic [@engelkeEstimationHuslerReissDistributions2015]. Stationarity and isotropy mean that the statistical properties of the spatial process are invariant under translation and rotation. Specifically, the dependence between two sites only depends on the distance between them, not the direction or their position within the spatial domain. The parameters $\rho$ and $\kappa$ in \eqref{eq-fractal-variogram} control the range and smoothness, respectively. The range parameter determines how quickly the dependence strength decreases over distance. The smoothness parameter governs the regularity of the process and affects its local behaviour.

Let $\bm{s}_i,\bm{s}_j\in\Omega$ be a pair of spatial locations and define random variables $X_i=X(\bm{s}_i)$ and $X_j=X(\bm{s}_j)$. The exponent measure of the bivariate random vectors $(X_i,X_j)$ is [@huserCompositeLikelihoodEstimation2013]
\begin{equation}\label{eq-brown-resnick-V}
    V(x_i,x_j) = \frac{1}{x_i} \Phi\left(\frac{a_{ij}}{2} + \frac{1}{a_{ij}}\log\frac{x_j}{x_i}\right) + \frac{1}{x_j} \Phi\left(\frac{a_{ij}}{2} + \frac{1}{a_{ij}}\log\frac{x_i}{x_j}\right),
\end{equation}
where $a_{ij} = \sqrt{\gamma(\bm{s}_i,\bm{s}_j)}$. The stationary/isotropic nature of the underlying process is apparent because $V$ depends on $\bm{s}_i$ and $\bm{s}_j$ only through $\|\bm{s}_i-\bm{s}_j\|_2$. 

*Other things I could mention: Davison et al. (2012) apply BR to rainfall data, finding $1/2<\kappa<1$. Although the Brown–Resnick processes are max-stable, the processes observed at a finite number of locations are also multivariate regularly varying.*

The Brown-Resnick process is intimately related to the Hüsler-Reiss distribution of @huslerMaximaNormalRandom1989. The Hüsler-Reiss distribution is of fundamental importance in multivariate extremes: it has been labelled the Gaussian distribution for extremes [@engelkeGraphicalModelsExtremes2019]. In $d\geq 2$ dimensions the distribution is parametrised by a matrix $\Lambda=(\lambda_{ij}^2)_{1\leq i,j\leq d}$ belonging to the class of symmetric, strictly conditionally negative definite matrices
\begin{equation*}
    \mathcal{D} := \left\lbrace M \in \R_+^{d\times d} \, : \, M=M^T,\, \mathrm{diag}(M)=\bm{0},\, \bm{x}^TM\bm{x} < 0 \,\forall \bm{x}\in\R^d\setminus\{\bm{0}\} \text{ such that } \sum_{j=1}^d x_j=0\right\rbrace.
\end{equation*}
The class of Hüsler-Reiss distributions is closed in the sense that if $\bm{X}=(X_1,\ldots,X_d)$ follows a Hüsler-Reiss distribution with parameter matrix $\Lambda$, then any random sub-vector $(X_i,X_j)$ is also Hüsler-Reiss distributed with parameter $\lambda_{ij}^2$. This permits very flexible control over the pairwise dependence structure. The dependence between any pair of variables $X_i$ and $X_j$ can be adjusted by modifying the corresponding parameter $\lambda_{ij}$, subject to the constraint $\Lambda\in\mathcal{D}$. The finite-dimensional distribution of a Brown-Resnick process at locations $\bm{s}_1,\ldots,\bm{s}_d$ is precisely the Hüsler-Reiss distribution with $\Lambda = (\gamma(\bm{s}_i,\bm{s}_j)/4)_{1\leq i,j \leq d}$ [@engelkeEstimationHuslerReissDistributions2015]. Due to this link, the Hüsler-Reiss distribution may be parametrised in terms of its variogram matrix $\Gamma:=4\Lambda\in\mathcal{D}$ [@engelkeSparseStructuresMultivariate2021; @fomichovSphericalClusteringDetection2023] and the exponent measure of $(X_i,X_j)$ is given by \eqref{eq-brown-resnick-V} with $a_{ij}$ replaced by $2\lambda_{ij}$.

#### The max-linear model

The final parametric model we consider is the max-linear (factor) model [@einmahlMestimatorTailDependence2012; @fougeresDenseClassesMultivariate2013; @yuenCRPSMestimationMaxstable2014]. *Its exact origin is unclear, but it seems to stem from around these papers.* Max-linear models are a simple but flexible class possessing important theoretical properties. Any discrete angular measure concentrating on finitely many points corresponds to a max-linear model [@yuenCRPSMestimationMaxstable2014]. Due to its flexibility and theoretical properties, the max-linear model has enjoyed widespread use across several areas of extremes, including clustering [@janssenKmeansClusteringExtremes2020; @medinaSpectralLearningMultivariate2021], graphical modelling for causal inference [@gissiblIdentifiabilityEstimationRecursive2019; @gissiblMaxlinearModelsDirected2018; @tranCausalDiscoveryRiver2021] and tail event probability estimation [@kirilioukEstimatingProbabilitiesMultivariate2022]. In future sections/chapters, the max-linear model will be applied in more general settings where the marginal distributions are Fréchet with shape parameter $\alpha\geq 1$ and the angular measure is defined with respect to the $L_\alpha$-norm on $\R^d$. In anticipation of this, the max-linear model is introduced in this more general setting. To revert to the setting established in the previous sections, the reader may simply take $\alpha=1$.

:::{#def-max-linear}
Let $A=(\bm{a}_1,\ldots,\bm{a}_q)\in\R_+^{d\times q}$ for some $q\geq 1$. Assume that $\bm{a}_j\neq\bm{0}$ for all $j=1,\ldots,q$ and each row has unit $L_\alpha$-norm, i.e. $\sum_{j=1}^q a_{ij}^\alpha =1$ for $i=1,\ldots,d$. A random vector $\bm{X}=(X_1,\ldots,X_d)$ with discrete probability angular measure
\begin{equation}\label{eq-max-linear-H}
    H(\cdot) = \frac{1}{\sum_{j=1}^q \|\bm{a}_j\|_{\alpha}^{\alpha}}\sum_{j=1}^q \|\bm{a}_j\|_{\alpha}^{\alpha} \delta_{\bm{a}_j/\|\bm{a}_j\|_{\alpha}}(\cdot)
\end{equation}
is said to follow the max-linear model with parameter matrix $A$.
:::

The row-wise unit-norm constraint on $A$ results ensures the marginal components are Fréchet distributed with unit scale and shape $\alpha$. Setting $\alpha=1$, we see that \eqref{eq-max-linear-H} is a valid angular measure: for any $i=1,\ldots,d$,
\begin{equation*}
    \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \,\dee H(\bm{\theta}) 
    = \frac{1}{\sum_{j=1}^q \|\bm{a}_j\|_1} \sum_{j=1}^q \int_{\mathbb{S}_{+(1)}^{d-1}} \theta_i \|\bm{a}_j\|_1 \delta_{\bm{a}_j/\|\bm{a}_j\|_1}(\bm{\theta})\,\dee \bm{\theta}
    = \frac{\sum_{j=1}^q a_{ij}}{\sum_{i=1}^d \sum_{j=1}^q  a_{ij}} 
    = \frac{1}{d}.
\end{equation*}
The number of free parameters is $d\times(q-1)$ and the order of the columns of $A$ is inconsequential. The factors $\bm{a}_1,\ldots,\bm{a}_q$ correspond to the possible directions that extremal observations may take. The column  norms $\|\bm{a}_1\|_{\alpha},\ldots,\|\bm{a}_q\|_{\alpha}$ determine the respective weights assigned to these directions. There is a direct correspondence between the class of discrete angular measure placing mass on $q<\infty$ points and the class of max-linear random vectors with $q$ factors [@yuenCRPSMestimationMaxstable2014]. Moreover, the class of angular measures \eqref{eq-max-linear-H} is dense in the class of valid angular measures [@fougeresDenseClassesMultivariate2013]. In other words, any extremal dependence structure can be arbitrarily well-approximated by that of a max-linear model with sufficiently many factors. This makes max-linear modelling a versatile and powerful framework, despite its simplicity.

There are several ways to construct a random vector $\bm{X}=(X_1,\ldots,X_d)$ with angular measure \eqref{eq-max-linear-H}. This thesis uses two constructions. Let $Z_1,\ldots,Z_q$ be independent Fréchet random variables with unit scale and shape parameter $\alpha$, and set $\bm{Z}=(Z_1,\ldots,Z_q)$. The two constructions are
\begin{equation}\label{eq-max-linear-X}
    \bm{X} = A \times_{\max} \bm{Z} := \left( \bigvee_{j=1}^q a_{1j}Z_j, \ldots, \bigvee_{j=1}^q a_{dj}Z_j \right)
\end{equation}
and
\begin{equation}\label{eq-max-linear-X-cooley}
    \bm{X} = A \otimes \bm{Z} := \bigoplus_{j=1}^q (\bm{a}_j \odot Z_j).
\end{equation}
Adopting the terminology of @cooleyDecompositionsDependenceHighdimensional2019, we refer to these as the max-stable and transformed-linear constructions, respectively. Under the max-stable construction, each component $X_i$ is the maximum of linear combinations of the heavy-tailed latent variables $Z_1,\ldots,Z_q$. The second construction, employed in @cooleyDecompositionsDependenceHighdimensional2019, is defined in terms of vector space operations $\oplus$ and $\odot$ defined therein. These operations will be defined explicitly and discussed later in Section XX. The difference between the two constructions manifests in their realisations, as illustrated in Figure 7 in the Supplementary Material of @cooleyDecompositionsDependenceHighdimensional2019. The directions of large realisations of the max-stable construction tend to correspond almost exactly to the points $\bm{a}_1/\|\bm{a}_1\|_{\alpha},\ldots,\bm{a}_q/\|\bm{a}_q\|_{{\alpha}}$. Under the transformed-linear construction, the directions of extreme events tend to lie in a neighbourhood of, but not exactly on, these discrete locations. 

Computing joint tail event probabilities is straightforward under the max-linear model. Suppose $\bm{X}$ is max-linear with parameter matrix $A$. Consider the extreme failure region
\begin{equation*}
\mathcal{R}_f(x) := \{\bm{y}\in\R_+^d : f(\bm{y})>x\}
\end{equation*}
for some function $f:\R_+^d\to\R$. Provided the failure region is sufficiently extreme (distant from the origin), then
\begin{equation}\label{eq-max-linear-failure-probability}
\mathbb{P}(\bm{X}\in\mathcal{R}_f(x)) \approx \sum_{j=1}^q \frac{\|\bm{a}_j\|_{\alpha}^{\alpha}}{r_\star(\bm{a}_j/\|\bm{a}_j\|_{\alpha})^\alpha},
\end{equation}
where $r_\star=r_\star(\bm{\theta})$ is such that $f(r_\star \bm{\theta})=x$ [@cooleyDecompositionsDependenceHighdimensional2019; @kirilioukEstimatingProbabilitiesMultivariate2022]. The formulae corresponding to some popular failure regions are listed below:
\begin{align*}
f(\bm{y}) = \max\bm{y}, \qquad & \mathbb{P}(\max\bm{X}>x) \approx \sum_{j=1}^q \max_{i=1,\ldots,d}\left(\frac{a_{ij}}{x}\right)^{\alpha} \\
f(\bm{y}) = \min\bm{y}, \qquad & \mathbb{P}(\min\bm{X}>x) \approx \sum_{j=1}^q \min_{i=1,\ldots,d}\left(\frac{a_{ij}}{x}\right)^{\alpha} \\
f(\bm{y}) = \bm{v}^T\bm{y}, \qquad & \mathbb{P}(\bm{v}^T\bm{X}>x) \approx \sum_{j=1}^q \left(\frac{\bm{v}^T\bm{a}_j}{x}\right).
\end{align*}
The first and second regions concern extreme events affecting at least one variable or all variables simultaneously, respectively. For the third region, the weight vector $\bm{v}$ satisfies $v_i\geq 0$ and $v_1+\ldots+v_d=1$. Such regions are of interest for climate event attribution [@kirilioukClimateExtremeEvent2020] or quantifying the Value-at-Risk of an asset portfolio [@yuenUpperBoundsValuerisk2014]. Each of these failure probabilities may be perceived as a measure of risk. Risk mitigation is the practice of taking action -- bolstering flood defences or diversifying a portfolio -- to ensure these probabilities are acceptably small.  

### Multivariate regular variation

Multivariate regular variation (MRV) provides an alternative framework for characterising the probabilistic structure of the joint tail of random vectors. By imposing a regularity structure on the joint tail, MRV facilitates the development of theoretically justified procedures for extrapolating the probability law from moderately large values to more extreme tail regions. We introduce the concept of regular variation in the univariate setting before extending to the multivariate case.

:::{#def-regular-variation-function}
A function $f:\R_+\to\R_+$ is regularly varying with index $\alpha\in\R$ if, for all $x>0$,
    \begin{equation}
        \lim_{t\to\infty}\frac{f(tx)}{f(t)} = x^\alpha.
    \end{equation}
:::

If $\alpha=0$, then $f$ is called slowly-varying. Intuitively, a regularly varying function is one that behaves like a power function as the argument approaches infinity. This notion is generalised to random variables by taking the distributional tail as the function of interest. 

:::{#def-regular-variation-random-variable}
A non-negative random variable $X$ is regularly varying with tail index $\alpha \geq 0$ if the right-tail of its distribution function is regularly varying with index $-\alpha$, i.e. for all $x>1$,
    \begin{equation*}
        \lim_{t\to\infty} \mathbb{P}(X > tx \mid X > t) = x^{-\alpha}.
    \end{equation*}
:::

If $X$ is regularly varying with index $\alpha$, then its survivor function is of the form
\begin{equation}\label{eq-regularly-varying-survivor-function}
    \mathbb{P}(X>x) = x^{-\alpha} L(x)
\end{equation}
for some slowly-varying function $L$ [@jessenRegularlyVaryingFunctions2006]. Regularly varying random variables are those with power law tails. In fact, a random variable $X$ is regularly varying if and only if it belongs to the Fréchet MDA (CITE). Crucially, \eqref{eq-regularly-varying-survivor-function} reveals that regularly varying distributions possess asymptotic scale invariance, in the sense that for all $\lambda>0$,
\begin{equation*}
    \mathbb{P}(X > \lambda x) = (\lambda x)^{-\alpha} L(\lambda x) \sim \lambda^{-\alpha} \mathbb{P}(X > x).
\end{equation*}
The ubiquity of regular variation in extreme value statistics is due to this homogeneity property. Under regular variation, the probability law of $X$ at some level $\lambda x$ is identical to the probability law at level $\lambda$, up to some constant factor. An analogous interpretation holds when regular variation is generalised to multivariate random vectors, where the joint tail distribution is represented by a homogeneous limit measure.

Although MRV can be formulated more generally -- see Section 6.5.5 in @resnickHeavytailPhenomenaProbabilistic2007 -- we exclusively focus on random vectors $\bm{X}$ taking values on the positive orthant $\R_+^d:=[0,\infty)^d$. This common assumption is not as restrictive as it might initially seem. In most applications, the risk being assessed is directional. For example, a climatologist might model the lows or the highs of precipitation records depending on they are analysing drought risk or flood risk. Without loss of generality and by means of a transformation if necessary, this direction of interest can be defined as ‘positive’.

:::{#def-mrv}
A random vector $\bm{X}=(X_1,\ldots,X_d)$ is multivariate regularly varying with tail index $\alpha>0$, denoted $\bm{X}\in\mathcal{RV}_+^d(\alpha)$, if it satisfies the following (equivalent) statements [@resnickHeavytailPhenomenaProbabilistic2007]: 

1. There exists a sequence $b_n\to\infty$ and a non-negative Radon measure $\nu$ on $\mathbb{E}_0:=[0,\infty]^d\setminus\{\bm{0}\}$ such that
\begin{equation}\label{eq-mrv}
n\mathbb{P}(b_n^{-1}\bm{X} \in \cdot) \stackrel{\mathrm{v}}{\rightarrow} \nu(\cdot),\qquad (n\to\infty),
\end{equation}
where $\stackrel{\mathrm{v}}{\rightarrow}$ denotes vague convergence in the space of non-negative Radon measures on $\mathbb{E}_0$. The exponent measure $\nu$ is homogeneous of order $-\alpha$, that is, for any $s>0$,
\begin{equation}
    \nu(s\,\cdot)=s^{-\alpha}\nu(\cdot).
\end{equation}
2. Let $\|\cdot\|$ be an arbitrary norm on $\R^d$. Denote the radial and angular components of $\bm{X}$ by $R:=\|\bm{X}\|$ and $\bm{\Theta}:=\bm{X}/\|\bm{X}\|$. Then there exists a sequence $b_n\to\infty$ and a finite measure $H$ on the simplex
    \begin{equation}\label{eq-general-simplex}
        \mathbb{S}_{+}^{d-1}:=\{\bm{x}\in\R_+^d:\|\bm{x}\|=1\}
    \end{equation}
    such that
\begin{equation}\label{eq-polar-mrv}
n\mathbb{P}((b_n^{-1}R,\bm{\Theta}) \in \cdot) \stackrel{\mathrm{v}}{\rightarrow} \nu_{\alpha}\times H(\cdot),\qquad (n\to\infty),
\end{equation}
in the space of non-negative Radon measures on $(0,\infty]\times\mathbb{S}_{+}^{d-1}$, where $\nu_{\alpha}((x,\infty))=x^{-\alpha}$ for any $x>0$.
:::

The limit measures $\nu$ and $H$ in \eqref{eq-mrv} and \eqref{eq-polar-mrv} are related via 
\begin{equation}\label{eq-nu-H-relation}
    \nu(\{\bm{x}\in\mathbb{E}_0:\|\bm{x}\| > s,\bm{x}/\|\bm{x}\|\in\cdot\}) 
    = s^{-\alpha}H(\cdot),\qquad  \nu(\dee r\times \dee \bm{\theta})
    =\alpha r^{-\alpha-1}\dee r\,\dee H(\bm{\theta}).
\end{equation}
The attractive feature of MRV is best represented by its pseudo-polar formulation \eqref{eq-polar-mrv}. This states that the extremal behaviour of $\bm{X}$ is fully characterised by two quantities: the tail index and the angular measure. The tail index $\alpha$ represents the index of regular variation of the (univariate) radial component. It governs the heavy-tailedness of the size (norm) of $\bm{X}$. The angular measure $H$ fully characterises the dependence structure. Crucially, the right-hand side of \eqref{eq-polar-mrv} is a product measure, signifying that the radial and angular components are independent in the limit. 

The MRV property implicitly requires that the marginal components $X_1,\ldots,X_d$ are heavy-tailed with a shared tail index. Standard practice is to standardise the margins prior to modelling the dependence structure (Section XX), so this is not restrictive. In this thesis, we will always choose Fréchet margins with unit scale and shape parameter $\alpha>0$, that is
\begin{equation}\label{eq-alpha-frechet}
    \mathbb{P}(X_i<x) = \exp(-x^{-\alpha}), \qquad (x>0).
\end{equation}
An MRV random vector on $\alpha$-Fréchet margins \eqref{eq-alpha-frechet} has tail index $\alpha$. Thus, as before, fixing the margins deals with the tail index and the angular measure becomes the object of interest.

The angular measure is unique only with respect to a pre-specified norm $\|\cdot\|$ and lies on the corresponding unit simplex \eqref{eq-general-simplex}. As mentioned previously, we exclusively choose the $L_p$-norm 
\begin{equation}
    \|\cdot\|_p : \R^d \to \R, \qquad \|\bm{x}\|_p = \left(\sum_{i=1}^d x_i^{p}\right)^{1/p}
\end{equation}
with \eqref{eq-Lp-simplex} the corresponding simplex. The mass of the angular measure is $m:=H(\mathbb{S}_{+}^{d-1})\in(0,\infty)$. The sequence $\{b_n\}$ and the quantity $m$ are jointly determined by \eqref{eq-polar-mrv}. Replacing $\{b_n\}$ by $\{sb_n\}$ for some $s>0$ yields a new angular measure $H'=s^{-\alpha}H$ whose mass is $m'=s^{-\alpha}m$. We are free to choose whether the scaling information is contained in $\{b_n\}$ or $m$. Possible reasons for preferring one over the other are discussed in @fougeresDenseClassesMultivariate2013, but ultimately it is an arbitrary modelling choice. In previous sections, $H$ was normalised to be a probability measure with $m=1$. Henceforth, we will tend to specify $\{b_n\}$ and push the scaling information on to $H$. With $\bm{X}$ standardised to $\alpha$-Fréchet margins, the centre of mass of $H$ must lie in the simplex interior:
\begin{equation}\label{eq-H-mean-constraints-general}
    \int_{\mathbb{S}_{+}^{d-1}} \theta_i \,\dee H(\bm{\theta}) = \mu > 0, \qquad (i=1,\ldots,d).
\end{equation}
Were this not the case it would imply that at least one variable can never be extreme, contradicting the assumption that all variables have equally heavy tails. The value of $\mu$ depends on the choice of norm and the mass of $H$. If $\|\cdot\|=\|\cdot\|_1$, then $\mu=m/d$ in accordance with \eqref{eq-H-mean-constraints}. If $\|\cdot\|=\|\cdot\|_2$, then $m/d\leq \mu \leq m/\sqrt{d}$ according to Lemma 2.1 in @fomichovSphericalClusteringDetection2023. The lower and upper bounds are attained when $H$ places all its mass at the vertices of the simplex or at its centre, respectively. These can be understood as the limiting cases of extremal dependence, which is formalised in the next section.

### Extremal dependence measures

The extremal dependence structure of a random vector $\bm{X}$ can be quantified and classified using a plethora of summary measures [@colesDependenceMeasuresExtreme1999]. We focus on the tail dependence coefficient and the extremal dependence measure. 

#### The tail dependence coefficient

Extremal dependence is analogous to, but separate from, the notion of statistical dependence in non-extreme statistics. In particular, two random processes might appear independent in the bulk of the distribution but exhibit dependence in their extremes, or vice versa. The extremal dependence structure may be very complex; angular measures form an infinite-dimensional class subject only to a set of moment constraints. For example, suppose $X_i$ and $X_j$ represent the recorded values of a meteorological variable measured at two spatial locations. The extremal dependence between $X_i$ and $X_j$ may depend on the spatial proximity of the sites, the topography of the spatial domain, the physics of the climatological process, and a multitude of other factors. The complexity grows as more variables are introduced, as higher-order dependencies come into play. Extremal dependence measures aim to provide summary information about particular aspects of the dependence structure. One such measure is the tail dependence coefficient (CITE).

:::{#def-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ with $X_i\sim F_i$ for $i=1,\ldots,d$. Let $\beta\subseteq\{1,\ldots,d\}$ with $|\beta|\geq 2$ and define $\bm{X}_\beta := \{X_i : i\in\beta\}$. The tail dependence coefficient associated with $\beta$ is (CITE e.g. Simpson et al 2020)
\begin{equation}
    \chi_\beta = \lim_{u\to 1} \chi_\beta (u) = \lim_{u\to 1} \frac{\mathbb{P}(F_i(X_i) > u : i \in \beta)}{1-u}.
\end{equation}
When $\beta=\{i,j\}$ for $i\neq j$, we write $\chi_\beta=:\chi_{ij}$.
:::

We say that $X_i$ and $X_j$ are asymptotically independent (AI) if and only if $\chi_{ij}=0$. Asymptotic independence means that both variables cannot take extreme values simultaneously. If $\chi_{ij}\in(0,1]$, then the variables are asymptotically dependent (AD) and may be simultaneously extreme. The interpretation of $\chi_\beta$ for $|\beta|>2$ is more subtle. If $\chi_\beta\in(0,1]$, then all components of $\bm{X}_\beta$ may be simultaneously large. If $\chi_\beta=0$, then the corresponding variables may not be concomitantly extreme, but this does not preclude the possibility that $\chi_{\beta'}>0$ for some $\beta'\subset\beta$ with $|\beta'|\geq 2$.

The nullity of otherwise of the tail dependence coefficients is determined by which subspaces of the simplex are charged with $H$-mass. Specifically, $\chi_\beta >0$ if and only if there exists $\beta'\supseteq\beta$ such that
\begin{equation}\label{eq-asy-dep-chi-H}
    H(\{\bm{\theta}\in\mathbb{S}_+^{d-1} : \theta_i > 0 \iff i\in\beta'\}) >0.
\end{equation}
For example, consider the angular measures
\begin{equation}\label{eq-limiting-discrete-H}
    H^{(1)} = \frac{m}{d}\sum_{i=1}^d \delta_{\bm{e}_i}, \qquad H^{(2)} = m\delta_{\bm{1}_d/\|\bm{1}_d\|},
\end{equation}
where $\bm{e}_1,\ldots,\bm{e}_d$ denote the canonical basis vectors of $\R^d$. The measure $H^{(1)}$ places all its mass on the vertices of the simplex. This corresponds to full asymptotic independence, since then $\chi_\beta = 0$ for all $\beta\subseteq\{1,\ldots,d\}$ with cardinality at least equal to two. The angular measure $H^{(2)}$ concentrates at a single point at the centre of the simplex. This implies that $\chi_{\{1,\ldots,d\}}>0$ and consequently $\chi_{\beta}>0$ for all subsets $\beta$.

If the bivariate exponent measure $V_{ij}$ of $(X_i,X_j)$ is known, then the tail dependence coefficient $\chi_{ij}$ may be computed using the relation $\chi_{ij}=2-V_{ij}(1,1)$ [@colesDependenceMeasuresExtreme1999]. The following examples illustrate this for selected parametric models. 

:::{#exm-symmetric-logistic-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ be symmetric logistic distributed with dependence parameter $\gamma\in(0,1]$. For any $i\neq j$, let $V_{ij}$ denote the bivariate exponent measure of $(X_i,X_j)$. Then
\begin{equation*}
    \chi_{ij} = 2- V_{ij}(1,1) = 2 - \left[ \left(x_i^{-1/\gamma} + x_j^{-1/\gamma} \right)^\gamma \right] = 2-2^\gamma.
\end{equation*}
Therefore $X_i$ and $X_j$ are asymptotically independent when $\gamma=1$ and approach complete asymptotic dependence as $\gamma\to 0$.
:::

:::{#exm-husler-reiss-chi}
Let $\bm{X}=(X_1,\ldots,X_d)$ be Hüsler-Reiss distributed with parameter matrix $\Lambda=(\lambda_{ij}^2)$. For any $i\neq j$, let $V_{ij}$ denote the bivariate exponent measure of $(X_i,X_j)$. Then
\begin{equation*}
    \chi_{ij} = 2 - V_{ij}(1,1) = 2 - 2 \Phi\left(\lambda_{ij} + \frac{1}{2\lambda_{ij}}\log 1 \right) = 2 - 2\Phi(\lambda_{ij}),
\end{equation*}
where $\Phi$ is the standard normal distribution function. Variables $X_i$ and $X_j$ are asymptotically dependent for all $\lambda_{ij}>0$, with asymptotic independence in the limit as $\lambda_{ij}\to\infty$. *Refer back to this equation when discussing Hazra and Bose changepoint method -- it gives one-to-one relationship between HR parameter and dependence strength, so testing for change in $\lambda$ or $\chi$ are equivalent.*
:::

:::{#exm-max-linear-chi}
Suppose $\bm{X}=(X_1,\ldots,X_d)$ is max-linear with parameter matrix $A\in\R_+^{d\times q}$. Substituting \eqref{eq-max-linear-H} into \eqref{eq-exponent-measure} yields
\begin{equation}
    \chi_{ij} = 2 - V_{12}(1,1) = 2 - 2\int_{\mathbb{S}_{+(1)}^1} (\theta_1 \vee \theta_2) \,\dee H(\bm{\theta}) = 2 - \sum_{l=1}^q (a_{il} \vee a_{jl}).
\end{equation}
Consider two max-linear random vectors with discrete angular measures $H^{(1)}$ and $H^{(2)}$ as in \eqref{eq-limiting-discrete-H}. The parameter matrices are given by
\begin{equation*}
A^{(1)} =  I_d \in \R_+^{d\times d}, \qquad A^{(2)} = \bm{1}_d \in \R_+^{d\times 1}.
\end{equation*}
The tail dependence coefficients under these models are
\begin{equation*}
\chi_{ij}^{(1)} = 2 - \sum_{j=1}^2 \max(0,1) = 0, \qquad \chi_{ij}^{(2)} = 2 - \sum_{j=1}^1 \max(1,1) = 1,
\end{equation*}
corresponding to complete dependence and asymptotic dependence, as expected.
:::

Estimates of $\chi_{ij}$ are obtained by estimating $\hat{\chi}_{ij}(u)$ at a sequence of high quantiles $u$ approaching one. The \texttt{taildep} function in the R package \texttt{extRemes} achieves this using the estimator given in Equation 2.62 in @reissStatisticalAnalysisExtreme2007 and produces a diagnostic plot as shown in @fig-chi-estimation. For this example the data were generated from a symmetric logistic model with $\gamma=0.5$. The horizontal dashed line indicates the true value $\chi_{ij}=2-\sqrt{2}\approx 0.59$, while the blue points represent the estimates $\hat{\chi}_{ij}(u)$ over the range $0.8 \leq u \leq 0.995$. The shaded region depicts the 95\% Wald confidence interval. We encounter a bias-variance trade-off in relation to quantile/threshold, similar in nature to that described in Section XX with respect to the selecting the block size/threshold. 

```{r background-make-fig-chi-estimation}
#| label: fig-chi-estimation
#| fig-cap: "Empirical estimates $\\hat{\\chi}_{12}(u)$ of the tail dependence coefficient for bivariate symmetric logistic data with $\\gamma=0.5$ and $n=5,000$ observations. The true coefficient $\\chi_{12}=2-2^\\gamma\\approx 0.59$ is marked by the dashed line. The shaded region represents the 95\\% Wald confidence interval."
#| fig-scap: "Empirical estimates $\\hat{\\chi}_{12}(u)$ for bivariate symmetric logistic data."
#| fig-height: 2.8
#| fig-width: 5

set.seed(1)
gamma <- 0.5
X <- rmev(n = 5000, d = 2, par = 1 / gamma, model = "log")

taildep(X, meas = "chi", qlim = c(0.8, 0.995), plot = FALSE)$chi %>%
  as.data.frame() %>%
  mutate(q = seq(from = 0.8, to = 0.995, length.out = 40)) %>%
  ggplot(aes(x = q, y = coef)) +
  geom_ribbon(aes(ymax = upperci, ymin = lowerci), fill = "lightblue", colour = NA, alpha = 0.4) +
  geom_line(colour = "blue") +
  geom_point(colour = "blue") +
  geom_hline(yintercept = sl_chi(gamma), linetype = "dashed", colour = "black") +
  scale_x_continuous(limits = c(0.8, 1), expand = expansion(mult = c(0.01, 0)), breaks = breaks_extended(n = 8)) +
  theme_light() +
  labs(x = expression(u),
       y = expression(hat(chi)[12](u)))
```

Estimation of $\chi_\beta$ for $|\beta| > 2$ is more complicated and is related to the task of determining the support of the angular measure [@goixSparseRepresentationMultivariate2017; @meyerMultivariateSparseClustering2023; @simpsonDeterminingDependenceStructure2020]. This thesis primarily concerns dependence at the pairwise level, so we direct the reader to the aforementioned papers and the review @engelkeSparseStructuresMultivariate2021 for further details.

Let $\chi=(\chi_{ij})$ denote the Tail Dependence Matrix (TDM) of bivariate tail dependence coefficients with diagonal entries $\chi_{ii}:=1$. The TDM provides a high level summary of the extremal dependence structure. It has been applied for exploratory analysis [@huangNewExploratoryTools2019] and considered as a tool for clustering [@fomichovSphericalClusteringDetection2023]. Other works focus on its theoretical properties. @shyamalkumarTailDependenceMatrices2020 conjecture that the 'realisation problem' -- determining whether a given matrix is a valid TDM -- is NP-complete; this was recently proved by @janssenTaildependenceExceedanceSets2023. By establishing a correspondence between the class of TDMs and a metric space, @janssenTaildependenceExceedanceSets2023 also show that, in certain cases, higher order tail-dependence is determined by the bivariate TDM. Section XX introduces a similar (and similarly named) matrix, the Tail *Pairwise* Dependence Matrix (TPDM), which is the eponym of this thesis. Rather than the tail dependence coefficient $\chi_{ij}$, the TPDM is founded on an alternative bivariate summary measure called the Extremal Dependence Measure (EDM).

#### Extremal dependence measure

The extremal dependence measure (EDM) is a pairwise summary measure similar to $\chi_{ij}$. It was originally proposed @resnickExtremalDependenceMeasure2004 and later generalised by @larssonExtremalDependenceMeasure2012.

:::{#def-edm}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with angular measure $H$. The EDM between $X_i$ and $X_j$ is
\begin{equation}
    \mathrm{EDM}_{ij} := \int_{\mathbb{S}_{+}^{d-1}} \theta_i\theta_j \,\dee H(\bm{\theta}).
\end{equation}
:::

The EDM depends on the choice of norm via the angular measure, but @larssonExtremalDependenceMeasure2012 show that EDMs under different norms are equivalent in a certain sense. The EDM was originally defined by @resnickExtremalDependenceMeasure2004 for bivariate random vectors $\bm{X}=(X_1,X_2)$. In their definition, the integrand is
\begin{equation}
\left(\frac{4}{\pi}\right)^2 \arctan\left(\frac{\theta_2}{\theta_1}\right)\left[\frac{\pi}{2} - \arctan\left(\frac{\theta_2}{\theta_1}\right) \right].
\end{equation}
rather than $\theta_1\theta_2$. The original and refined versions are also equivalent. 

Being explicitly defined in terms of the angular measure, the EDM's interpretation in terms of AD/AI is straightforward. Recall from \eqref{eq-asy-dep-chi-H} that variables $X_i$ and $X_j$ are asymptotically independent if and only if $H(\{\bm{\theta}:\theta_i,\theta_j > 0\})=0$. Then
\begin{equation*}
\chi_{ij} = 0 \iff \int_{\{\bm{\theta}\in\mathbb{S}_{+}^{d-1} \, : \, \theta_i,\theta_j > 0\}} \theta_i \theta_j \,\dee H(\bm{\theta}) = 0 \iff \mathrm{EDM}_{ij}=0.
\end{equation*}
The EDM is maximal when $X_i$ and $X_j$ are perfectly asymptotically dependent. The maximal value depends on the choice of norm and the mass of the angular measure. When $d=2$ and $\|\cdot\|=\|\cdot\|_p$ we have $\mathrm{EDM}_{ij} \leq 2^{-2/p}m$ with equality if and only if $H$ places all its mass at the simplex barycentre, that is $H(\{(2^{-1/p}, 2^{-1/p})\}) = m$.

We return to the EDM in Section XX when introducing the tail pairwise dependence matrix.

## Inference

We now shift our attention to the topic of (non-parametric) inference in multivariate extremes. The general approach entails using the angular components of large observations to learn a model for $H$. This strategy is justified by the MRV assumption: \eqref{eq-polar-mrv} implies that
\begin{equation}\label{eq-extremal-angles-converge-H}
    \bm{\Theta}\mid (R>t) \stackrel{d}{\to} H(\cdot), \qquad (t\to\infty).
\end{equation}
The angular measure is the limiting distribution of the angles of exceedances of some radial threshold. By analogy to the peaks-over-threshold approach (Section XX), it suggests itself to base inference on the subset of data points whose norm exceeds some high fixed threshold. Increasing the threshold reduces the number of observations that enter into the estimators, and vice versa. It is generally more convenient to specify the desired number of threshold exceedances, denoted $k$, and set the threshold accordingly. This approach is most conveniently described using order statistics. 

### Framework and notation

Consider a $d$-dimensional MRV random vector $\bm{X}\in\mathcal{RV}_+^d(\alpha)$. Let $\bm{X}_1,\bm{X}_2,\ldots$ denote a sequence of independent copies of $\bm{X}$ and fix a norm $\|\cdot\|$ on $\R^d$. For $i\geq 1$, denote by
\begin{equation}
    R_i := \|\bm{X}_i\|, \qquad \bm{\Theta}_i := (\Theta_{i1},\ldots,\Theta_{id})=\frac{\bm{X}_i}{\|\bm{X}_i\|},
\end{equation}
the radial and angular components of $\bm{X}_i$ with respect to the chosen norm. Assume that the distribution of $\|\bm{X}\|$ is continuous. Then for any $n\geq 1$, there exists a permutation $\pi:\{1,\ldots,n\}\to\{1,\ldots,n\}$ such that 
\begin{equation*}
    \|\bm{X}_{(1),n}\| > \|\bm{X}_{(2),n}\| > \ldots > \|\bm{X}_{(n),n}\|,
\end{equation*}
where $\bm{X}_{(i),n}:=\bm{X}_{\pi(i)}$ for $i=1,\ldots,n$. The random variable $\|\bm{X}_{(j),n}\|$ is called the $j$th (upper) order statistic of $\{\|\bm{X}_i\|:i=1,\ldots,n\}$. Henceforth, we suppress the dependence on $n$ in our order statistic notation. Let the radial and angular components of $\bm{X}_{(i)}$ be denoted by
\begin{equation}
    R_{(i)} = \|\bm{X}_{(i)}\|, \qquad \bm{\Theta}_{(i)} = (\Theta_{(i),1},\ldots,\Theta_{(i),d})=\frac{\bm{X}_{(i)}}{\|\bm{X}_{(i)}\|}.
\end{equation}
Performing inference based on the $k=k(n)$ largest observations is equivalent to performing inference based on the set of observations whose norm exceeds the threshold $t=R_{(k+1)}$.

### Selecting the radial threshold or the number of exceedances

All estimators will require on choosing the number of extreme observations $k$ that enter into them. In theoretical analyses, it is customary to choose the sequence $\{k(n):n\geq 1\}$ such that
\begin{equation}\label{eq-k-rate-conditions}
    \lim_{n\to\infty}k(n)=\infty, \qquad \lim_{n\to\infty}\frac{k(n)}{n}=0.
\end{equation}
These arise as sufficient conditions for proving various asymptotic properties (e.g. consistency, asymptotic normality) of estimators. The condition $k\to\infty$ ensures that the number of extremes -- the effective sample size -- grows arbitrarily large. The second condition $k/n\to 0$ requires that the proportion of threshold exceedances becomes vanishingly small, ensuring that inference is targeting the tail. 
In practice, $n$ is fixed and selecting $k$ requires striking a balance between these two aspects. Choosing $k$ too small reduces the amount of available information and leads to unnecessarily high uncertainty. If $k$ is too large, we risk using data that does not reflect the extremal dependence structure leading to bias. An appropriate choice depends on both the sample size and the underlying distribution of $\bm{X}$. If the convergence in \eqref{eq-extremal-angles-converge-H} is rapid, then a low threshold may be adequate. Several threshold selection procedures have been proposed in univariate extremes (Section XX), but the literature on radial threshold selection is comparatively scant. By combining two sub-tests regarding (i) independence of the radial and angular components and (ii) regular variation of the radial component, @einmahlTestingMultivariateRegular2020 devise a formal procedure testing the validity of the MRV assumption. They suggest choosing the threshold by examining a plot of the sequence of p-values against $k$. The support-detection algorithm of @meyerMultivariateSparseClustering2023 chooses $k$ automatically via minimisation of a penalised log-likelihood. This procedure is specific to their setting and relies on additional technical assumptions. Most applied studies use a rule-of-thumb approach and/or produce a threshold stability plot checking the (in)sensitivity of some quantity to the choice of $k$ -- see @jiangPrincipalComponentAnalysis2020, @szemkusSpatialPatternsIndices2024 and @russellAnalyzingDependenceMatrices2018 for examples. 

### The empirical angular measure

Once the tuning parameter $k$ has been chosen, attention turns towards the extremal angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$. In view of \eqref{eq-extremal-angles-converge-H}, the empirical distribution of $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ is the natural non-parametric estimator for the angular measure.

:::{#def-empirical-angular-measure}
The empirical angular measure based on $\bm{X}_1,\ldots,\bm{X}_n$ is the random measure on $\mathbb{S}_{+}^{d-1}$ defined as
\begin{equation}
    \hat{H}(\cdot) := \frac{m}{k}\sum_{i=1}^n \delta_{\bm{\Theta}_i}(\cdot) \ind\{R_i > R_{(k+1)}\} = \frac{m}{k} \sum_{i=1}^k \delta_{\bm{\Theta}_{(i)}}(\cdot).
\end{equation}
:::

Note that $\hat{H}$ does not enforce the moment constraints \eqref{eq-H-mean-constraints}, so is not necessarily a valid angular measure. @einmahlMaximumEmpiricalLikelihood2009 construct an alternative non-parametric estimator that does enforce these restrictions, but it is limited to the bivariate setting. Proposition 3.3 in @janssenKmeansClusteringExtremes2020 establishes consistency $\hat{H}\overset{p}{\to}H$ of the empirical angular measure provided the level $k$ satisfies the rate conditions \eqref{eq-k-rate-conditions}. Their result holds for general norms in arbitrary dimensions. @clemenconConcentrationBoundsEmpirical2023 conduct a non-asymptotic (i.e. finite sample) analysis of $\hat{H}$, establishing high-probability bounds on the worst-case estimation error $\sup_{A\in\mathcal{A}}|H(A) - \hat{H}(A)|$ over classes $\mathcal{A}$ of Borel subsets on $\mathbb{S}_{+}^{d-1}$. Their results hold with $\|\cdot\|=\|\cdot\|_p$ for $p\in[1,\infty]$. Since $\hat{H}$ is a discrete measure concentrating at $k$ points, there exists a max-linear random vector $\bm{X}$ with parameter matrix 
\begin{equation}\label{eq-empirical-A}
    \hat{A} := \left(\frac{m}{k}\right)^{1/\alpha}\left(\bm{\Theta}_{(1)}, \ldots, \bm{\Theta}_{(k)}\right) \in\R_+^{d\times k}.
\end{equation}
whose angular measure is $\hat{H}$. Estimates of tail event probabilities under the empirical model $\hat{H}$ may then be computed using the formula \eqref{eq-max-linear-failure-probability}.

### Non-parametric estimators

@larssonExtremalDependenceMeasure2012 remark that analysing extremal dependence often involves quantities of the form
\begin{equation}\label{eq-expectation-f}
    \mathbb{E}_{H}[f(\bm{\Theta})] := \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee H(\bm{\theta}) = \mathbb{E}_{m^{-1}H}[mf(\bm{\Theta})],
\end{equation}
where $f:\mathbb{S}_{+}^{d-1}\to\R$. We have already seen an example of this in @def-edm: the EDM between $X_i$ and $X_j$ is defined as \eqref{eq-expectation-f} with $f(\bm{\theta})=\theta_i\theta_j$. We reiterate that in our notation, the expectation is with respect to a measure $H$ that is not necessarily normalised. When manipulating expectations/variances, the following relations may be useful to bear in mind:
\begin{align*}
\mathbb{E}_{H} [f(\bm{\Theta})] &= \mathbb{E}_{m^{-1}H}[m f(\bm{\Theta})] = m \mathbb{E}_{m^{-1}H}[f(\bm{\Theta})] \\ 
\mathrm{Var}_{H}[f(\bm{\Theta})] &= \mathbb{E}_{m^{-1}H} [m^2 f(\bm{\Theta})^2] - \mathbb{E}_{m^{-1}H} [mf(\bm{\Theta})]^2 = m^2 \mathrm{Var}_{m^{-1}H}[f(\bm{\Theta})].
\end{align*}
@kluppelbergEstimatingExtremeBayesian2021 opt to normalise $H$ and absorb $m$ into $f$. For example, the EDM would correspond to $f(\bm{\theta})=m\theta_i\theta_j$ in their notation. Suppressing the normalising constant arguably results in less cumbersome notation, but in any case the choice is purely stylistic.

To construct non-parametric estimators of quantities \eqref{eq-expectation-f}, we simply replace $H$ with the empirical angular measure $\hat{H}$, yielding [@kluppelbergEstimatingExtremeBayesian2021]
\begin{equation}
    \hat{\mathbb{E}}_{H} [f(\bm{\Theta})] := \mathbb{E}_{\hat{H}}[f(\bm{\Theta})] = \int_{\mathbb{S}_{+}^{d-1}} f(\bm{\theta})\,\dee \hat{H}(\bm{\theta}) = \frac{m}{k}\sum_{i=1}^k f(\bm{\Theta}_{(i)}).
\end{equation}

@kluppelbergEstimatingExtremeBayesian2021 prove asymptotic normality of these estimators by generalising a result in @larssonExtremalDependenceMeasure2012. 

:::{#thm-clt-extremes}
Let $f:\mathbb{S}_+^{d-1}\to\R$ be continuous and assume $k$ satisfies the rate conditions $\eqref{eq-k-rate-conditions}$. Moreover, suppose that
\begin{equation}\label{eq-clt-rate-condition}
    \lim_{n\to\infty} \sqrt{k}\left[\frac{n}{k}\mathbb{E}[f(\bm{\Theta}_1)\ind\{R_1 \geq b_{\lfloor n/k \rfloor} t^{-1/\alpha}\}] - \mathbb{E}_{H}[f(\bm{\Theta})]\frac{n}{k}\bar{F}_R(b_{\lfloor n/k \rfloor} t^{-1/\alpha})\right]=0
\end{equation}
holds locally uniformly for $t\in[0,\infty)$, where $\bar{F}_R(\cdot)=\mathbb{P}(R>\cdot)$ denotes the survivor function of $R$. Finally, assume that
\begin{equation}\label{eq-clt-variance-condition}
    \nu^2 := \mathrm{Var}_{H}(f(\bm{\Theta})) > 0.
\end{equation}
Then
\begin{equation}
    \sqrt{k}\left[\hat{\mathbb{E}}_{H} [f(\bm{\Theta})] - \mathbb{E}_{H} [f(\bm{\Theta})]\right]\to N(0, \nu^2), \qquad (n\to\infty).
\end{equation}
:::

The rate condition \eqref{eq-clt-rate-condition} requires that the dependence between the radius and angle decays sufficiently quickly. This condition is non-observable and must be assumed.

:::{#exm-edm-estimator}
Let $\bm{X}_1,\ldots,\bm{X}_n$ be independent copies of $\bm{X}\in\mathcal{RV}_+^d(\alpha)$. The estimator for the EDM between $X_i$ and $X_j$ is
\begin{equation*}
\widehat{\mathrm{EDM}}_{ij} := \hat{\mathbb{E}}_{H} [\Theta_i\Theta_j] \frac{m}{k}\sum_{l=1}^k \Theta_{(l),i}\Theta_{(l),j}.
\end{equation*}
Under the conditions of @thm-clt-extremes,
\begin{equation*}
\sqrt{k}[\widehat{\mathrm{EDM}}_{ij} - \mathrm{EDM}_{ij}] \to N(0, \nu_{ij}^2), \qquad \nu_{ij}^2 = \mathrm{Var}_H(\Theta_i\Theta_j).
\end{equation*}
:::


## Tail pairwise dependence matrix (TPDM)

This section introduces the key protagonist of this thesis: the tail pairwise dependence matrix (TPDM).

### Definition and examples

*Preamble.*

:::{#def-tpdm}
Let $\bm{X}\in\mathcal{RV}_+^d(2)$ with normalising sequence $b_n=n^{1/2}$. Let $H$ denote the angular measure with respect to $\|\cdot\|_2$. The TPDM of $\bm{X}$ is the $d\times d$ matrix
\begin{equation}\label{eq-tpdm}
    \Sigma = (\sigma_{ij}), \qquad \sigma_{ij} = \int_{\mathbb{S}_{+(2)}^{d-1}} \theta_i \theta_j \,\dee H(\bm{\theta}) = \mathbb{E}_{H} [\Theta_i\Theta_j].
\end{equation}
:::

The TPDM is essentially a matrix of EDMs subject to additional restrictions on the tail index, normalising sequence, and norm. Each off-diagonal entry $\sigma_{ij}$ may be interpreted as summarising the dependence between $X_i$ and $X_j$, with $\sigma_{ij}=0$ if and only if the corresponding variables are asymptotically independent. The original definition was generalised by @kirilioukEstimatingProbabilitiesMultivariate2022 to permit general $\alpha$.

:::{#def-tpdm-alpha}
For $\alpha\geq 1$, let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with normalising sequence $b_n=n^{1/\alpha}$. Let $H$ denote the angular measure with respect to $\|\cdot\|_\alpha$. The TPDM of $\bm{X}$ is the $d\times d$ matrix
\begin{equation}\label{eq-tpdm-alpha}
    \Sigma = (\sigma_{ij}), \qquad \sigma_{ij} = \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee H(\bm{\theta}) = \mathbb{E}_{H} [\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}].
\end{equation}
:::

The tail index of $\bm{X}$ is now arbitrary, but the normalisation sequence and norm are still required to conform with this index. It is obvious that these definitions coincide when $\alpha=2$, but @kirilioukEstimatingProbabilitiesMultivariate2022 provide no direct rationale for why \eqref{eq-tpdm-alpha} is the natural generalisation of \eqref{eq-tpdm}. Appendix XX provides a series of results shedding light on this matter. After generalising a result in @fixSimultaneousAutoregressiveModels2021 (@lem-angular-density-transformation), we prove that the TPDM is invariant to the choice of $\alpha$ (@prp-tpdm-h1-formula). This culminates in an expression for the TPDM (for any $\alpha$) in terms of the $L_1$ angular density that does not depend on $\alpha$. We now use of this formula and the angular densities in @semadeniInferenceAngularDistribution2020 to compute the TPDM under the symmetric logistic and Hüsler-Reiss models. These model TPDMs will be especially useful in Chapter XX for evaluating the performance of TPDM estimators.

:::{#exm-symmetric-logistic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ follows the symmetric logistic distribution with dependence parameter $\gamma\in(0,1)$. For any $i\neq j$,
\begin{equation}\label{eq-symmetric-logistic-tpdm}
   \sigma_{ij} = \frac{1-\gamma}{\gamma} \int_0^1 [u(1-u)]^{\frac{1}{\gamma}-\frac{3}{2}}[(1-u)^{1/\gamma} + u^{1/\gamma}]^{\gamma-2}\,\dee u.
\end{equation}
:::

:::{#exm-husler-reiss-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ follows the Hüsler-Reiss distribution with parameter matrix $\Lambda=(\lambda_{ij}^2)$. For any $i\neq j$, 
\begin{equation}\label{eq-husler-reiss-tpdm}
    \sigma_{ij} = \int_0^1 \frac{\exp(-\lambda_{ij}/4)}{2\lambda_{ij} u(1-u)} \phi\left(\frac{1}{2\lambda_{ij}}\log\left(\frac{u}{1-u}\right)\right) \,\dee u.
\end{equation}
:::

The blue lines in @fig-parametric-chi-tpdm plot \eqref{eq-symmetric-logistic-tpdm} and \eqref{eq-husler-reiss-tpdm} against the model parameter. For comparison, we also include the tail dependence coefficients (red lines) computed using @exm-symmetric-logistic-chi and @exm-husler-reiss-chi. For both models, the strength of association is a decreasing function of the model parameter, with complete dependence  (resp. asymptotic independence) as the parameter approaches zero (resp. its upper limit). For the Hüsler-Reiss distribution, dependence is very weak beyond $\lambda\approx 3$. We can check that this is correct by comparing with Figure 1 in the Supplementary Material of @cooleyDecompositionsDependenceHighdimensional2019. The figure reveals that for a Brown-Resnick process with semi-variogram \eqref{eq-fractal-variogram} with range $\rho=2.4$ and smoothness $\kappa=1.8$, dependence vanishes beyond a distance of approximately 12 units. Recall from Section XX that the dependence between two sites $h$ units apart under the Brown-Resnick model is equivalent to the dependence between two Hüsler-Reiss variables with dependence parameter $\lambda_{ij}=\sqrt{2(h/\rho)^\kappa}/2$. Setting $h=12$ gives $\lambda_{ij}=\sqrt{2(12/2.4)^{1.8}}/2\approx 3.01$, corroborating the results of @fig-parametric-chi-tpdm. Further verification of our expressions are provided by the shaded regions in @fig-parametric-chi-tpdm. These represent the minimum/maximum values of 10 estimates of $\chi_{ij}$ and $\sigma_{ij}$ for a sequence of values of $\gamma$ and $\lambda$. The estimates are obtained from large samples ($n=5\times 10^5$) so it is reasonable to neglect the influence of estimation error. The empirical estimates agree with our calculations.


```{r background-make-fig-parametric-chi-tpdm}
#| label: fig-parametric-chi-tpdm
#| fig-cap: "True dependence strengths for the symmetric logistic (left) and Hüsler-Reiss (right) models, measured using the tail dependence coefficient (red line) and TPDM (blue line). The shaded regions represent the minimum/maximum values of empirical estimates over 10 repeated simulations using bivariate samples of size $n=5\\times 10^5$."
#| fig-scap: "Dependence $\\chi$ and $\\sigma$ for symmetric logistic and Hüsler-Reiss models."
#| fig-height: 3.5

data <- readRDS(file.path("scripts", "background", "results", "parametric_chi_tpdm_empirical.RDS")) 

p1 <- ggplot() + 
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) +
  geom_function(aes(colour = "chi"), fun = sl_chi, xlim = c(0, 1)) + 
  geom_function(aes(colour = "tpdm"), fun = sl_tpdm, xlim = c(0, 1)) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 5)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(gamma), 
       y = "Dependence strength", 
       title = "Symmetric logistic") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10))  

p2 <- ggplot() + 
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) + 
  geom_function(aes(colour = "chi"), fun = hr_chi, xlim = c(0, 3.5)) + 
  geom_function(aes(colour = "tpdm"), fun = hr_tpdm, xlim = c(0, 3.5)) + 
  scale_x_continuous(limits = c(0, 3.5), expand = c(0, 0), breaks = breaks_extended(n = 4)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(lambda), 
       y = "Dependence strength", 
       title = "Hüsler-Reiss") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10)) 

ggarrange(p1, p2, ncol = 2, common.legend = TRUE) 
``` 

The angular measure of a max-linear random vector is discrete, so the angular density does not exist. Nevertheless, it is straightforward to compute the model TPDM directly from the definition [@cooleyDecompositionsDependenceHighdimensional2019; @kirilioukEstimatingProbabilitiesMultivariate2022].

:::{#exm-max-linear-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is max-linear with parameter matrix $A$. Then for any $i\neq j$,
\begin{align*}
\sigma_{ij} 
&= \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^{\alpha/2}\theta_j^{\alpha/2} \,\dee H(\bm{\theta}) \\
&= \sum_{l=1}^q \|\bm{a}_{l}\|_\alpha^\alpha \left(\frac{a_{li}}{\|\bm{a}_{l}\|_\alpha}\right)^{\alpha/2}\left(\frac{a_{lj}}{\|\bm{a}_{l}\|_\alpha}\right)^{\alpha/2} \\
&= \sum_{l=1}^q a_{il}^{\alpha/2}a_{jl}^{\alpha/2}.
\end{align*}
Therefore $\Sigma=A^{\alpha/2}(A^{\alpha/2})^T$. Taking $A$ to be $A^{(1)}$ and $A^{(2)}$ as defined in @exm-max-linear-chi, the corresponding TPDMs are
\begin{equation*}
\Sigma^{(1)} = I_d I_d^T  = I_d, \qquad
\Sigma^{(2)} = \bm{1}_d\bm{1}_d^T = J_d,
\end{equation*}
where $J_d$ is the $d\times d$ all-ones matrix. By construction, these represent the TPDMs under asymptotic dependence and complete dependence, respectively.
:::

The connection between $A$ and $\Sigma$ will play a prominent role in this thesis. *Say more about this?*

### Interpretation of the TPDM entries

The definition of the TPDM
\begin{equation}\label{eq-tpdm-covariance-form}
    \Sigma = \mathbb{E}_{H}\left[\bm{\Theta}^{\alpha/2}(\bm{\Theta}^{\alpha/2})^T\right],
\end{equation}
bears a striking resemblance to the definition of a covariance matrix in the non-extreme setting. The covariance matrix represents the second-order (central) moment of a random vector. Its diagonal entries convey the scale (variance) of the components, while the off-diagonal entries summarise the strength of association (unnormalised correlation) between all pairs of variables. The TPDM entries offer analogous interpretations, except the notions of scale and association are adapted to refer to properties of the joint distributional tail.

:::{#def-scale}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with normalisation sequence $b_n$. For $i=1,\ldots,d$, the scale of $X_i$ is defined as [@kluppelbergEstimatingExtremeBayesian2021]
\begin{equation*}
    \mathrm{scale}(X_i) = \left[\int_{\mathbb{S}_+^{d-1}}\theta_i^\alpha\,\dee H(\bm{\theta})\right]^{1/\alpha}.
\end{equation*}
:::

As discussed earlier, a well-defined notion of scale must fix either the sequence $b_n$ or the mass of the angular measure in advance. In the above definition, the normalisation sequence is fixed and scaling information is contained in $H$. The scale is so-called because it yields information about the scale of the marginal distributions. Using \eqref{eq-nu-H-relation}, one can show that
\begin{align*}
    \lim_{n\to\infty} n\mathbb{P}(b_n^{-1}X_i > x)
    &= \int_{\mathbb{S}_{+(\alpha)}^{d-1}}\int_{x/\theta_i}^\infty \alpha r^{-\alpha-1}\,\dee r\,\dee H(\bm{\theta}) \\
    &= \int_{\mathbb{S}_{+(\alpha)}^{d-1}} [r^{-\alpha}]_{\infty}^{x/\theta_i} \,\dee H(\bm{\theta}) \\
    &= x^{-\alpha} [\mathrm{scale}(X_i)]^\alpha,
\end{align*}
Moreover, it behaves like a measure of scale: for any $c>0$,
\begin{align*}
    \mathrm{scale}(cX_i) &= 
    \left[\frac{\lim_{n\to\infty} n\mathbb{P}(b_n^{-1}cX_i > x)}{x^{-\alpha}}\right]^{1/\alpha} \\
    &= \left[c^\alpha\frac{\lim_{n\to\infty} n\mathbb{P}(b_n^{-1}X_i > x/c)}{(x/c)^{-\alpha}}\right]^{1/\alpha}\\
    &=c \, \cdot \, \mathrm{scale}(X_i).
\end{align*}
Comparing @def-scale against @def-tpdm-alpha, the diagonal entries of the TPDM are related to the marginal scales via $\mathrm{scale}(X_i)=\sigma_{ii}^{1/\alpha}$. Consequently, if the marginal distributions are standardised to have unit scales, then all diagonal entries of the TPDM are equal to one. Moreover, when $b_n=n^{1/\alpha}$ and $\|\cdot\|=\|\cdot\|_{\alpha}$, the mass of the angular measure relates to the marginal scales via
\begin{equation*}
  \sum_{i=1}^d \sigma_{ii}
    = \sum_{i=1}^d \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \theta_i^\alpha \,\dee H(\bm{\theta}) 
    =  \int_{\mathbb{S}_{+(\alpha)}^{d-1}} \sum_{i=1}^d \theta_i^\alpha \,\dee H(\bm{\theta}) 
    = \int_{\mathbb{S}_{+(\alpha)}^{d-1}}\dee H(\bm{\theta})
    = m.
\end{equation*}
In this thesis, all random vectors will be pre-processed to be on $\alpha$-Fréchet margins and we take $b_n=n^{1/\alpha}$, so that
\begin{align*}
  \sigma_{ii} 
    &= \mathrm{scale}(X_i)^\alpha \\
    &= \frac{\lim_{n\to\infty} n\mathbb{P}(X_i > n^{1/\alpha}x)}{x^{-\alpha}} \\
    &= \frac{\lim_{n\to\infty} n \left\lbrace 1 - \exp\left[-(n^{1/\alpha}x)^{-\alpha}\right] \right\rbrace}{x^{-\alpha}} \\
    &= 1,
\end{align*}
and 
\begin{equation*}
m = \sum_{i=1}^d \sigma_{ii} = d.
\end{equation*}
Standardising the margins is akin to working with re-scaled variables with unit variance in the non-extremes setting. The appropriate analogue to the TPDM then becomes the correlation rather than covariance matrix.

As mentioned earlier, the TPDM's off-diagonal entries are simply pairwise EDMs. Thus the interpretation of $\sigma_{ij}$ is inherited from the EDM: $X_i$ and $X_j$ are asymptotically independent if and only $\sigma_{ij}=0$, and the magnitude of $\sigma_{ij}>0$ reveals the strength of tail dependence between $X_i$ and $X_j$. Like a correlation matrix, $\sigma_{ij}$ attains its maximal value (one) when $X_i$ and $X_j$ are completely dependent (@exm-max-linear-tpdm).


### Decompositions of the TPDM

The TPDM is useful as a summary statistic for quantifying pairwise dependencies, but what sets it apart from other pairwise dependence matrices (e.g. the TDM)? The TPDM admits two types of decomposition: eigendecomposition and the completely positive decomposition [@cooleyDecompositionsDependenceHighdimensional2019]. These underpin the key statistical applications of the TPDM described in Section XX. The following results and proofs are reproduced from @kirilioukEstimatingProbabilitiesMultivariate2022.

:::{#prp-tpdm-symmetric-positive-definite}
The TPDM is symmetric and positive semi-definite. 
:::

::: {.proof}
For any $i,j=1,\ldots,d$,
\begin{equation*}
    \sigma_{ij} = \int_{\mathbb{S}_+^{d-1}} \theta_i^{\alpha/2} \theta_j^{\alpha/2} \,\dee H(\bm{\theta}) = \int_{\mathbb{S}_+^{d-1}} \theta_j^{\alpha/2} \theta_i^{\alpha/2} \,\dee H(\bm{\theta}) =\sigma_{ji}.
\end{equation*}
Hence $\Sigma=\Sigma^T$. For any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation*}
  \bm{y}^T \Sigma \bm{y} 
  = \bm{y}^T \mathbb{E}_{H}[\bm{\Theta}^{\alpha/2}(\bm{\Theta}^{\alpha/2})^T] \bm{y} 
  = \mathbb{E}_{H}\left[\left(\bm{y}^T \bm{\Theta}^{\alpha/2} \right)^2 \right] \geq 0.
\end{equation*}
:::

By standard linear algebra results, the TPDM can be decomposed as $\Sigma=UDU^T$, where $D\in\R^{d\times d}$ is a diagonal matrix of eigenvalues $\lambda_1 \geq \ldots \geq \lambda_d \geq 0$ and $U\in\R^{d\times d}$ is an orthogonal matrix whose columns are the corresponding eigenvectors $\bm{u}_1,\ldots,\bm{u}_d\in\R^d$. The eigendecomposition potentially offers a low-rank representation of the TPDM expressed in terms of its eigenvalues and eigenvectors. In contrast, the completely positive decomposition represents the matrix as a product of a (potentially low-rank) non-negative matrix with its transpose.

:::{#def-cp}
A matrix $M\in\R^{d\times d}$ is completely positive (CP) if there exists a matrix $B\in\R_+^{d\times q}$ such that $M=BB^T$.
:::

:::{#prp-tpdm-completely-positive}
The TPDM is completely positive.
:::

::: {.proof}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ with angular measure $H$ and TPDM $\Sigma$. By Proposition 5 in @fougeresDenseClassesMultivariate2013, there exists a sequence of matrices $\{A_q\in\R_+^{d\times q}:q\geq 1\}$ such that $H_q\overset{v}{\to} H$, where $H_q$ is the angular measure of the max-linear random vector $\bm{X}_q\in\mathcal{RV}_+^d(\alpha)$ parametrised by $A_q$. The TPDM of $\bm{X}_q$ is $\Sigma_q=A_q^{\alpha/2}(A_q^{\alpha/2})^T$ by @exm-max-linear-tpdm. Thus, $\{\Sigma_q : q\geq 1\}$ is a sequence of completely positive matrices. The limit $\lim_{q\to\infty}\Sigma_q=\Sigma$ must also be completely positive because the set of completely positive matrices is closed [@haufmannCompletelyPositiveMatrices2011, Theorem 2.1.9].
:::

In principle this provides a way to check whether a given matrix is a TPDM, but the membership problem for the completely positive cone is NP-hard [@dickinsonComputationalComplexityMembership2014]. The following example illustrates how these two decompositions apply to the symmetric logistic model and hints towards their use for dimension reduction, to be formalised in Section XX.

:::{#exm-symmetric-logistic-tpdm-decomposition}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is symmetric logistic with parameter $\gamma\in(0,1]$. Then 
\begin{equation*}
\Sigma = (1-\sigma)I_d + \sigma J_d,
\end{equation*}
where the constant $\sigma$ depends on $\gamma$ via the formula in @exm-symmetric-logistic-tpdm. The eigenvalues of $\Sigma$ are $\lambda_1=1+(d-1)\sigma$ and $\lambda_2 = \ldots = \lambda_d = 1-\sigma$. The principal eigenvector is $\bm{u}_1=d^{-1/2}\bm{1}_d$ and the remaining eigenvectors $\bm{u}_2,\ldots,\bm{u}_d$ are orthogonal to $\bm{u}_1$. Rewriting the TPDM as
\begin{equation*}
\Sigma = \sum_{i=1}^d(1-\sigma)\bm{e}_i\bm{e}_i^T + \sigma\bm{1}_d\bm{1}_d^T,
\end{equation*}
and using @exm-max-linear-tpdm, the TPDM of $\bm{X}$ is identical to that of a max-linear random vector $\bm{Y}=(Y_1,\ldots,Y_d)\in\mathcal{RV}_+^d(\alpha)$ with parameter matrix
\begin{equation*}
A = 
\begin{pmatrix}
(1-\sigma)^{1/\alpha} & 0 & 0 & \cdots & 0 & \sigma^{1/\alpha} \\
0 & (1-\sigma)^{1/\alpha} & 0 & \cdots & 0 & \sigma^{1/\alpha} \\
0 & 0 & (1-\sigma)^{1/\alpha} & \cdots & 0 & \sigma^{1/\alpha} \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
0 & 0 & 0 & \cdots & (1-\sigma)^{1/\alpha} & \sigma^{1/\alpha}
\end{pmatrix}.
\end{equation*}
Consider the limiting case of complete dependence as $\gamma\to 0$, whereby the angular measure tends towards $H^{(2)}$ from \eqref{eq-limiting-discrete-H} in the limit. The eigenvalues are $\lambda_1\to d$, $\lambda_2,\ldots,\lambda_d\to 0$, indicating a single eigenvector $\bm{u}_1$ is sufficient to fully `explain' the dependence structure. We also have $A\to(\bm{0},\ldots,\bm{0},\bm{1}_d)=\bm{1}_d=A^{(2)}$. (This is an abuse of notation; we simply mean that zero columns have no effect and may be omitted.) Both perspectives point towards a low-rank representation of the dependence structure involving the vector directed towards the centre of the simplex. Indeed, this perfectly describes $H^{(2)}=d\delta_{\bm{1}_d/\|\bm{1}_d\|_{\alpha}}$.
:::

### The empirical TPDM

:::{#def-empirical-tpdm}
Let $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ on Fréchet margins \eqref{eq-alpha-frechet} and let $H$ be the angular measure with respect to $\|\cdot\|_\alpha$ and normalising sequence $b_n=n^{1/\alpha}$. Let $\bm{X}_1,\ldots,\bm{X}_n$ be an iid sample of $\bm{X}$. The empirical TPDM estimator is the $d\times d$ matrix
\begin{equation}\label{eq-empirical-tpdm}
  \hat{\Sigma} = (\hat{\sigma}_{ij}), \qquad 
  \hat{\sigma}_{ij} := \hat{E}_H[\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}]=\frac{d}{k}\sum_{l=1}^k \Theta_{(l),i}^{\alpha/2}\Theta_{(l),j}^{\alpha/2}.
\end{equation}
:::

Note that the empirical TPDM implicitly depends on the customary tuning parameter $k$ -- or equivalently a radial threshold $t>0$ -- via the empirical angular measure. 

:::{#prp-empirical-tpdm-completely-positive}
The empirical TPDM is completely positive.
:::

::: {.proof}
Let $A=\hat{A}$, the $d\times k$ matrix with non-negative entries defined in \eqref{eq-empirical-A}. Then
\begin{equation*}
    \hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T = \frac{d}{k} \sum_{i=1}^k \bm{\Theta}_{(i)}^{\alpha/2} \left(\bm{\Theta}_{(i)}^{\alpha/2}\right)^T = \hat{\Sigma}.
\end{equation*}
:::

:::{#prp-empirical-tpdm-symmetric-positive-definite}
The empirical TPDM is symmetric and positive semi-definite.
:::

::: {.proof}
By complete positivity, $\hat{\Sigma}=AA^T$ for some matrix $A$. For any $\bm{y}\in\R^d\setminus\{\bm{0}\}$,
\begin{equation}
    \bm{y}^T\hat{\Sigma}\bm{y} = \bm{y}^T AA^T \bm{y} = \|A^T\bm{y}\|_2^2 \geq 0.
\end{equation}
Since $\mathrm{rank}(\hat{\Sigma})=\mathrm{rank}(AA^T)=\mathrm{rank}(A)$, the empirical TPDM is positive definite if and only if the columns of $A$ are linearly independent.
:::

:::{#prp-empirical-tpdm-normality-entries}
Under the conditions of @thm-clt-extremes, the entries of $\hat{\Sigma}$ are consistent and asymptotically normal, that is, for any $i,j=1,\ldots,d$,
\begin{equation}
\sqrt{k}(\hat{\sigma}_{ij} - \sigma_{ij}) \to\mathrm{N}(0,\nu_{ij}^2), \qquad 
\nu_{ij}^2 := \mathrm{Var}_{H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2}).
\end{equation}
:::  

::: {.proof}
See @exm-edm-estimator. 
:::

If $X_i$ and $X_j$ are asymptotically independent ($\sigma_{ij}=0$), then $\nu_{ij}^2=0$ and the limit distribution is degenerate. In this case, the above result only proves consistency, i.e. $\hat{\sigma}_{ij}\to 0$, and cannot be used to formally test for asymptotic independence [@lehtomaaAsymptoticIndependenceSupport2020].

Using asymptotic normality one may construct asymptotic confidence intervals
\begin{equation}\label{eq-tpdm-confidence-interval}
    \lim_{n\to\infty}\mathbb{P}\left[ |\sigma_{ij}-\hat{\sigma}_{ij}| < z_{\beta/2}\sqrt{\nu_{ij}^2/k}\right] = 1-\beta,
\end{equation}
where $z_{\beta/2}=\Phi^{-1}(1-\beta/2)$. If the angular measure is known the asymptotic variance $\nu_{ij}^2$ may be computed using the formula derived in Appendix XX.


:::{#exm-symmetric-logistic-asymptotic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_d)\in\mathcal{RV}_+^d(\alpha)$ is symmetric logistic with $\gamma=0.6$. Using  @exm-symmetric-logistic-tpdm and results in Appendix XX, $\sigma_{ij}\approx 0.760$ and $\nu_{ij}^2 \approx 0.065$ for all $i\neq j$. For sufficiently large $n$, 
\begin{equation*}
    \mathbb{P}\left[\hat{\sigma}_{ij} \in \left(0.760 \pm 1.96 \sqrt{\frac{0.065}{k}}\right)\right] \approx 0.95.
\end{equation*}
For example, setting $n=10^4$ and $k=\sqrt{n}$ yields $\mathbb{P}(0.710 < \hat{\sigma}_{ij} < 0.810 )\approx 0.95.$
:::

In practice, the asymptotic variance may be replaced with the plug-in estimator [@leePartialTailCorrelation2023]
\begin{equation*}
\hat{\nu}_{ij}^2 := \frac{1}{k-1} \sum_{l=1}^k \left(d \Theta_{(l),i}\Theta_{(l),j} - \hat{\sigma}_{ij} \right)^2.
\end{equation*}

The following result, proved by @kraliCausalityEstimationMultivariate2018 for $\alpha=2$, generalises asymptotic normality of the empirical TPDM to the entire matrix, rather than just individual entries. This is most simply expressed in terms of upper-half vectorisations of $\Sigma$ and $\hat{\Sigma}$, that is
\begin{align}
    \bm{\sigma}
    &:= \mathrm{vecu}(\Sigma) 
    := (\sigma_{12},\sigma_{13},\ldots,\sigma_{1d},\sigma_{23},\ldots,\sigma_{2d},\ldots,\sigma_{d-1,d}), \label{eq-vecu-sigma} \\
    \hat{\bm{\sigma}} 
    &:= \mathrm{vecu}(\hat{\Sigma}) 
    := (\hat{\sigma}_{12},\hat{\sigma}_{13},\ldots,\hat{\sigma}_{1d},\hat{\sigma}_{23},\ldots,\hat{\sigma}_{2d},\ldots,\hat{\sigma}_{d-1,d}). \label{eq-vecu-sigma-hat}
\end{align}
Each vector contains 
\begin{equation*}
|\{(i,j):1\leq i < j \leq d\}|=\binom{d}{2}=\frac{1}{2}d(d-1)
\end{equation*}
entries. This is justified because the matrices are symmetric and their diagonal entries are irrelevant. Components are indexed according to the sub-indices of the corresponding matrix entry, e.g. the first entry of $\bm{\sigma}$ is $\sigma_{12}$ rather than $\sigma_{1}$.

:::{#prp-empirical-tpdm-normality}
Under the conditions of @thm-clt-extremes, the estimator $\hat{\bm{\sigma}}$ is consistent and asymptotically normal, i.e. 
\begin{equation*}
    \sqrt{k}(\hat{\bm{\sigma}}-\bm{\sigma}) \to N(\bm{0},V),
\end{equation*}
The diagonal and off-diagonal entries of the ${d \choose 2}\times{d \choose 2}$ asymptotic covariance matrix $V$ are given by
\begin{equation*}
   v_{ij,lm} 
   := \lim_{n\to\infty}k\mathrm{Cov}(\hat{\sigma}_{ij},\hat{\sigma}_{lm}) 
   = \begin{cases}
      \nu_{ij}^2, & (i,j) = (l,m),\\
      \rho_{ij,lm} & \text{otherwise},
    \end{cases}
\end{equation*}
where $\nu_{ij}^2$ is as defined in @prp-empirical-tpdm-normality-entries and
\begin{equation*}
    \rho_{ij,lm} := \frac{1}{2}\left[\mathrm{Var}_{H}(\Theta_i^{\alpha/2}\Theta_j^{\alpha/2} + \Theta_l^{\alpha/2}\Theta_m^{\alpha/2}) - \nu_{ij}^2 - \nu_{lm}^2\right].
 \end{equation*}
:::

The proof can be found in Appendix XX. It extends the proof of Theorem 5.23 in @kraliCausalityEstimationMultivariate2018 to permit general $\alpha$. The following example illustrates an application of @prp-empirical-tpdm-normality to the max-linear model.

:::{#exm-max-linear-asymptotic-tpdm}
Suppose $\bm{X}=(X_1,\ldots,X_4)\in\mathcal{RV}_+^4(1)$ is max-linear with (randomly generated) parameter matrix $A\in\R_+^{4\times 12}$ as shown in @fig-max-linear-example-A-Sigma-V (top). The TPDM $\Sigma=A^{1/2}(A^{1/2})^T$ is visualised in the bottom-left plot, with each cell's colour intensity representing the magnitude of the corresponding entry of $\Sigma$. All pairs of components exhibit strong dependence. The matrix in the bottom-right is the asymptotic covariance matrix $V$ of $\hat{\bm{\sigma}}$, derived in Appendix XX. It has ${4\choose 2}=6$ rows and columns. *Any comments about the matrix itself?* We now run simulations verifying/illustrating @prp-empirical-tpdm-normality for this example. We generate $n=10^4$ independent observations $\bm{x}_1,\ldots,\bm{x}_n$ of $\bm{X}=A\times_{\max}\bm{Z}$ (see eq-max-linear-X) and compute the empirical TPDM using $k=\sqrt{n}=100$ extremes. Repeating this process, we obtain 1,000 independent realisations of $\hat{\Sigma}$. After row-wise vectorisation, these estimates should be approximately $N(\bm{\sigma}, k^{-1}V)$ distributed. @fig-max-linear-example-ggpairs examines whether this is the case. First consider the diagonal panels. These show that the density function of an $N(\sigma_{ij},\nu_{ij}^2/k)$ random variable (blue curve) provides a good fit for the empirical distribution of $\hat{\sigma}_{ij}$ (red histogram). Now consider the scatter plots in the lower triangular portion of the plot. The grey points represent 1,000 realisations of $(\hat{\sigma}_{ij},\hat{\sigma}_{lm})$. The blue ellipses are the true asymptotic 95\% data ellipses centred at $(\sigma_{ij},\sigma_{lm})$ (blue crosses). Their orientation relates to the association $\rho_{ij,lm}$ between $\hat{\sigma}_{ij}$ and $\hat{\sigma}_{lm}$, while the lengths of the major and minor axes are dictated by the asymptotic variances $\nu_{ij}^2,\nu_{lm}^2$. The red ellipses and crosses are defined analogously but estimated from the data. They are generally in close agreement. The upper-triangular panels list the values of $\rho_{ij,lm}$ (blue) alongside empirical estimates (red) based on the sample covariance between $\hat{\sigma}_{ij}$ and $\hat{\sigma}_{lm}$. 
:::

```{r make-fig-max-linear-example-A-Sigma-V}
#| label: fig-max-linear-example-A-Sigma-V
#| fig-cap: "Visual representation of the matrices discussed in @exm-max-linear-asymptotic-tpdm. Top: a randomly generated max-linear parameter matrix $A$ with $d=4$ and $q=12$. Bottom left: the TPDM $\\Sigma$ of $\\bm{X}=A\\times_{\\max}\\bm{Z}$. Bottom right: the asymptotic covariance matrix $V$ of $\\hat{\\bm{\\sigma}}$."
#| fig-scap: "Max-linear parameter matrix $A$ and the associated $\\Sigma$ and $V$."
#| fig-height: 7

data <- readRDS("scripts/background/results/max-linear-asymptotic-tpdm.RDS")

p1 <- plot_tpdm(data$A, y_labels = FALSE) %>% update(aspect = 0.5)
p2 <- plot_tpdm(data$Sigma)
p3 <- plot_tpdm_eigen(data$V)

ggarrange(p1, ggarrange(p2, p3, ncol = 2), nrow = 2)
```


```{r make-fig-max-linear-example-ggpairs}
#| label: fig-max-linear-example-ggpairs
#| fig-cap: "Pairs plot illustrating asymptotic normality of the empirical TPDM -- see @exm-max-linear-asymptotic-tpdm for details. All panels: red represents the empirical quantity based on the 1,000 repeated simulations; blue represents the theoretical quantity based on asymptotic normality. Diagonal panels: the distribution (histogram or density function) of $\\hat{\\sigma}_{ij}$. Lower triangular panels: pairwise scatter plots of $(\\hat{\\sigma}_{ij},\\hat{\\sigma}_{lm})$ (grey points) along with the mean (crosses) and the 95\\% data ellipse. Upper triangular panels: the entries $v_{ij,lm}$ of $V$."
#| fig-scap: "Empirical verification of asymptotic normality of $\\hat{\\bm{\\sigma}}$."
#| fig-height: 6
#| warning: false
#| message: false

tpdm_max_linear_ggpairs(data = data$emp_V, Sigma = data$Sigma, V = data$V, k = data$k)
```


## Existing applications and extensions of the TPDM

The general objective of this thesis is to develop novel statistical tools for analysing extremal dependence based on the TPDM. Before presenting these, we acquaint the reader with existing TPDM-based methods, selected according to their relevance to the thesis. Our survey divides the related literature into two main categories: principal components analysis (PCA) and inference for the max-linear model. Clustering features occasionally (e.g. in Chapter XX), but does not constitute an essential pillar of our research; a brief overview of TPDM-based clustering algorithms [@fomichovSphericalClusteringDetection2023; @richardsModernExtremeValue2024] is contained in Appendix XX. Further interesting topics that are not covered include time series [@mhatreTransformedLinearModelsTime2021; @wixsonAttributionSeasonalWildfire2023] and graphical models [@gongPartialTailCorrelationCoefficient2024; @leePartialTailCorrelation2023]. Throughout this section, $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ is a random vector on $\alpha$-Fréchet margins with angular measure $H$ with respect to $\|\cdot\|=\|\cdot\|_\alpha$ and $b_n=n^{1/\alpha}$, while $\bm{X}_1,\ldots,\bm{X}_n$ are independent copies of $\bm{X}$.

### Principal component analysis (PCA) for extremes

In classical multivariate statistics, principal component analysis (PCA) is the flagship method for reducing the dimension of a random vector by finding linear subspaces that minimise the distance between the data and its low-dimensional projections [@blanchardStatisticalPropertiesKernel2007; @jolliffePrincipalComponentAnalysis2002]. The central idea is to transform the original set of correlated variables into a new set of uncorrelated variables -- the principal components -- which are ordered so that the first few capture most of the variability in the data. Computing these variables boils down to computing the eigendecomposition of a symmetric, positive semi-definite matrix. A more detailed review of the theory of PCA is given in Appendix XX.

In multivariate extremes, it is often assumed that the angular measure has a low-dimensional structure [@engelkeSparseStructuresMultivariate2021]. For example, weather extremes typically exhibit spatial patterns related to geographical or topographical drivers, e.g. north/south, coastal/inland, high-lying/low-lying [@bernardClusteringMaximaSpatial2013; @jiangPrincipalComponentAnalysis2020]. These patterns permit a description of a process' extremal behaviour in terms of a smaller number of variables. This is the key objective of PCA for extremes. 

Classical (non-extreme) PCA is not appropriate for this task for several reasons. The original variables $X_1,\ldots,X_d$ are usually heavy-tailed, so the requirement on the existence second-order moments may be violated. (The variance of an $\alpha$-regularly varying random variable is infinite if $\alpha<2$.) Standard PCA reveals relationships between variables in the centre rather than the tail of the joint distribution, because it arises from the covariance matrix. Moreover, it captures dependence in both directions around the origin/mean, whereas we focus on a particular direction of interest. Finally, standard PCA fails to capitalise on the probabilistic structure inherent to MRV random vectors. The heavy-tailed, univariate radial component accounts for most of the variability in the data, but it is (asymptotically) independent of the angular component that actually contains the relevant information about the association between the variables. This suggests performing dimension reduction on the (empirical) angular measure via eigendecomposition of the (empirical) TPDM [@cooleyDecompositionsDependenceHighdimensional2019; @dreesPrincipalComponentAnalysis2021].

@dreesPrincipalComponentAnalysis2021 adopt a risk minimisation perspective aiming to minimise the mean-squared reconstruction error of $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ with respect to the limit distribution $H$. They define the (asymptotic) risk of a subspace $\mathcal{S}\subset\R^d$ as
\begin{equation*}
    R(\mathcal{S}) = \mathbb{E}_{H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_2^2],
\end{equation*}
where $\Pi_{\mathcal{S}}$ denotes orthogonal projection onto $\mathcal{S}$. The true risk cannot be minimised directly because $H$ is unknown. Instead, they minimise the empirical risk
\begin{equation*}
    \hat{R}(\mathcal{S}) 
    := \hat{\mathbb{E}}_{H}[\|\bm{\Theta}-\Pi_{\mathcal{S}}\bm{\Theta}\|_2^2] 
    = \frac{d}{k}\sum_{i=1}^k \|\bm{\Theta}_{(i)}-\Pi_{\mathcal{S}}\bm{\Theta}_{(i)}\|_2^2.
\end{equation*}
This is justified because, above a sufficiently high threshold, the extremal angles will lie in a neighbourhood of the target subspace. Let $\mathcal{V}_p$ denote the class of all linear subspaces of dimension $1\leq p \leq d$ in $\R^d$. Minimisers of $\hat{R}$ are computed via eigendecomposition of the empirical TPDM. Let $(\hat{\bm{u}}_j,\hat{\lambda}_j)$ denote the (ordered) eigenpairs of $\hat{\Sigma}$ for $j=1,\ldots,d$. Then $\hat{\mathcal{S}}_p:=\mathrm{span}\{\hat{\bm{u}}_1,\ldots,\hat{\bm{u}}_d\}$ minimises $R$ in $\mathcal{V}_p$ and $\hat{R}(\mathcal{S}_p)=\sum_{j>p}\hat{\lambda}_j$ [@dreesPrincipalComponentAnalysis2021, Lemma 2.1]. If the data exhibit a low-dimensional (linear) structure, then one can find $p\ll d$ such that the risk is acceptable small. It is recommended to plot $\hat{R}(\mathcal{S}_p)$ against $p$ when choosing the number of principal components to retain. In terms of theoretical statistical guarantees, they prove that the learnt subspace converges to the optimal one as the sample size increases to infinity [@dreesPrincipalComponentAnalysis2021, Theorem 2.4]. Suppose there exists $p^\star < d$ and a linear subspace $\mathcal{S}^\star \in\mathcal{V}_{p^\star}$ such that $R(\mathcal{S}^\star) = 0$ and $R(\mathcal{S}) > 0$ for any $\mathcal{S}\in\cup_{p>p^\star}\mathcal{V}_{p}$. Then, provided $k(n)$ satisfies the rate conditions \eqref{eq-k-rate-conditions}, $\hat{\mathcal{S}}_{p^\star}\to \mathcal{S}^\star$ in the sense that
\begin{equation*}
    \lim_{n\to\infty} \sup_{\bm{\theta}\in\mathbb{S}_{+(\alpha)}^{d-1}} \|\Pi_{\hat{\mathcal{S}}_{p^\star}}\bm{\theta} - \Pi_{\mathcal{S}^\star}\bm{\theta}\|_2  = 0.
\end{equation*}

Treating the angles $\bm{\Theta}_{(1)},\ldots,\bm{\Theta}_{(k)}$ as points in $\R^d$ rather than $\mathbb{S}_{+(\alpha)}^{d-1}$ simplifies the derivation of theoretical guarantees but creates interpretability issues. The rank-$p$ reconstructions of the angles do not lie in the simplex, in general. Shifting/normalising the reconstructed vectors corrects this, but optimality properties will be not be preserved. One may also question the appropriateness of the Euclidean norm as a measure for the angular reconstruction error. In the context of clustering, @janssenKmeansClusteringExtremes2020 argue that angular distances (e.g. the cosine dissimilarity) are a more natural choice. On a similar note, their working hypothesis that the low-dimensional structure of $H$ is linear in $\R^d$ is restrictive. @avella-medinaKernelPCAMultivariate2022 develop a kernel PCA method for extracting non-linear patterns. In Chapter XX, we propose our own PCA method, inspired by compositional PCA, that addresses all of these concerns: reconstructions are in $\mathbb{S}_{+(\alpha)}^{d-1}$, errors are defined using a simplicial metric, and non-linearity (curvature) in the data is captured.

@cooleyDecompositionsDependenceHighdimensional2019 propose an alternative approach based on the so-called transformed-linear inner product space on $\R_+^d$, the sample space of $\bm{X}$. It is grounded on the softplus transformation
\begin{equation*}
    \tau:\R\to \R_+, \qquad \tau(x) = \log[1+\exp(x)].
\end{equation*}
This transformation is bijective with inverse function $\tau^{-1}(y)=\log[\exp(y)-1]$ and, crucially, it is tail-preserving, i.e. $\lim_{x\to 1}\tau(x)/x = 1$. The role of $\tau$ is to provide a pathway between $\R^d$ and $\R_+^d$ that doesn't disturb the tails. The inner product space is constructed as follows. For any $\bm{x},\bm{y}\in\R_+^d$ and $\alpha\in\R$, they define operations
\begin{equation*}
    \bm{x} \oplus \bm{y} = \tau[\tau^{-1}(\bm{x})+\tau^{-1}(\bm{y})],\qquad 
    \alpha \odot \bm{x} = \tau[a\tau^{-1}(\bm{x})].
\end{equation*}
and an inner product and norm
\begin{equation*}
    \left\langle \bm{x},\bm{y}\right\rangle_\tau =\left\langle \tau^{-1}(\bm{x}),\tau^{-1}(\bm{y})\right\rangle ,\qquad 
    \|\bm{x}\|_\tau = \left\langle \bm{x},\bm{x}\right\rangle_\tau^{1/2} = \|\tau^{-1}(\bm{x})\|_2.
\end{equation*}
The PCA procedure may then be formulated in the transformed-linear space $\mathcal{H}=\R_+^d$ or in $\R^d$ under the transform/back-transform approach (see Appendix XX). As in @dreesPrincipalComponentAnalysis2021, let $(\hat{\bm{u}}_j,\hat{\lambda}_j)$ be the ordered eigenpairs (in $\R^d$) of $\hat{\Sigma}$. Then $\{\hat{\bm{\omega}}_1,\ldots,\hat{\bm{\omega}}_d\}=\{\tau(\hat{\bm{u}}_1),\ldots,\tau(\hat{\bm{u}}_d)\}$ forms an orthonormal basis of $\R_+^d$. In this new basis, the random vector $\bm{X}$ may be decomposed as
\begin{equation}\label{eq-cooley-pca-expansion}
    \bm{X} = \bigoplus_{j=1}^d (\hat{V}_j \odot \hat{\bm{\omega}}_j) = \tau\left(\sum_{j=1}^d \hat{V}_j\hat{\bm{u}}_j\right),
\end{equation}
where $\hat{V}_j = \left\langle \bm{X},\hat{\bm{\omega}}_j\right\rangle_\tau$ for $j=1,\ldots,d$. Truncating the expansion \eqref{eq-cooley-pca-expansion} yields low-rank reconstructions of $\bm{X}$. The random variables $\hat{V}_1,\ldots,\hat{V}_d$ are called the extremal principal components of $\bm{X}$. The MRV $\R^d$-valued random vector $\hat{\bm{V}}$ has the same dimensions as $\bm{X}$, but its components are ordered according to their contribution to the extremal behaviour of $\bm{X}$ in the sense that [@cooleyDecompositionsDependenceHighdimensional2019, Proposition 6]
\begin{equation*}
    \mathrm{scale}(|\hat{V}_i|) = \hat{\lambda}_i^{1/\alpha}, \qquad (i=1,\ldots,d),
\end{equation*}
Thus, the $i$th eigenvector $\hat{\bm{\omega}}_i$ represents the direction of maximum scale after accounting for information contained in the previous eigenvectors $\{\hat{\bm{\omega}}_j:j<i\}$. 

Visualising/examining the TPDM eigenvectors a powerful tool for gaining insight into the extremal dependence structure. In a study of precipitation extremes in the United States, @jiangPrincipalComponentAnalysis2020 relate the leading eigenvectors to the El-Niño Southern Oscillation (ENSO), a cyclical phenomenon that is known to be a key climatological driver. Low-rank reconstructions of Hurricane Floyd broadly capture the large-scale structure, but recreating localised features requires a large number of components. @russellAnalyzingDependenceMatrices2018 compare covariance matrix eigenvectors against TPDM eigenvectors to characterise performance differences between typical and elite-level National Football League (NFL) performers across a battery of physical tests. @szemkusSpatialPatternsIndices2024 apply PCA to the cross-TPDM, an extension to the TPDM that is analogous to the cross-covariance matrix, to analyse the dynamics of compound extreme weather events. For event detection and attribution purposes, they devise indices quantifying whether particular patterns of interest -- those signified by the cross-TPDM's singular vectors -- are highly pronounced. @rohrbeckSimulatingFloodEvent2023 move beyond exploratory analysis and demonstrate how the framework can be used to generate synthetic extreme events. Hazard event sets are widely used in catastrophe modelling to assess exposure to extreme events (CITE). Their sampling algorithm exploits the fact that the leading components of $\hat{\bm{V}}$ account for a significant proportion of the extremal behaviour of $\bm{X}$. Roughly speaking, dependence between $\hat{V}_1,\ldots,\hat{V}_p$ is captured with a flexible model and a simple model is used to account for the remaining, relatively unimportant components. They use this model to generate samples of $\hat{\bm{V}}$, from which samples of $\bm{X}$ are produced via \eqref{eq-cooley-pca-expansion}. 

Results based on @cooleyDecompositionsDependenceHighdimensional2019 require accurate estimation of the TPDM so that the empirical eigenvectors reflect the true eigenvectors. However, in weak-dependence scenarios the empirical TPDM suffers from a positive bias (Section XX). This is problematic when the spatial extent of the study region is large relative to that of the modelled phenomenon. @jiangPrincipalComponentAnalysis2020 ameliorate this using a 'pairwise-thresholded' estimator instead of \eqref{eq-empirical-tpdm}, defined as
\begin{equation*}
    \hat{\Sigma}^{(p)}=(\hat{\sigma}_{ij}^{(p)}), \qquad \hat{\sigma}_{ij}^{(p)} = \frac{2}{k} \sum_{l=1}^n \Theta_{li}\Theta_{lj}\ind\{R_l^{ij} > R_{(k+1)}^{ij}\},
\end{equation*}
where $R_l^{ij} = \|(X_{li},X_{lj})\|$ and $R_{(k+1)}^{ij}$ is the $(k+1)$th upper order statistic of $\{R_l^{ij}:l=1,\ldots,n\}$. However, the resulting estimator $\hat{\Sigma}^{(p)}$ is not positive semi-definite. This may be resolved by projecting $\hat{\Sigma}^{(p)}$ onto the space of correlation matrices [@highamComputingNearestCorrelation2002], but this ad-hoc step does not address the fundamental problem. Partly motivated by this, Chapter XX proposes an improved estimator that is positive semi-definite.

### Inference for the max-linear model

Estimating the parameter matrix $A$) of the max-linear model is a challenging task. The lack of an angular density function precludes the use of standard maximum likelihood procedures. @einmahlContinuousUpdatingWeighted2018 propose a procedure that minimises a weighted least-squares distance to some initial (non-parametric) estimator. Their procedure becomes computationally intensive when $q$ is large. @janssenKmeansClusteringExtremes2020 and @medinaSpectralLearningMultivariate2021 cluster the angles of extreme observations and identify the normalised columns of $A$ with the $q$ cluster centres. The minimum-distance and clustering approaches assume $q$ is fixed; @kirilioukHypothesisTestingTail2020 present a hypothesis test to assist with choosing $q$. 

Recently, the TPDM has emerged as a promising tool for inference for the max-linear model [@fixSimultaneousAutoregressiveModels2021; @kirilioukEstimatingProbabilitiesMultivariate2022]. Recall from @exm-max-linear-tpdm that the TPDM of a max-linear random vector $\bm{X}\in\mathcal{RV}_+^d(\alpha)$ is $\Sigma = \hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T$. Now consider @prp-tpdm-completely-positive, which says that the TPDM is completely positive (@def-cp). Based on this connection, originally observed by @cooleyDecompositionsDependenceHighdimensional2019, any matrix belonging to the set
\begin{equation*}
    \mathcal{CP}(\hat{\Sigma}) := \left\lbrace \hat{A}\in\R_+^{d\times q} : q\geq 1, \,\hat{\Sigma}=\hat{A}^{\alpha/2}(\hat{A}^{\alpha/2})^T\right\rbrace.
\end{equation*}
may be considered a reasonable estimate for $A$, in the sense that the pairwise dependencies of the fitted model conform with those implied by $\hat{\Sigma}$. The set $\mathcal{CP}(\hat{\Sigma})$ is in one-to-one correspondence with the set of completely positive (CP) factors of $\hat{\Sigma}$. We call $\hat{A}\in\mathcal{CP}(\hat{\Sigma})$ a CP-estimate of $A$. In general, a completely positive matrix may have many CP factorisations [@shaked-mondererNumberCPFactorizations2020]. Among these, the simplest CP-estimate is the empirical estimate $\hat{A}\in\R_+^{d\times k}$ as defined in \eqref{eq-empirical-A}. @cooleyDecompositionsDependenceHighdimensional2019 describe the empirical estimate as `naive' because it probably contains more columns than necessary. @kirilioukEstimatingProbabilitiesMultivariate2022 provide an algorithm for efficiently factorising $\hat{\Sigma}$ to obtain further. The performance of their CP-estimation procedure is assessed in simulation studies by computing tail event probability estimates under the true and fitted models using \eqref{eq-max-linear-failure-probability}. The fitted models capture the dependence structure reasonably well, except for certain classes of failure regions. This is partly attributed to estimation error in the TPDM. 

@fixSimultaneousAutoregressiveModels2021 analyse the effect of TPDM estimation error for max-linear model fitting in more detail. Focussing on spatial extremes, they define the extremal spatial autoregressive (SAR) model, a special case of the max-linear model where $A=A(\rho)$ is determined by a single dependence parameter $\rho\in (0,1/4)$. The model parameter $\rho$ is estimated by minimising the discrepancy between $\hat{\Sigma}$ and the theoretical TPDM $\Sigma(\rho):=A(\rho)A(\rho)^T$ (assuming $\alpha=2$):
\begin{equation}\label{eq-sar-rho-estimator}
    \hat{\rho} = \argmin_{\rho\in(0,1/4)}\|\Sigma(\rho) - \hat{\Sigma}\|_F^2.
\end{equation}
They find that $\hat{\rho}$ has a positive bias when $\rho$ is small (weak dependence). The proximate cause is that $\hat{\Sigma}$ overestimates weak dependencies, biasing the fitted model. This fundamental problem, and their proposed remedy, is the subject the following section. 

## Bias in the empirical TPDM in weak-dependence scenarios

The empirical TPDM is consistent and asymptotically unbiased (@prp-empirical-tpdm-normality). This provides a guarantee that, with sufficient data, the empirical TPDM reflects the true pairwise dependence structure. The associated rate of convergence is $\mathcal{O}(k^{-1/2})$, where $k=k(n)$ represents the number of extreme observations and satisfies the rate conditions \eqref{eq-k-rate-conditions}. However, in real-world applications, data are limited and extreme observations are scarce. For example, commonly available climate records typically span approximately 50 years [@boulaguiemModelingSimulatingSpatial2022]. A study of summer heatwaves might then be based on, say, $n\approx 50 \times 100 = 5,000$ daily observations. The second condition in \eqref{eq-k-rate-conditions} requires that the effective sample size is some small fraction of $n$, resulting in a very limited number of extreme data points. Asymptotic guarantees are therefore of limited value for the sample sizes available in practice. This motivates an analysis of the empirical TPDM's finite-sample performance. As alluded to in the previous section, it will transpire that the TPDM is biased in scenarios where tail dependence is weak [@cooleyDecompositionsDependenceHighdimensional2019; @fixSimultaneousAutoregressiveModels2021; @mhatreTransformedLinearModelsTime2021], herein referred to as the `(weak dependence) bias issue'. Chapter XX proposes bias-corrected estimators with superior finite-sample performance, but the bias issue will arise at various points in the preceding chapters, so we choose to highlight it now.

### Bias in the TPDM and threshold-based estimators

The bias issue is not exclusive to the empirical TPDM. In fact, it applies more generally to threshold-based estimators in multivariate extremes. For example, @huserLikelihoodEstimatorsMultivariate2016 conduct simulation studies examining the finite-sample performance of estimators of $\gamma$, the dependence parameter of the symmetric logistic model. The results show that block-maxima based estimators have a small bias but very high variability. On the other hand, the estimator $\hat{\gamma}$ based on threshold exceedances tends to overestimate the dependence strength, that is $\mathrm{Bias}(\hat{\gamma}) = \mathbb{E}[\hat{\gamma}] - \gamma < 0$. This discrepancy increases as dependence weakens; see the second column of Figure 3 in @huserLikelihoodEstimatorsMultivariate2016. Problems of a similar nature can be found across the multivariate extremes literature, for example in spatial modelling [@boulaguiemModelingSimulatingSpatial2022, Figure 6c] and lower-tail dependence modelling [@dobricNonparametricEstimationLower2005]. 

```{r background-make-fig-parametric-chi-tpdm-small-n}
#| label: fig-parametric-chi-tpdm-small-n
#| fig-cap: "True dependence strengths for the symmetric logistic (left) and Hüsler-Reiss (right) models, measured using the tail dependence coefficient (red line) and TPDM (blue line). The shaded regions represent the minimum/maximum values of empirical estimates over 10 repeated simulations using bivariate samples of size $n=5\\times 10^3$."
#| fig-scap: "Bias in estimation of $\\sigma$ for symmetric logistic and Hüsler-Reiss models."
#| fig-height: 3.5

data <- readRDS(file.path("scripts", "background", "results", "parametric_chi_tpdm_empirical_smalln.RDS")) 

p1 <- ggplot() + 
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "log"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) +
  geom_function(aes(colour = "chi"), fun = sl_chi, xlim = c(0, 1)) + 
  geom_function(aes(colour = "tpdm"), fun = sl_tpdm, xlim = c(0, 1)) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0, 0), breaks = breaks_extended(n = 5)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(gamma), 
       y = "Dependence strength", 
       title = "Symmetric logistic") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10))  

p2 <- ggplot() + 
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = chi), fun.min = min, fun.max = max, geom = "ribbon", fill = "red", colour = NA, alpha = 0.2) +
  stat_summary(data = filter(data, model == "hr"), aes(x = dep_par, y = sigma), fun.min = min, fun.max = max, geom = "ribbon", fill = "blue", colour = NA, alpha = 0.2) + 
  geom_function(aes(colour = "chi"), fun = hr_chi, xlim = c(0, 3.5)) + 
  geom_function(aes(colour = "tpdm"), fun = hr_tpdm, xlim = c(0, 3.5)) + 
  scale_x_continuous(limits = c(0, 3.5), expand = c(0, 0), breaks = breaks_extended(n = 4)) + 
  scale_y_continuous(limits = c(0, 1), expand = expansion(mult = c(0, 0.02))) + 
  scale_colour_manual(values = c("red", "blue"), labels = expression(chi, sigma)) + 
  theme_light() + 
  labs(colour = "Measure", 
       x = expression(lambda), 
       y = "Dependence strength", 
       title = "Hüsler-Reiss") + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 10)) 

ggarrange(p1, p2, ncol = 2, common.legend = TRUE) 
``` 

The empirical TPDM suffers from the same issue when dependence is weak. This phenomenon is illustrated in @fig-parametric-chi-tpdm-small-n. Like in @fig-parametric-chi-tpdm, the blue lines represent the true dependence strength for a given model parameter and the shaded regions indicate the minimum/maximum over a set of ten empirical estimates. The only difference is that here the underlying sample size is $n=5\times 10^3$, whereas in @fig-parametric-chi-tpdm it was $n=5\times 10^5$. In both plots, the tuning parameter was set as $k=\sqrt{n}$. The plot definitively shows that the empirical TPDM overestimates the dependence as $\gamma$ and $\lambda$ approach their upper limits, or, equivalently, as $\sigma\to 0$. *Should I keep $\chi$ in the plot?* This can be summarised as
\begin{equation}\label{eq-empirical-tpdm-bias}
    \sigma_{ij} \ll 1 \implies \mathrm{Bias}(\hat{\sigma}_{ij}) = \mathbb{E}[\hat{\sigma}_{ij}] - \sigma_{ij} > 0.
\end{equation}
Note that overestimating the dependence strength corresponds to a positive bias in the TPDM estimate, so the inequality is reversed when compared to $\gamma$.

Estimation error in the empirical TPDM was first studied by @cooleyDecompositionsDependenceHighdimensional2019. Figure 3 in their Supplementary Material assesses the accuracy of the eigenvalues/eigenvectors of the empirical TPDM based on data generated from a Brown-Resnick model. The leading eigenvalue is consistently overestimated ($\hat{\lambda}_1 > \lambda_1$) and subsequent eigenvalues are underestimated ($\hat{\lambda}_j < \lambda_j$ for $j\geq 2$). The sample covariance matrix suffers a similar deficiency, especially when the sample size and dimension are comparable in magnitude [@mestreImprovedEstimationEigenvalues2008]. The magnitude of the bias depends on the sample size and the proportion $k/n$. Errors in the eigenvalues may influence the results of downstream PCA analysis, e.g. in deciding how many components are to be retained in the PCA.

### Existing bias-correction approaches for the TPDM

The first strategy for tackling the bias issue is found in @mhatreTransformedLinearModelsTime2021. Working in a time series context, they study serial dependence in extremes using the tail pairwise dependence function (TPDF) $\sigma(h)$, which summarises the tail dependence between $X_t$ and $X_{t+h}$ for a tail stationary time series $\{X_t:t=1\ldots,n\}$. Simulation experiments reveal that the empirical TPDF $\hat{\sigma}(h)$ is biased at higher lags where the true dependence is close to zero. To counteract this, they subtract the mean from the time series in pre-processing. The rationale for this is described in terms of the position of extreme points in a lag plot (i.e. a scatter plot of $(X_t, X_{t+h})$ for fixed $h$). Subtracting the mean has negligible effect on the angles corresponding to joint extremes, but points near a coordinate axis are shifted even closer to the axis. 

@fixSimultaneousAutoregressiveModels2021 develop the first bias-corrected estimate of the TPDM. Recall from Section XX the problem of estimating the spatial dependence parameter $\rho$ of the extremal SAR model \eqref{eq-sar-model}. When the study domain is large or the modelled phenomenon is highly localised, the pairwise dependence between distant sites is weak and the empirical TPDM is prone to overestimation in the corresponding entries. This bias carries over to $\hat{\rho}$ defined in \eqref{eq-sar-rho-estimator}. Their bias-corrected estimate $\tilde{\Sigma}$ reduces the entries of $\hat{\Sigma}$ by element-wise application of the soft-thresholding operator [@rothmanGeneralizedThresholdingLarge2009], that is
\begin{equation}\label{eq-fix-bias-corrected-tpdm}
    \tilde{\Sigma}=(\tilde{\sigma}_{ij}), \qquad \tilde{\sigma}_{ij} = \begin{cases}
        \hat{\sigma}_{ij}, & i=j, \\
        (\hat{\sigma}_{ij} - \lambda)_+, & i\neq j.
    \end{cases}
\end{equation}
The threshold $\lambda\geq 0$ is selected by assuming that the pairwise tail dependence vanishes to zero as the distance between two sites increases. For $i\neq j$, let $h_{ij}$ denote the (known) spatial distance between the sites corresponding to the variables $X_i$ and $X_j$. Treating the empirical TPDM entries $\{\hat{\sigma}_{ij}:i\neq j\}$ as functions of distance, they model tail dependence strength against spatial distance via
\begin{equation*}
    \hat{\sigma}(h) = \beta_0 \exp(-\beta_1 h) + \beta_2.
\end{equation*}
The parameters $\beta_0,\beta_1,\beta_2$ are estimated from the data $\{(\hat{\sigma}_{ij},h_{ij}):1\leq i<j\leq d\}$ by non-linear least squares estimation, e.g. using \texttt{nls()}. Since $\hat{\sigma}(h)\to\beta_2$ as $h\to\infty$, the horizontal asymptote $\hat{\beta}_2$ of the fitted model is used as a proxy for the bias at large distances. It suggests itself to choose $\lambda=\hat{\beta}_2$. Clearly this procedure is only viable in spatial contexts where a notion of proximity exists.

The contrasting strategies of @mhatreTransformedLinearModelsTime2021 and @fixSimultaneousAutoregressiveModels2021 point towards two qualitatively different ways of improving tail dependence estimation. The first approach acts directly on the data by moving (some of) the extremal angles $\bm{\theta}_{(1)},\ldots,\bm{\theta}_{(k)}$ closer to boundary of the simplex in some principled way. In other words, improved inference may be achieved by perturbing the empirical angular measure. This outlook is central to Chapter XX, where we employ sparse simplex projections [@meyerSparseRegularVariation2021] to fit max-linear models. Under the second approach, bias-correction is undertaken as a post-processing step. Chapter XX pursues this idea in more detail. We propose a general class of shrinkage/thresholded TPDM estimators that includes \eqref{eq-fix-bias-corrected-tpdm}. Unlike @fixSimultaneousAutoregressiveModels2021, our tuning procedure for selecting the hyperparameter $\lambda$ is purely data-driven and can be applied in general settings, not just spatial.

